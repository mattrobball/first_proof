# Proof Pipeline Transcript

- Problem: `10`
- Started (UTC): `2026-02-11T20:45:21.899837+00:00`
- Finished (UTC): `2026-02-11T21:24:19.400720+00:00`
- Max loops: `5`
- Executed loops: `5`
- Rigor: `graduate`
- Reviewer perspectives: 3
- Final verdict: `right_track`

## Loop 1

### Researcher Agent

## Relevant Theorems
- **Representer theorem (RKHS regularization):** For empirical losses depending only on finitely many evaluations and strictly increasing norm penalty, minimizers lie in the span of kernel sections. This justifies \(A_k = K W\) for the mode-\(k\) RKHS factor (Kimeldorf-Wahba, 1971; Schölkopf-Herbrich-Smola, 2001; RKHS foundation: Aronszajn, 1950).
- **Kronecker/vec identity:** \((C\otimes A)\,\mathrm{vec}(B)=\mathrm{vec}(A B C^\top)\). This is the key to turning \((Z\otimes K)\) products into matrix operations on \(W\), without forming \(N\times nr\) matrices (Magnus-Neudecker; Golub-Van Loan).
- **Khatri-Rao Gram identity:** \((\odot_i A_i)^\top(\odot_i A_i)=*_{i}(A_i^\top A_i)\) (Hadamard product). Useful for small-\(r\) preconditioner construction (Kolda-Bader, 2009).
- **PCG convergence theorem:** If \(A\) and preconditioner \(P\) are SPD, PCG error satisfies
  \[
  \|e_m\|_A \le 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m \|e_0\|_A,\quad
  \kappa=\kappa(P^{-1/2}AP^{-1/2}).
  \]
  (Hestenes-Stiefel, 1952; Saad, *Iterative Methods for Sparse Linear Systems*).
- **Spectral equivalence principle:** If \(c_1 P\preceq A\preceq c_2 P\), then \(\kappa(P^{-1}A)\le c_2/c_1\). This is the standard justification framework for preconditioner quality.

## Key Definitions
- **Observed set and mask:** Let \(\Omega\subset[n]\times[M]\), \(|\Omega|=q\). \(S\in\mathbb{R}^{N\times q}\) is the selection matrix, \(P_\Omega:=SS^\top\) is a diagonal projector onto observed entries in \(\mathrm{vec}(T)\).
- **Unknown:** \(w=\mathrm{vec}(W)\in\mathbb{R}^{nr}\), with \(A_k=KW\).
- **Linear system operator:**
  \[
  A w := \left[(Z\otimes K)^\top P_\Omega (Z\otimes K)+\lambda(I_r\otimes K)\right]w.
  \]
- **Equivalent matrix form for \(V\in\mathbb{R}^{n\times r}\):**
  \[
  \mathcal{A}(V)=K\Big((\Omega\odot(KVZ^\top))Z+\lambda V\Big),\quad
  A\,\mathrm{vec}(V)=\mathrm{vec}(\mathcal{A}(V)).
  \]
- **Sampled row features:** For observed pair \((i_t,m_t)\), define \(z_t:=Z_{m_t,:}\in\mathbb{R}^r\). Then data Hessian term is \(\sum_{t=1}^q (z_t z_t^\top)\otimes(k_{i_t}k_{i_t}^\top)\), \(k_{i_t}^\top\) = row \(i_t\) of \(K\).

## Proof Strategies
1. **Use matrix-free PCG on \(A w=b\)** with \(b=(I_r\otimes K)\mathrm{vec}(B)\), \(B=TZ\). Never form \(A\), \(Z\otimes K\), or any \(N\)-scale object.
2. **Matvec implementation (core step):**
   - Compute \(U=KV\) (\(O(n^2r)\)).
   - Initialize \(C=0\in\mathbb{R}^{n\times r}\).
   - For each observed entry \(t=1,\dots,q\):
     - \(\alpha_t=\langle U_{i_t,:}, z_t\rangle\) (\(O(r)\)).
     - \(C_{i_t,:}\mathrel{+}= \alpha_t z_t\) (\(O(r)\)).
   - Output \(Y=K C+\lambda U\) (\(O(n^2r)\)).
   - Then \(\mathrm{vec}(Y)=A\,\mathrm{vec}(V)\).
   - Total per matvec: \(O(qr+n^2r)\) (plus \(O(q(d-1)r)\) if each \(z_t\) is generated on-the-fly from CP factors).
3. **RHS computation without \(N\):**
   - Sparse MTTKRP: \(B_{i_t,:}\mathrel{+}=T_{i_t,m_t} z_t\) over \(q\) observations (\(O(qr)\)).
   - Then \(b=\mathrm{vec}(KB)\) (\(O(n^2r)\)).
4. **Preconditioner choice (practical strong option):**
   - \(P=(G_\Omega+\lambda I_r)\otimes (K+\delta I_n)\), \(\delta>0\) nugget for robustness.
   - \(G_\Omega=\sum_{t=1}^q z_t z_t^\top\) (\(O(qr^2)\)).
   - Apply \(P^{-1}\) to residual \(R\in\mathbb{R}^{n\times r}\) via two small solves:
     - Solve \((K+\delta I)Y=R\),
     - Solve \(X(G_\Omega+\lambda I)=Y\),
     - return \(\mathrm{vec}(X)\).
   - Precompute Cholesky factors: \(O(n^3+r^3)\). Per iteration preconditioner cost: \(O(n^2r+nr^2)\).
5. **Complexity bound:**
   - Per PCG iteration: \(O(qr+n^2r+nr^2)\), dominated by \(O(qr)\) when \(q\gg n,r\).
   - Total: \(O\!\left((qr+n^2r+nr^2)\,m + qr^2 + n^3 + r^3\right)\), with \(m=O(\sqrt{\kappa}\log(1/\varepsilon))\).
   - No \(O(N)\) operations.

## Gaps and Concerns
- **SPD condition is not automatic:** If \(K\) is only PSD, \(A\) can be singular (nullspace propagates through both terms). For strict PCG guarantees, assume \(K\succ0\) or use \(K+\delta I\), \(\delta>0\), or solve in \(\mathrm{range}(K)\).
- **Need \(\lambda>0\):** Without regularization, ill-posedness/slow convergence can be severe with missing data.
- **Preconditioner quality depends on sampling structure:** Spectral-equivalence claims need assumptions (roughly balanced/mixing observations across mode-\(k\) indices and rank directions).
- **Do not materialize full \(Z\) when \(M\) is huge:** Use only sampled rows \(z_t\) (computed or cached from observed multi-indices).
- **Index-order consistency is critical:** The mapping between tensor indices, mode-\(k\) unfolding, and \(\mathrm{vec}\) order must match \(S\), otherwise the operator is wrong.
- **Numerical monitoring:** Use relative preconditioned residual and max-iteration safeguards; with extreme missingness, stagnation is possible even with PCG.

### Mentor Agent

## Definitions
Let \(k\) be the target mode, \(n:=n_k\), \(M:=\prod_{i\neq k} n_i\), \(N=nM\), and rank \(r\).

Let \(\Omega\subset [n]\times[M]\) be the observed index set in mode-\(k\) unfolding, \(|\Omega|=q\), with observed triples \((i_t,m_t,y_t)\), \(t=1,\dots,q\), where \(y_t=T_{i_t,m_t}\).

Define
\[
Z:=A_d\odot\cdots\odot A_{k+1}\odot A_{k-1}\odot\cdots\odot A_1\in\mathbb{R}^{M\times r},
\quad
z_t:=Z_{m_t,:}\in\mathbb{R}^r.
\]
Unknown is \(W\in\mathbb{R}^{n\times r}\), \(w:=\mathrm{vec}(W)\), and \(A_k=KW\).

Define \(P_\Omega:=SS^\top\) (projector selecting observed entries in \(\mathrm{vec}(T)\)) and linear system
\[
A w=b,\quad
A=(Z\otimes K)^\top P_\Omega (Z\otimes K)+\lambda(I_r\otimes K),\quad
b=(I_r\otimes K)\mathrm{vec}(B),\ B=TZ.
\]

Define mask matrix \(\mathbf{1}_\Omega\in\{0,1\}^{n\times M}\). For \(V\in\mathbb{R}^{n\times r}\),
\[
\mathcal A(V):=
K\!\left((\mathbf{1}_\Omega\odot (KVZ^\top))Z+\lambda V\right),
\quad
A\,\mathrm{vec}(V)=\mathrm{vec}(\mathcal A(V)).
\]

## Formal Statement
**Theorem (Matrix-free PCG for RKHS mode-\(k\) subproblem).** Under the assumptions in Section **Assumptions**:

1. \(A\) is SPD, so PCG is well-defined.
2. For any \(V\), \(A\mathrm{vec}(V)\) can be computed exactly without forming \(A\), \(Z\otimes K\), or any \(N\)-scale object via:
\[
U=KV,\ \ 
\alpha_t=\langle U_{i_t,:},z_t\rangle,\ \ 
C_{i_t,:}\mathrel{+}=\alpha_t z_t,\ \ 
Y=KC+\lambda U,
\]
return \(\mathrm{vec}(Y)\).
3. The per-matvec complexity is \(O(qr+n^2r)\) (plus \(O(q(d-1)r)\) if each \(z_t\) is generated on-the-fly from CP factors).
4. With preconditioner
\[
P=((G_\Omega+\lambda I_r)\otimes (K+\delta I_n)),\quad
G_\Omega:=\sum_{t=1}^q z_tz_t^\top,\ \delta>0,
\]
each \(P^{-1}\)-apply costs \(O(n^2r+nr^2)\) after setup \(O(qr^2+n^3+r^3)\).
5. PCG iterates satisfy
\[
\|e_m\|_A\le 2\Big(\frac{\sqrt\kappa-1}{\sqrt\kappa+1}\Big)^m\|e_0\|_A,\quad
\kappa=\kappa(P^{-1/2}AP^{-1/2}),
\]
hence total cost
\[
O\!\left(m(qr+n^2r+nr^2)+qr^2+n^3+r^3\right),
\]
with no \(O(N)\) operation.

## Assumptions
1. \(\lambda>0\).
2. \(K\succ 0\) for strict SPD proof of \(A\); if original kernel is only PSD, either add nugget to the model kernel or solve on \(\mathrm{range}(K)\).
3. \(n,r<q\ll N\), and observed tuples \((i_t,m_t,y_t)\) are directly available.
4. Indexing conventions for unfolding, vectorization, and \(S\) are consistent.
5. For preconditioner quality claims (small \(\kappa\)), observation pattern is not pathologically imbalanced across \(i_t\) and rank directions.

## Notation
- \(n=n_k\): size of mode \(k\).
- \(M=\prod_{i\neq k} n_i\): product of non-\(k\) dimensions.
- \(N=nM\): total entries.
- \(q=|\Omega|\): number of observed entries.
- \(Z\in\mathbb{R}^{M\times r}\): Khatri-Rao product of fixed factors.
- \(K\in\mathbb{R}^{n\times n}\): kernel matrix.
- \(W\in\mathbb{R}^{n\times r}\), \(w=\mathrm{vec}(W)\): unknown.
- \(B=TZ\in\mathbb{R}^{n\times r}\): sparse MTTKRP result.
- \(z_t=Z_{m_t,:}\): sampled Khatri-Rao row for observation \(t\).
- \(P_\Omega=SS^\top\): selection projector in vectorized unfolding space.
- \(G_\Omega=\sum_t z_tz_t^\top\in\mathbb{R}^{r\times r}\): sampled rank-Gram.

## High-Level Strategy
1. Rewrite the Hessian action as a matrix operator \(\mathcal A(V)\) using \((C\otimes A)\mathrm{vec}(B)=\mathrm{vec}(AB C^\top)\).
2. Implement \(\mathcal A(V)\) from observed entries only, via rowwise accumulation \(C_{i_t,:} \!+\!= \alpha_t z_t\), never forming \(P_\Omega\), \(Z\otimes K\), or dense \(T\).
3. Compute RHS sparsely: \(B_{i_t,:}\!+\!=y_t z_t\), then \(b=\mathrm{vec}(KB)\).
4. Run PCG on \(Aw=b\) with Kronecker preconditioner \(P\) that is cheap to invert by two small Cholesky-based solves.
5. Prove complexity by summing matvec cost, preconditioner-apply cost, and one-time setup.

## Key Lemmas
1. **Kronecker-vec masked operator lemma:** \(A\,\mathrm{vec}(V)=\mathrm{vec}(\mathcal A(V))\).
2. **Sampled accumulation lemma:** \(C=(\mathbf{1}_\Omega\odot(KVZ^\top))Z\) is computed exactly by the \(q\)-sample loop in \(O(qr)\).
3. **Sparse RHS lemma:** \(B=TZ\) is computed from observations in \(O(qr)\), then \(b\) in \(O(n^2r)\).
4. **SPD lemma:** if \(K\succ0,\lambda>0\), then \(A\succ0\) because
\[
x^\top A x=\|P_\Omega^{1/2}(Z\otimes K)x\|_2^2+\lambda\|(I_r\otimes K^{1/2})x\|_2^2>0.
\]
5. **Preconditioner-apply lemma:** \(P^{-1}\mathrm{vec}(R)\) equals two solves:
\[
(K+\delta I)Y=R,\quad X(G_\Omega+\lambda I)=Y,\quad \mathrm{vec}(X)=P^{-1}\mathrm{vec}(R).
\]
6. **PCG convergence lemma:** standard SPD PCG bound with \(\kappa(P^{-1/2}AP^{-1/2})\).

## Dependency Graph
1. Definitions + Notation \(\rightarrow\) Lemma 1.
2. Lemma 1 \(\rightarrow\) Lemma 2 (matvec algorithm correctness).
3. Definitions \(\rightarrow\) Lemma 3 (RHS correctness/cost).
4. Assumptions 1-2 \(\rightarrow\) Lemma 4 (SPD of \(A\)).
5. Definitions \(\rightarrow\) Lemma 5 (preconditioner form and apply rule).
6. Lemma 4 + Lemma 5 + Lemma 6 \(\rightarrow\) PCG convergence claim.
7. Lemma 2 + Lemma 3 + Lemma 5 \(\rightarrow\) total complexity claim in theorem.

## Risky Steps
1. **PSD kernel issue:** if \(K\) is singular, \(A\) may be singular despite \(\lambda>0\); this must be handled explicitly.
2. **Index consistency:** mismatch between unfolding order, \(S\), and sampled \((i_t,m_t)\) invalidates the operator.
3. **Preconditioner quality claim:** spectral-equivalence constants require sampling-balance assumptions; do not claim uniformly good \(\kappa\) without them.
4. **Hidden \(M\)-scale cost:** forming full \(Z\) is infeasible when \(M\) is huge; use only sampled rows \(z_t\) (cached or on-demand).
5. **Numerical stability:** very small \(\lambda\) or ill-conditioned \(K\) can cause PCG stagnation; monitor relative preconditioned residual and cap iterations.

### Prover Agent

## Complete Proof

We prove the theorem in five parts, corresponding to claims (1)–(5) of the formal statement. Throughout, we use the notation and assumptions established by the mentor.

**Part 1: \(A\) is symmetric positive definite.**

Recall
\[
A = (Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda (I_r \otimes K).
\]

Since \(P_\Omega = S S^\top\) is a projection matrix (hence PSD), the first term \((Z \otimes K)^\top S S^\top (Z \otimes K)\) is PSD for any \(Z, K\). The second term \(\lambda (I_r \otimes K)\) is SPD when \(\lambda > 0\) and \(K \succ 0\), since eigenvalues of \(I_r \otimes K\) are exactly the eigenvalues of \(K\) each with multiplicity \(r\), all positive. Thus \(A\) is the sum of a PSD matrix and an SPD matrix, hence SPD. In particular, for any \(x \neq 0\):
\[
x^\top A x = \|S^\top (Z \otimes K) x\|_2^2 + \lambda \, x^\top (I_r \otimes K) x > 0,
\]
where the strict inequality follows because \(x^\top (I_r \otimes K) x > 0\) for all \(x \neq 0\) when \(K \succ 0\). Since \(A\) is SPD, the PCG iteration is well-defined and convergent.

**Part 2: Matrix-free matvec via the operator \(\mathcal{A}(V)\).**

We must show that \(A \operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))\) where
\[
\mathcal{A}(V) = K\!\left((\mathbf{1}_\Omega \odot (K V Z^\top)) Z + \lambda V\right),
\]
and that this is implemented without forming any \(N\)-scale object by the four-step procedure:
\[
U = KV, \quad \alpha_t = \langle U_{i_t,:}, z_t \rangle, \quad C_{i_t,:} \mathrel{+}= \alpha_t \, z_t, \quad Y = KC + \lambda U.
\]

The proof proceeds by expanding \(A \operatorname{vec}(V)\) and applying the Kronecker-vec identity. See Lemma 1 below for the algebraic verification, and Lemma 2 for the equivalence of the sampled accumulation loop to the masked matrix product.

**Part 3: Per-matvec complexity is \(O(qr + n^2 r)\).**

Each application of \(\mathcal{A}\) involves:
- Computing \(U = KV\): matrix-matrix multiply \(K \in \mathbb{R}^{n \times n}\) times \(V \in \mathbb{R}^{n \times r}\), cost \(O(n^2 r)\).
- The accumulation loop over \(t = 1, \ldots, q\): computing \(\alpha_t = \langle U_{i_t,:}, z_t \rangle\) costs \(O(r)\) per observation, and updating \(C_{i_t,:} \mathrel{+}= \alpha_t z_t\) costs \(O(r)\) per observation, for a total of \(O(qr)\). (Initialize \(C = 0 \in \mathbb{R}^{n \times r}\) in \(O(nr)\).)
- Computing \(Y = KC + \lambda U\): one more \(n \times n\) by \(n \times r\) multiply plus an elementwise scaled addition, cost \(O(n^2 r)\).

Total: \(O(n^2 r + qr + n^2 r) = O(qr + n^2 r)\), since \(nr \leq qr\) by assumption \(n < q\).

If the rows \(z_t\) are not precomputed but generated on-the-fly from the \(d-1\) CP factor matrices via elementwise products \(z_t = A_1(j_1^{(t)},:) \odot A_2(j_2^{(t)},:) \odot \cdots\) (omitting factor \(k\)), each \(z_t\) costs \(O((d-1)r)\), adding \(O(q(d-1)r)\) to the total.

**Part 4: Preconditioner application cost.**

The Kronecker preconditioner is
\[
P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n),
\]
where \(G_\Omega = \sum_{t=1}^q z_t z_t^\top \in \mathbb{R}^{r \times r}\) and \(\delta > 0\).

**Setup (one-time):**
- Form \(G_\Omega\): accumulate \(q\) rank-1 updates \(z_t z_t^\top\), each costing \(O(r^2)\), total \(O(qr^2)\).
- Cholesky factor \(G_\Omega + \lambda I_r \in \mathbb{R}^{r \times r}\): cost \(O(r^3)\).
- Cholesky factor \(K + \delta I_n \in \mathbb{R}^{n \times n}\): cost \(O(n^3)\).

Total setup: \(O(qr^2 + n^3 + r^3)\).

**Per-apply:** Given residual \(R \in \mathbb{R}^{n \times r}\), compute \(X\) such that \(\operatorname{vec}(X) = P^{-1} \operatorname{vec}(R)\). By the Kronecker inverse identity \(P^{-1} = (G_\Omega + \lambda I_r)^{-1} \otimes (K + \delta I_n)^{-1}\), we have (see Lemma 5):
\[
(K + \delta I_n) \, Y_{\cdot, j} = R_{\cdot, j} \quad \text{for } j = 1, \ldots, r,
\]
then
\[
(G_\Omega + \lambda I_r) \, X_{i, \cdot}^\top = Y_{i, \cdot}^\top \quad \text{for } i = 1, \ldots, n.
\]

The first step solves \(r\) systems of size \(n\) using the precomputed Cholesky factor: \(O(n^2 r)\) (forward-backward substitution costs \(O(n^2)\) per right-hand side). The second step solves \(n\) systems of size \(r\): \(O(n r^2)\). Total per-apply: \(O(n^2 r + n r^2)\).

**Part 5: PCG convergence and total complexity.**

Since \(A \succ 0\) (Part 1) and \(P \succ 0\) (both Kronecker factors are SPD: \(K + \delta I \succ 0\) trivially, and \(G_\Omega + \lambda I \succ 0\) since \(\lambda > 0\)), the preconditioned system \(P^{-1/2} A P^{-1/2}\) is SPD with condition number \(\kappa = \kappa(P^{-1/2} A P^{-1/2})\).

By the standard PCG convergence theorem (see Lemma 6), the \(A\)-norm error satisfies:
\[
\|e_m\|_A \leq 2 \left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^m \|e_0\|_A.
\]

To achieve \(\|e_m\|_A / \|e_0\|_A \leq \epsilon\), it suffices to take \(m = O(\sqrt{\kappa} \log(1/\epsilon))\) iterations.

Each PCG iteration requires one matvec (\(O(qr + n^2 r)\) by Part 3) and one preconditioner apply (\(O(n^2 r + nr^2)\) by Part 4). The total cost for \(m\) iterations is therefore:
\[
O\!\left(m(qr + n^2 r + n r^2) + qr^2 + n^3 + r^3\right),
\]
where the second group is the one-time preconditioner setup. Since \(n, r < q \ll N\), no operation scales as \(O(N)\). \(\blacksquare\)

---

## Lemma Proofs

### Lemma 1: Kronecker-vec masked operator identity

**Statement.** For any \(V \in \mathbb{R}^{n \times r}\),
\[
A \operatorname{vec}(V) = \operatorname{vec}\!\left(K\big((\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V\big)\right).
\]

**Proof.** We expand each term in \(A \operatorname{vec}(V) = \big[(Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda (I_r \otimes K)\big] \operatorname{vec}(V)\).

*Regularization term.* By the standard Kronecker-vec identity \((C \otimes A) \operatorname{vec}(B) = \operatorname{vec}(ABC^\top)\):
\[
\lambda (I_r \otimes K) \operatorname{vec}(V) = \lambda \operatorname{vec}(K V I_r^\top) = \lambda \operatorname{vec}(KV).
\]

*Data-fit term.* First, \((Z \otimes K) \operatorname{vec}(V) = \operatorname{vec}(KVZ^\top)\). This is a vector in \(\mathbb{R}^{nM}\), which we can identify with the matrix \(KVZ^\top \in \mathbb{R}^{n \times M}\).

The projector \(P_\Omega\) acts on vectorized \(n \times M\) matrices by zeroing out entries not in \(\Omega\). In matrix form, \(P_\Omega \operatorname{vec}(X) = \operatorname{vec}(\mathbf{1}_\Omega \odot X)\) where \(\mathbf{1}_\Omega\) is the binary mask and \(\odot\) is the Hadamard (entrywise) product. To verify: if \((i, m) \in \Omega\), the corresponding entry of \(\operatorname{vec}(X)\) is kept; otherwise it is zeroed. This is exactly what the Hadamard product with \(\mathbf{1}_\Omega\) does.

So:
\[
P_\Omega (Z \otimes K) \operatorname{vec}(V) = P_\Omega \operatorname{vec}(KVZ^\top) = \operatorname{vec}(\mathbf{1}_\Omega \odot (KVZ^\top)).
\]

Now apply \((Z \otimes K)^\top\). Using the identity \((Z \otimes K)^\top \operatorname{vec}(X) = (Z^\top \otimes K^\top) \operatorname{vec}(X) = \operatorname{vec}(K^\top X Z) = \operatorname{vec}(KXZ)\) (since \(K\) is symmetric, \(K^\top = K\)):
\[
(Z \otimes K)^\top \operatorname{vec}(\mathbf{1}_\Omega \odot (KVZ^\top)) = \operatorname{vec}\!\left(K \big(\mathbf{1}_\Omega \odot (KVZ^\top)\big) Z\right).
\]

Combining:
\[
A \operatorname{vec}(V) = \operatorname{vec}\!\left(K(\mathbf{1}_\Omega \odot (KVZ^\top))Z\right) + \lambda \operatorname{vec}(KV) = \operatorname{vec}\!\left(K\big[(\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V\big]\right).
\]

This equals \(\operatorname{vec}(\mathcal{A}(V))\). \(\square\)

### Lemma 2: Sampled accumulation correctness

**Statement.** The matrix \(C := (\mathbf{1}_\Omega \odot (KVZ^\top)) Z \in \mathbb{R}^{n \times r}\) is computed exactly by:
1. Set \(U = KV\), \(C = 0\).
2. For \(t = 1, \ldots, q\): compute \(\alpha_t = \langle U_{i_t,:}, z_t \rangle\) and update \(C_{i_t,:} \mathrel{+}= \alpha_t z_t\).

The cost is \(O(qr)\) (excluding the \(O(n^2 r)\) cost of forming \(U\)).

**Proof.** We must show that the loop produces the same matrix \(C\) as the formula \((\mathbf{1}_\Omega \odot (UZ^\top))Z\), where \(U = KV\).

Consider the \((i, m)\)-entry of \(UZ^\top\):
\[
(UZ^\top)_{i,m} = \sum_{s=1}^r U_{i,s} Z_{m,s} = \langle U_{i,:}, Z_{m,:} \rangle.
\]

After masking, \((\mathbf{1}_\Omega \odot (UZ^\top))_{i,m} = \langle U_{i,:}, Z_{m,:} \rangle\) if \((i, m) \in \Omega\) and \(0\) otherwise.

The \(i\)-th row of \(C = (\mathbf{1}_\Omega \odot (UZ^\top))Z\) is:
\[
C_{i,:} = \sum_{m=1}^M (\mathbf{1}_\Omega)_{i,m} \langle U_{i,:}, Z_{m,:} \rangle \, Z_{m,:} = \sum_{\{t : i_t = i\}} \langle U_{i,:}, z_t \rangle \, z_t = \sum_{\{t : i_t = i\}} \alpha_t \, z_t,
\]
where the second equality uses the fact that the nonzero contributions come precisely from the observations \((i_t, m_t) \in \Omega\) with \(i_t = i\), and \(Z_{m_t,:} = z_t\).

This is exactly what the accumulation loop computes: for each \(t\), it adds \(\alpha_t z_t\) to row \(i_t\) of \(C\). The order of summation is immaterial since addition is commutative and associative (in exact arithmetic).

Each of the \(q\) iterations performs an inner product (\(O(r)\)) and a scaled vector addition (\(O(r)\)), giving total cost \(O(qr)\). \(\square\)

### Lemma 3: Sparse RHS computation

**Statement.** The right-hand side \(b = \operatorname{vec}(KB)\) where \(B = T_{(k)} Z \in \mathbb{R}^{n \times r}\) (with \(T_{(k)}\) the mode-\(k\) unfolding) can be computed from the observed entries alone in \(O(qr)\), and then \(b = \operatorname{vec}(KB)\) costs \(O(n^2 r)\).

**Proof.** The \((i, s)\)-entry of \(B = T_{(k)} Z\) is:
\[
B_{i,s} = \sum_{m=1}^M (T_{(k)})_{i,m} Z_{m,s}.
\]

Since only entries \((i_t, m_t) \in \Omega\) of \(T_{(k)}\) are observed (with values \(y_t\)), and unobserved entries do not contribute to the reconstruction problem (they are not part of the data-fit term), the sparse MTTKRP computes:
\[
B_{i,s} = \sum_{\{t : i_t = i\}} y_t \, (z_t)_s.
\]

This is implemented by initializing \(B = 0\) and, for each \(t = 1, \ldots, q\), updating \(B_{i_t,:} \mathrel{+}= y_t z_t\). Each update costs \(O(r)\), so the total is \(O(qr)\).

Then \(b = \operatorname{vec}(KB)\) requires the matrix product \(KB\) (\(n \times n\) times \(n \times r\)), costing \(O(n^2 r)\). \(\square\)

**Remark on RHS consistency.** We verify that this \(b\) matches the definition \(b = (I_r \otimes K) \operatorname{vec}(B)\). Indeed, \((I_r \otimes K) \operatorname{vec}(B) = \operatorname{vec}(K B I_r^\top) = \operatorname{vec}(KB)\), consistent with the mentor's definition.

### Lemma 4: SPD property of \(A\)

**Statement.** Under Assumptions 1–2 (\(\lambda > 0\), \(K \succ 0\)), \(A \succ 0\).

**Proof.** For any nonzero \(x \in \mathbb{R}^{nr}\):
\[
x^\top A x = x^\top (Z \otimes K)^\top P_\Omega (Z \otimes K) x + \lambda \, x^\top (I_r \otimes K) x.
\]

The first term is \(\|S^\top (Z \otimes K) x\|_2^2 \geq 0\).

For the second term, let \(V \in \mathbb{R}^{n \times r}\) be the matricization of \(x\), so \(x = \operatorname{vec}(V)\). Then:
\[
x^\top (I_r \otimes K) x = \operatorname{vec}(V)^\top (I_r \otimes K) \operatorname{vec}(V) = \operatorname{vec}(V)^\top \operatorname{vec}(KV) = \operatorname{tr}(V^\top K V).
\]

Since \(K \succ 0\), we can write \(K = K^{1/2} K^{1/2}\) (symmetric positive definite square root), so:
\[
\operatorname{tr}(V^\top K V) = \operatorname{tr}(V^\top K^{1/2} K^{1/2} V) = \|K^{1/2} V\|_F^2.
\]

If \(x \neq 0\) then \(V \neq 0\), and since \(K^{1/2}\) is nonsingular, \(K^{1/2} V \neq 0\), so \(\|K^{1/2} V\|_F^2 > 0\). Therefore:
\[
x^\top A x \geq 0 + \lambda \|K^{1/2} V\|_F^2 > 0.
\]

Hence \(A \succ 0\). \(\square\)

**Remark on the PSD case.** If \(K\) is only PSD (i.e., has a nontrivial nullspace), then \(I_r \otimes K\) is only PSD, and the regularization term \(\lambda x^\top (I_r \otimes K) x\) can be zero for \(x \in \ker(I_r \otimes K) = \mathbb{R}^r \otimes \ker(K)\). In this case, \(A\) is SPD if and only if \((Z \otimes K)^\top P_\Omega (Z \otimes K)\) is positive definite on \(\ker(I_r \otimes K)\), which is not guaranteed in general. The remedy is either: (a) add a nugget \(\epsilon I_n\) to \(K\) so that \(K + \epsilon I \succ 0\), which is standard in kernel methods; or (b) restrict the problem to \(\operatorname{range}(K)\) by projecting out the nullspace. Both approaches are noted in Assumption 2.

### Lemma 5: Preconditioner apply via two triangular solves

**Statement.** With \(P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)\), the action \(\operatorname{vec}(X) = P^{-1} \operatorname{vec}(R)\) for \(R \in \mathbb{R}^{n \times r}\) is computed by:
1. Solve \((K + \delta I_n) Y = R\) (i.e., solve for each column of \(Y\)).
2. Solve \(X (G_\Omega + \lambda I_r) = Y\) (i.e., solve for each row of \(X\)).

The cost is \(O(n^2 r + nr^2)\) per apply after one-time Cholesky factorizations costing \(O(n^3 + r^3)\).

**Proof.** By the Kronecker product inverse identity, for invertible \(A, B\):
\[
(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}.
\]

Applied here:
\[
P^{-1} = (G_\Omega + \lambda I_r)^{-1} \otimes (K + \delta I_n)^{-1}.
\]

Both factors are SPD (\(G_\Omega \succeq 0\) and \(\lambda > 0\) give \(G_\Omega + \lambda I_r \succ 0\); \(K \succ 0\) by assumption and \(\delta > 0\) gives \(K + \delta I_n \succ 0\)), hence invertible.

Using the Kronecker-vec identity:
\[
P^{-1} \operatorname{vec}(R) = \big((G_\Omega + \lambda I_r)^{-1} \otimes (K + \delta I_n)^{-1}\big) \operatorname{vec}(R) = \operatorname{vec}\!\left((K + \delta I_n)^{-1} R \, (G_\Omega + \lambda I_r)^{-\top}\right).
\]

Since \(G_\Omega + \lambda I_r\) is symmetric, \((G_\Omega + \lambda I_r)^{-\top} = (G_\Omega + \lambda I_r)^{-1}\). So:
\[
X = (K + \delta I_n)^{-1} R \, (G_\Omega + \lambda I_r)^{-1}.
\]

This factors as:
- Step 1: \(Y = (K + \delta I_n)^{-1} R\), i.e., solve \((K + \delta I_n) Y = R\). This requires \(r\) solves of size \(n\), each costing \(O(n^2)\) given the Cholesky factor, for a total of \(O(n^2 r)\).
- Step 2: \(X = Y (G_\Omega + \lambda I_r)^{-1}\), i.e., solve \(X (G_\Omega + \lambda I_r) = Y\), equivalently \((G_\Omega + \lambda I_r) X^\top = Y^\top\). This requires \(n\) solves of size \(r\), each costing \(O(r^2)\) given the Cholesky factor, for a total of \(O(nr^2)\).

Total per-apply: \(O(n^2 r + nr^2)\). The one-time Cholesky factorizations cost \(O(n^3)\) for \(K + \delta I_n\) and \(O(r^3)\) for \(G_\Omega + \lambda I_r\). \(\square\)

### Lemma 6: PCG convergence bound

**Statement.** The preconditioned CG iterates for the SPD system \(Aw = b\) with SPD preconditioner \(P\) satisfy:
\[
\|e_m\|_A \leq 2 \left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^m \|e_0\|_A, \quad \kappa = \kappa(P^{-1/2} A P^{-1/2}).
\]

**Proof.** This is the standard result from the theory of the conjugate gradient method applied to the preconditioned system. We include the argument for completeness.

PCG applied to \(Aw = b\) with preconditioner \(P\) is mathematically equivalent to standard CG applied to the transformed system \(\hat{A} \hat{w} = \hat{b}\), where \(\hat{A} = P^{-1/2} A P^{-1/2}\), \(\hat{w} = P^{1/2} w\), and \(\hat{b} = P^{-1/2} b\).

By the CG optimality property, the \(m\)-th iterate \(\hat{w}_m\) minimizes the \(\hat{A}\)-norm of the error over the Krylov subspace:
\[
\hat{w}_m = \arg\min_{\hat{w} \in \hat{w}_0 + \mathcal{K}_m(\hat{A}, \hat{r}_0)} \|\hat{w} - \hat{w}_*\|_{\hat{A}},
\]
where \(\hat{r}_0 = \hat{b} - \hat{A}\hat{w}_0\) and \(\hat{w}_* = \hat{A}^{-1}\hat{b}\).

Equivalently, \(\hat{e}_m = \hat{w}_m - \hat{w}_*\) satisfies:
\[
\|\hat{e}_m\|_{\hat{A}} = \min_{p \in \mathcal{P}_m,\, p(0)=1} \|p(\hat{A}) \hat{e}_0\|_{\hat{A}} \leq \min_{p \in \mathcal{P}_m,\, p(0)=1} \max_{\mu \in \sigma(\hat{A})} |p(\mu)| \cdot \|\hat{e}_0\|_{\hat{A}},
\]
where \(\mathcal{P}_m\) denotes polynomials of degree \(\leq m\).

The eigenvalues of \(\hat{A}\) lie in \([\lambda_{\min}(\hat{A}), \lambda_{\max}(\hat{A})]\). Choosing the Chebyshev polynomial of degree \(m\) on this interval, scaled to satisfy \(p(0) = 1\):
\[
\min_{p \in \mathcal{P}_m,\, p(0)=1} \max_{\mu \in [\lambda_{\min}, \lambda_{\max}]} |p(\mu)| \leq \frac{2}{1 + (T_m(\frac{\lambda_{\max}+\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}))^{-1}} \leq 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m,
\]
where \(\kappa = \lambda_{\max}/\lambda_{\min}\) and \(T_m\) is the Chebyshev polynomial of the first kind. The last inequality uses the bound \(T_m(x) \geq \frac{1}{2}\left(x + \sqrt{x^2-1}\right)^m\) for \(x \geq 1\), applied with \(x = (\kappa+1)/(\kappa-1)\).

Now we relate back to the original variables. The \(A\)-norm of the original error satisfies:
\[
\|e_m\|_A^2 = e_m^\top A e_m = (P^{-1/2} A P^{-1/2} \hat{e}_m)^\top \cdot \ldots
\]

More directly: since \(e_m = P^{-1/2} \hat{e}_m\), we have
\[
\|e_m\|_A^2 = \hat{e}_m^\top P^{-1/2} A P^{-1/2} \hat{e}_m = \|\hat{e}_m\|_{\hat{A}}^2.
\]

Therefore:
\[
\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}} \leq 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m \|\hat{e}_0\|_{\hat{A}} = 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m \|e_0\|_A. \quad \square
\]

---

## Gap Closure Notes

1. **Symmetry of \(K\).** Throughout, the Kronecker-vec identity \((C \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)\) is applied with \(A = K\). In Lemma 1, we use \((Z \otimes K)^\top = (Z^\top \otimes K^\top) = (Z^\top \otimes K)\), which requires \(K = K^\top\). This holds because \(K\) is a kernel matrix (Gram matrix of kernel evaluations), hence symmetric by construction. This symmetry is implicit in the mentor's setup but is used critically in the algebra.

2. **Projector \(P_\Omega\) acts as Hadamard masking.** The mentor defines \(P_\Omega = SS^\top\) as a projector in the vectorized space \(\mathbb{R}^{nM}\). We identified its action with Hadamard masking: \(P_\Omega \operatorname{vec}(X) = \operatorname{vec}(\mathbf{1}_\Omega \odot X)\). This holds because \(S \in \mathbb{R}^{nM \times q}\) has columns \(e_{(i_t, m_t)}\) (standard basis vectors indexed by the vectorization of position \((i_t, m_t)\)), so \(SS^\top\) is diagonal with ones at observed positions and zeros elsewhere. This is precisely the action of Hadamard multiplication by \(\mathbf{1}_\Omega\) under the natural bijection between \(\mathbb{R}^{nM}\) and \(\mathbb{R}^{n \times M}\).

3. **Commutativity of accumulation order.** In Lemma 2, we noted that the accumulation loop \(C_{i_t,:} \mathrel{+}= \alpha_t z_t\) produces the correct result regardless of the order in which observations are processed. This relies on exact arithmetic (commutativity and associativity of real addition). In finite-precision arithmetic, the result is subject to round-off but remains a faithful approximation, with errors bounded by standard floating-point accumulation analysis (\(O(q \epsilon_{\text{mach}})\) relative error per entry under no catastrophic cancellation).

4. **Well-definedness of the Cholesky factors.** The preconditioner-apply procedure (Lemma 5) requires Cholesky factorizations of \(K + \delta I_n\) and \(G_\Omega + \lambda I_r\). The former is SPD because \(K \succeq 0\) (PSD kernel matrix) and \(\delta > 0\). The latter is SPD because \(G_\Omega = \sum_t z_t z_t^\top \succeq 0\) and \(\lambda > 0\). Both Cholesky factorizations are therefore well-defined and numerically stable (with condition numbers bounded by those of the respective matrices).

5. **Consistency of the RHS definition.** The mentor defines \(b = (I_r \otimes K)\operatorname{vec}(B)\) with \(B = TZ\). In the sparse setting, \(T\) is not fully available; only the entries \((T_{(k)})_{i_t, m_t} = y_t\) are observed. Lemma 3 computes \(B\) using only these entries. This is consistent with the optimization objective, which penalizes fit only at observed entries (the data-fit term involves \(P_\Omega\)). Formally, the normal equations for
\[
\min_w \frac{1}{2}\|P_\Omega(\operatorname{vec}(T_{(k)}) - (Z \otimes K)w)\|^2 + \frac{\lambda}{2} w^\top (I_r \otimes K) w
\]
yield \(Aw = b\) with the sparse \(B\) exactly as computed in Lemma 3, since the unobserved entries of \(T_{(k)}\) are annihilated by \(P_\Omega\) in the gradient.

6. **No hidden \(O(N)\) costs.** We verify that no step requires \(O(N)\) storage or computation:
   - The full tensor \(T\) and its unfolding \(T_{(k)} \in \mathbb{R}^{n \times M}\) are never formed; only the \(q\) observed triples are used.
   - The full Khatri-Rao product \(Z \in \mathbb{R}^{M \times r}\) is never formed; only the sampled rows \(z_t \in \mathbb{R}^r\) (for \(t = 1, \ldots, q\)) are accessed.
   - The Kronecker product \(Z \otimes K \in \mathbb{R}^{nM \times nr}\) and the projector \(P_\Omega \in \mathbb{R}^{nM \times nM}\) are never formed.
   - All matrices that are explicitly formed have dimensions involving only \(n, r, q\) (specifically: \(K \in \mathbb{R}^{n \times n}\), \(V, U, C, B, Y \in \mathbb{R}^{n \times r}\), \(G_\Omega \in \mathbb{R}^{r \times r}\)), none of which is \(O(N)\) under the assumption \(n, r < q \ll N\).

7. **Preconditioner quality and the role of Assumption 5.** The convergence rate depends on \(\kappa = \kappa(P^{-1/2} A P^{-1/2})\). The preconditioner \(P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)\) approximates \(A\) well when the observation pattern is reasonably balanced. Specifically, if observations are distributed such that the per-row sampling density is approximately uniform, then \((\mathbf{1}_\Omega \odot (X)) Z \approx \frac{q}{nM} X Z\) for smooth \(X\), making \(A \approx \frac{q}{nM} (Z^\top Z \otimes K^2) + \lambda (I_r \otimes K)\), which is spectrally close to a scalar multiple of \(P\) with appropriate \(\delta\). The precise spectral equivalence constants depend on the sampling distribution and are not derived here; Assumption 5 (no pathological imbalance) ensures that \(\kappa\) remains moderate. Without this assumption, adversarial observation patterns can make \(\kappa\) arbitrarily large, degrading convergence.

### Editor Dispatch

**Reasoning:** codex_cli is strongest for strict step-by-step logical validation and detecting hidden gaps; claude_code is best for improving notation discipline, readability, and graduate-level presentation rigor; gemini_api is best matched to validating cited results, theorem statements, and hypothesis compliance.

- Correctness & Completeness -> codex_cli
- Clarity & Rigor -> claude_code
- Reference Validity -> gemini_api

### Reviews

#### Correctness & Completeness (by codex_cli)

Overall, the core linear-algebra steps are mostly sound (SPD argument, matrix-free matvec identity, and preconditioner apply/cost). The main correctness gap is the RHS definition consistency.

```json
{
  "issues": [
    {
      "severity": "major",
      "location": "Lemma 3 / RHS definition (and theorem definitions of b, B)",
      "reason": "The proof implicitly replaces B = T_{(k)}Z with a sparse observed-only accumulation, but that is only equal if T is explicitly the masked tensor T_Ω := 1_Ω ⊙ T (or unobserved entries are defined as zero). As written, if T denotes the full tensor, the computed B is not T_{(k)}Z, so b is mismatched.",
      "required_fix": "State explicitly that the objective uses T_Ω (observed entries only), redefine B as B := T_Ω Z, and derive b from normal equations as b = (Z ⊗ K)^T P_Ω vec(T) = vec(KB).",
      "suggestion": "Add a short derivation from the masked least-squares objective to the normal equations so the sparse accumulation formula for B is unambiguous and exact."
    },
    {
      "severity": "minor",
      "location": "Lemma 6 (PCG convergence proof)",
      "reason": "One displayed line is incomplete (`(P^{-1/2} A P^{-1/2} \\hat e_m)^T \\cdot ...`), and the Chebyshev minimax bound is written in a nonstandard form without justification. This weakens rigor at graduate level.",
      "required_fix": "Replace that segment with a clean standard statement: CG on \\hat A = P^{-1/2}AP^{-1/2} gives \\|e_m\\|_A = \\|\\hat e_m\\|_{\\hat A} and the textbook bound with κ(\\hat A).",
      "suggestion": "Either cite a standard theorem directly or provide the exact minimax identity \\(\\min_{p(0)=1}\\max_{\\lambda\\in[a,b]}|p(\\lambda)| = 1/|T_m((a+b)/(b-a))|\\) before the final inequality."
    },
    {
      "severity": "minor",
      "location": "Gap Closure Note 7 (preconditioner quality discussion)",
      "reason": "The claim that balanced sampling makes A spectrally close to a Kronecker form is heuristic and not proved (no sampling model, no concentration/spectral bounds).",
      "required_fix": "Either remove this as a proof claim or formalize a sampling assumption (e.g., i.i.d. Bernoulli/uniform without replacement) and prove explicit spectral-equivalence bounds.",
      "suggestion": "Keep the theorem’s unconditional PCG bound in terms of κ, and move small-κ intuition to a separate remark labeled as heuristic unless a formal lemma is added."
    }
  ],
  "residual_concerns": [
    "Uniqueness of observed pairs is implicitly used via Ω as a set; if duplicate observations are allowed, define how they are aggregated and how S, P_Ω are modified."
  ]
}
```

#### Clarity & Rigor (by claude_code)

## Clarity & Rigor Review

The proof is well-structured and covers all five claims of the theorem systematically. The lemma decomposition follows the dependency graph cleanly, and the gap closure notes address several implicit assumptions. However, there are issues of varying severity related to notational precision, missing justifications, and exposition clarity that would need attention for graduate-level peer review.

### Detailed Assessment

**Strengths:**
- The overall architecture (5-part proof + 6 lemmas + gap closure) is logical and easy to follow.
- Lemma 1 (Kronecker-vec masked operator) is proved carefully with each step justified.
- Lemma 4 (SPD property) is clean and uses the Frobenius norm characterization well.
- Gap closure notes 1–2 address genuine implicit assumptions (symmetry of K, projector-as-masking).
- The PSD remark after Lemma 4 is a valuable discussion of the boundary case.

**Issues identified below:**

---

**1. Chebyshev bound in Lemma 6 has an incorrect intermediate expression.** The standard Chebyshev minimax bound on $[\lambda_{\min}, \lambda_{\max}]$ is $\|e_m\|_A / \|e_0\|_A \leq 2 \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m$, derived via $1/T_m\!\bigl(\frac{\kappa+1}{\kappa-1}\bigr)$. The displayed intermediate fraction involving $1 + (T_m(\cdots))^{-1}$ in the denominator does not correspond to any standard form of this bound and is dimensionally suspect — the correct minimax value is simply $2/T_m\!\bigl(\frac{\kappa+1}{\kappa-1}\bigr)$ when properly normalized, not the expression shown.

**2. The relation $e_m = P^{-1/2}\hat{e}_m$ in Lemma 6 is stated without derivation.** The proof asserts "since $e_m = P^{-1/2}\hat{e}_m$" but this requires showing that the PCG iterates in the original space relate to CG iterates in the transformed space by exactly this scaling. The substitution $\hat{w} = P^{1/2}w$ gives $w_m = P^{-1/2}\hat{w}_m$ and $w_* = P^{-1/2}\hat{w}_*$, so $e_m = P^{-1/2}\hat{e}_m$ follows by linearity — but this step should be spelled out rather than left as "more directly."

**3. The definition of $B = TZ$ in the mentor's notation vs. $B = T_{(k)}Z$ in Lemma 3 is inconsistent.** The mentor writes $B = TZ$ while Lemma 3 writes $B = T_{(k)}Z$ where $T_{(k)}$ is the mode-$k$ unfolding. The tensor $T$ and its unfolding $T_{(k)}$ are different objects (one is order-$d$, the other is a matrix). Throughout the proof, $T$ appears to mean $T_{(k)}$ but this is never stated as a convention.

**4. Assumption 3 states $n, r < q$ but this is used asymmetrically.** The complexity argument in Part 3 claims "$nr \leq qr$ by assumption $n < q$" to simplify $O(n^2r + qr)$ to $O(qr + n^2r)$. But this simplification doesn't actually use $n < q$ — the two expressions are identical. The assumption $n < q$ would be needed to argue $O(nr) \subseteq O(qr)$ for the initialization cost, which is a minor point. More importantly, the relationship between $n^2r$ and $qr$ is never clarified: if $q < n^2$, the $n^2r$ term dominates, which is plausible but worth noting.

**5. The sparse MTTKRP in Lemma 3 tacitly assumes unobserved entries of $T_{(k)}$ are zero in the objective.** The gap closure note 5 partially addresses this but the argument is incomplete. It claims that unobserved entries "do not contribute to the reconstruction problem" and then derives normal equations. However, the objective function is stated only in gap closure note 5, not in the main proof body. For a self-contained proof, the optimization objective that gives rise to $Aw = b$ should appear early (in Part 1 or before), making it clear why only observed entries matter for $b$.

**6. Preconditioner setup cost for $G_\Omega$ omits the cost of accessing $z_t$ vectors.** Part 4 states that forming $G_\Omega$ costs $O(qr^2)$ by accumulating rank-1 updates. But if $z_t$ vectors are computed on-the-fly (as discussed in Part 3), each costs $O((d-1)r)$, making the true setup cost $O(q(d-1)r + qr^2)$. This should be stated consistently with Part 3's discussion.

**7. The notation $C_{i_t,:} \mathrel{+}=$ is algorithmic pseudocode, not mathematical notation.** While clear to anyone familiar with programming, this notation is non-standard in mathematical writing at graduate level. It appears in the theorem statement itself (claim 2), mixing algorithmic and mathematical registers.

**8. Gap closure note 7 makes qualitative claims about $\kappa$ without any formal bound.** The note discusses when $P$ approximates $A$ well but provides no theorem, no inequality, and no rate. The heuristic "$(\mathbf{1}_\Omega \odot X)Z \approx \frac{q}{nM}XZ$ for smooth $X$" is hand-wavy. This is acknowledged ("not derived here") but for graduate rigor, at minimum a reference to existing spectral equivalence results should be provided.

**9. Lemma 2 conflates $\mathbf{1}_\Omega \odot (KVZ^\top)$ with $\mathbf{1}_\Omega \odot (UZ^\top)$ without re-establishing the substitution.** The proof starts by saying "we must show the loop produces the same matrix $C$ as the formula $(\mathbf{1}_\Omega \odot (UZ^\top))Z$ where $U = KV$." This substitution is correct but should reference the definition of $\mathcal{A}(V)$ explicitly to close the logical chain — the reader must verify that $\mathcal{A}$ uses $KVZ^\top$ and that $U = KV$ makes this $UZ^\top$.

---

```json
{
  "issues": [
    {
      "severity": "major",
      "location": "Lemma 6, Chebyshev bound derivation",
      "reason": "The intermediate expression involving 2/(1 + T_m(...)^{-1}) is not a standard or correct form of the Chebyshev minimax bound. The correct minimax value is 2/T_m((kappa+1)/(kappa-1)), and the displayed formula does not reduce to this.",
      "required_fix": "Replace the intermediate bound with the standard form: min_{p} max_{mu} |p(mu)| = 1/T_m((kappa+1)/(kappa-1)) <= 2((sqrt(kappa)-1)/(sqrt(kappa)+1))^m.",
      "suggestion": "Use the standard two-step derivation: (1) the minimax polynomial on [lambda_min, lambda_max] with p(0)=1 has value 1/T_m((lambda_max+lambda_min)/(lambda_max-lambda_min)), (2) bound T_m from below using the exponential identity. Cite Golub & Van Loan or Saad's textbook for the clean statement."
    },
    {
      "severity": "major",
      "location": "Lemma 3 / overall proof structure",
      "reason": "The optimization objective that gives rise to the normal equations Aw = b is never stated in the main proof. The RHS b is defined algebraically but the reader cannot verify its correctness without knowing what is being minimized. This appears only in gap closure note 5, which is too late.",
      "required_fix": "State the optimization problem (regularized least-squares on observed entries) before Part 1, and derive the normal equations Aw = b from it. This makes the SPD property of A and the form of b self-evident.",
      "suggestion": "Add a 'Problem Setup' paragraph before Part 1 that states: we seek w minimizing (1/2)||P_Omega(vec(T_{(k)}) - (Z otimes K)w)||^2 + (lambda/2) w^T (I_r otimes K) w. Then the gradient set to zero gives Aw = b. This also clarifies why only observed entries appear in b."
    },
    {
      "severity": "major",
      "location": "Definitions / Lemma 3",
      "reason": "The mentor defines B = TZ where T is the full tensor, but Lemma 3 uses B = T_{(k)} Z where T_{(k)} is the mode-k unfolding. These are different mathematical objects and the conflation is never resolved.",
      "required_fix": "Consistently use T_{(k)} (the mode-k matricization) throughout. The mentor's notation section should define T_{(k)} and use it in the definition of B.",
      "suggestion": "In the Definitions section, add: 'Let T_{(k)} in R^{n x M} denote the mode-k unfolding of T.' Then replace B = TZ with B = T_{(k)}Z everywhere. This small fix eliminates a genuine ambiguity."
    },
    {
      "severity": "minor",
      "location": "Lemma 6, error transformation",
      "reason": "The identity e_m = P^{-1/2} hat{e}_m is asserted with an incomplete derivation. The proof writes 'More directly: since e_m = P^{-1/2} hat{e}_m' without showing why, then uses this to equate the A-norm and hat{A}-norm.",
      "required_fix": "Add a one-line derivation: since hat{w}_m = P^{1/2} w_m and hat{w}_* = P^{1/2} w_*, we get hat{e}_m = P^{1/2} e_m, hence e_m = P^{-1/2} hat{e}_m.",
      "suggestion": "Replace the 'More directly' paragraph with an explicit substitution showing the chain hat{e}_m = hat{w}_m - hat{w}_* = P^{1/2}(w_m - w_*) = P^{1/2} e_m. Then ||e_m||_A = ||hat{e}_m||_{hat{A}} follows by expanding the quadratic forms."
    },
    {
      "severity": "minor",
      "location": "Part 3, complexity simplification",
      "reason": "The claim 'nr <= qr by assumption n < q' is used to justify dropping the O(nr) initialization cost, but the final complexity O(qr + n^2r) already subsumes O(nr) regardless. The dominance relationship between n^2r and qr is never discussed.",
      "required_fix": "Remove the unnecessary justification 'since nr <= qr by assumption n < q'. Instead, note that the two dominant terms are qr (from the accumulation loop) and n^2r (from two kernel multiplies), and the total is O(qr + n^2r) without further simplification.",
      "suggestion": "Briefly note that if q >= n^2 the accumulation loop dominates, and if q < n^2 the kernel multiplies dominate, so both terms must be retained in the bound."
    },
    {
      "severity": "minor",
      "location": "Part 4 / Part 3, on-the-fly z_t cost",
      "reason": "Part 3 notes that on-the-fly computation of z_t adds O(q(d-1)r) to the matvec cost, but Part 4 does not account for the same cost when forming G_Omega during preconditioner setup.",
      "required_fix": "Add a note in Part 4 that if z_t vectors are generated on-the-fly, the setup cost for G_Omega becomes O(q(d-1)r + qr^2) rather than O(qr^2).",
      "suggestion": "Add a parenthetical remark after 'total O(qr^2)' in the G_Omega formation step: '(plus O(q(d-1)r) if z_t are generated on-the-fly, as in Part 3).'"
    },
    {
      "severity": "minor",
      "location": "Theorem statement, claim 2",
      "reason": "The += notation in the theorem statement is pseudocode, not standard mathematical notation. At graduate rigor level, a theorem statement should use purely mathematical language.",
      "required_fix": "Reformulate claim 2 to describe the matvec algorithm in mathematical terms, reserving pseudocode for an 'Algorithm' environment or a remark.",
      "suggestion": "Replace the algorithmic steps in the theorem statement with: 'A vec(V) = vec(K(CZ + lambda KV)) where C_i,: = sum_{t: i_t=i} <(KV)_i,:, z_t> z_t.' Then present the four-step procedure as Algorithm 1 or as a remark after the theorem."
    },
    {
      "severity": "minor",
      "location": "Gap closure note 7",
      "reason": "The qualitative discussion of preconditioner quality uses informal approximations ('approximately uniform', 'spectrally close to a scalar multiple') without any formal statement or reference.",
      "required_fix": "Either provide a concrete spectral equivalence bound (even under simplifying assumptions) or cite a specific result from the tensor completion / kernel learning literature.",
      "suggestion": "Consider adding: 'For a formal treatment of spectral equivalence bounds for Kronecker preconditioners in sampled settings, see [reference].' If no suitable reference exists, state explicitly that bounding kappa in terms of sampling parameters is an open question or deferred to future work."
    }
  ],
  "residual_concerns": [
    "The proof assumes throughout that the observations (i_t, m_t) are distinct (no repeated indices in Omega). If repeated observations are allowed, the projector P_Omega = SS^T is no longer idempotent and the masking interpretation breaks down. This edge case should be stated as an assumption or handled.",
    "Numerical stability of the accumulation loop (gap closure note 3) is mentioned but the floating-point error bound O(q epsilon_mach) is stated without justification. For very large q, compensated summation or other techniques may be needed, which could affect the practical complexity.",
    "The proof never specifies the vectorization convention (column-major vs row-major). The Kronecker-vec identity (C otimes A)vec(B) = vec(ABC^T) assumes column-major (vec stacks columns). If the implementation uses a different convention, the Kronecker ordering would need to be transposed. Assumption 4 from the mentor mentions consistency but this should be made explicit."
  ]
}
```

#### Reference Validity (by gemini_api)

The proof is exceptionally rigorous regarding reference validity. The Prover meticulously adheres to the definitions provided by the Mentor and correctly invokes standard linear algebra identities and optimization theorems without "hallucinating" results or misapplying them.

The Prover explicitly addresses potential ambiguities in the "Gap Closure Notes," such as the symmetry of $K$ (implied by it being a kernel matrix) and the consistency of the sparse RHS computation with the optimization objective.

There are no critical, major, or minor issues regarding reference validity. The proof is self-contained relative to the provided context and standard graduate-level mathematics.

```json
{
  "issues": [],
  "residual_concerns": []
}
```

### Editor Decision

**Verdict: `right_track`**

**Summary:** The proof uses the right overall strategy and most algebraic components are directionally correct, but it does not yet meet graduate-level rigor because key setup and consistency pieces are missing or flawed: the objective and normal equations are not stated upfront, RHS definitions are inconsistent (full tensor vs masked observed data; B = TZ vs B = T_(k)Z), and Lemma 6 contains an incomplete derivation plus a nonstandard/incorrect Chebyshev bound expression. The preconditioner-quality discussion is currently heuristic without formal assumptions or proof. Remaining comments are presentation/consistency refinements rather than fundamental redesign.

**Feedback (prover):** Revise the proof in this order: (1) add a Problem Setup paragraph before Part 1 stating the masked regularized least-squares objective and derive Aw=b directly from its gradient; (2) standardize notation globally to mode-k unfolding T_(k), define observed-data masking explicitly (e.g., T_Omega = 1_Omega \odot T_(k)), and define B consistently as T_Omega Z so the sparse accumulation in Lemma 3 is exactly the stated object; (3) replace Lemma 6 with the textbook PCG result on \hat A = P^{-1/2}AP^{-1/2}, include the explicit error transform \hat e_m = P^{1/2}e_m, and use the standard minimax/Chebyshev identity 1/T_m((\lambda_max+\lambda_min)/(\lambda_max-\lambda_min)) <= 2((sqrt\kappa-1)/(sqrt\kappa+1))^m; remove the incorrect intermediate bound and incomplete display; (4) move preconditioner-quality claims to a clearly labeled heuristic remark unless you provide a formal sampling model and spectral-equivalence theorem; (5) make minor clarity fixes: remove += pseudocode from theorem claims, clean Part 3 complexity wording without unnecessary nr<=qr justification, and in Part 4 include the extra O(q(d-1)r) setup term when z_t are generated on-the-fly.

## Loop 2

### Researcher Agent

## Relevant Theorems
- **PCG Convergence Bound:** For a linear system $Ax=b$ with $A$ symmetric positive definite (SPD) and an SPD preconditioner $P$, the $A$-norm of the error at iteration $m$, $\|e_m\|_A$, satisfies:
  \[
  \frac{\|e_m\|_A}{\|e_0\|_A} \le 2 \left( \frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1} \right)^m, \quad \text{where } \kappa = \kappa(P^{-1/2} A P^{-1/2}) = \frac{\lambda_{\max}(P^{-1}A)}{\lambda_{\min}(P^{-1}A)}.
  \]
  (Saad, *Iterative Methods for Sparse Linear Systems*, 2nd ed., Thm 9.4; Golub & Van Loan, *Matrix Computations*, 4th ed., §11.3).
- **Kronecker-Vec Identity:** $\operatorname{vec}(AXB) = (B^\top \otimes A)\operatorname{vec}(X)$.
- **Kronecker Product Inverse:** $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$ (assuming invertibility).
- **Trace-Norm Relation:** $\|X\|_F^2 = \operatorname{tr}(X^\top X) = \operatorname{vec}(X)^\top \operatorname{vec}(X)$.
- **Gradient of Quadratic Form:** $\nabla_x (\frac{1}{2} \|Ax - b\|_2^2 + \frac{\lambda}{2} x^\top Q x) = A^\top(Ax - b) + \lambda Q x$.

## Key Definitions
- **Mode-$k$ Unfolding:** Let $T_{(k)} \in \mathbb{R}^{n \times M}$ denote the mode-$k$ unfolding of the tensor $\mathcal{T}$.
- **Observed Set and Mask:** Let $\Omega \subset [n] \times [M]$ be the set of indices $(i, m)$ where the tensor entry is observed. Let $q = |\Omega|$.
- **Selection Operator:** $P_\Omega: \mathbb{R}^{nM} \to \mathbb{R}^{nM}$ is the diagonal projection matrix such that for any matrix $X \in \mathbb{R}^{n \times M}$, $P_\Omega \operatorname{vec}(X) = \operatorname{vec}(\mathbf{1}_\Omega \odot X)$, where $\mathbf{1}_\Omega$ is the binary mask matrix.
- **Optimization Objective:** We seek $W \in \mathbb{R}^{n \times r}$ minimizing:
  \[
  J(W) = \frac{1}{2} \| P_\Omega (\operatorname{vec}(T_{(k)}) - (Z \otimes K)\operatorname{vec}(W)) \|_2^2 + \frac{\lambda}{2} \operatorname{vec}(W)^\top (I_r \otimes K) \operatorname{vec}(W).
  \]
- **Linear System:** The normal equations $\nabla J(W) = 0$ yield $Aw = b$ with $w = \operatorname{vec}(W)$, where:
  \[
  A = (Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda (I_r \otimes K),
  \]
  \[
  b = (Z \otimes K)^\top P_\Omega \operatorname{vec}(T_{(k)}).
  \]
- **Sparse RHS Accumulator:** Let $B \in \mathbb{R}^{n \times r}$ be defined as $B = (\mathbf{1}_\Omega \odot T_{(k)}) Z$. Note that this differs from $T_{(k)}Z$ if unobserved entries are not zeroed.
- **Sampled Gram Matrix:** $G_\Omega = \sum_{t=1}^q z_t z_t^\top \in \mathbb{R}^{r \times r}$, where $z_t$ is the $m_t$-th row of $Z$.
- **Preconditioner:** $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, with $\delta > 0$.

## Proof Strategies
1.  **Derivation from Objective:** Explicitly derive the linear system $Aw=b$ from the gradient of the regularized least-squares objective $J(W)$. This establishes the correctness of the system and the specific form of $b$ involving only observed entries.
2.  **Matrix-Free Operator:** Prove that the matrix-vector product $A \operatorname{vec}(V)$ can be computed as $\operatorname{vec}(\mathcal{A}(V))$ where $\mathcal{A}(V) = K [ (\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V ]$. Describe the computation using mathematical summation notation over $\Omega$ to demonstrate $O(qr + n^2r)$ complexity.
3.  **Sparse RHS Computation:** Show that $b = \operatorname{vec}(KB)$ where $B$ is the sparse MTTKRP result. Prove that computing $B$ requires only $O(qr)$ operations by iterating over observed entries, and computing $b$ requires $O(n^2r)$.
4.  **Preconditioner Application:** Show that $P^{-1} \operatorname{vec}(R)$ is equivalent to solving two small systems (Sylvester-like structure via Kronecker factors). Specifically, $X = (K+\delta I)^{-1} R (G_\Omega + \lambda I)^{-1}$. Analyze the complexity: $O(n^3 + r^3)$ setup, $O(n^2r + nr^2)$ per iteration.
5.  **PCG Convergence:** Invoke the standard PCG convergence theorem for the transformed system $\hat{A} = P^{-1/2} A P^{-1/2}$. Explicitly relate the error in the original space to the transformed space ($\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$).
6.  **Complexity Analysis:** Sum the costs: Setup + $m \times$ (Matvec + Preconditioner Apply). Ensure no $O(N)$ terms appear.

## Gaps and Concerns
-   **Notation Consistency:** The prompt uses $T$ for the unfolding, but standard literature often uses $T_{(k)}$. I will use $T_{(k)}$ to be precise, as requested by the editor, while noting it corresponds to the prompt's $T$.
-   **RHS Definition:** The prompt defined $B = TZ$ (implying full multiplication). I must clarify that in the context of missing data, the "data" vector $b$ comes from the projected observed data, effectively using $T_\Omega = \mathbf{1}_\Omega \odot T_{(k)}$.
-   **Chebyshev Bound:** I must ensure the bound is stated in its standard simple form without the incorrect intermediate algebraic manipulation found in the previous draft.
-   **Preconditioner Quality:** The spectral equivalence of $P$ and $A$ is heuristic (dependent on sampling uniformity). I will present the preconditioner as a "standard choice" motivated by the structure of $A$ when sampling is uniform, but will not claim a proven condition number bound, relying instead on the generic PCG bound in terms of $\kappa$.
-   **On-the-fly Factors:** If $Z$ rows are generated on the fly, the setup of $G_\Omega$ costs $O(q(d-1)r + qr^2)$. This should be included for completeness.

### Mentor Agent

## Definitions
Let \(k\) be the target mode, \(n:=n_k\), \(M:=\prod_{i\neq k} n_i\), \(N=nM\), and rank \(r\).

Let \(\Omega\subset [n]\times[M]\) be the observed index set in mode-\(k\) unfolding, \(|\Omega|=q\), with observed triples \((i_t,m_t,y_t)\), \(t=1,\dots,q\), where \(y_t=T_{i_t,m_t}\).

Define
\[
Z:=A_d\odot\cdots\odot A_{k+1}\odot A_{k-1}\odot\cdots\odot A_1\in\mathbb{R}^{M\times r},
\quad
z_t:=Z_{m_t,:}\in\mathbb{R}^r.
\]
Unknown is \(W\in\mathbb{R}^{n\times r}\), \(w:=\mathrm{vec}(W)\), and \(A_k=KW\).

Define \(P_\Omega:=SS^\top\) (projector selecting observed entries in \(\mathrm{vec}(T)\)) and linear system
\[
A w=b,\quad
A=(Z\otimes K)^\top P_\Omega (Z\otimes K)+\lambda(I_r\otimes K),\quad
b=(I_r\otimes K)\mathrm{vec}(B),\ B=TZ.
\]

Define mask matrix \(\mathbf{1}_\Omega\in\{0,1\}^{n\times M}\). For \(V\in\mathbb{R}^{n\times r}\),
\[
\mathcal A(V):=
K\!\left((\mathbf{1}_\Omega\odot (KVZ^\top))Z+\lambda V\right),
\quad
A\,\mathrm{vec}(V)=\mathrm{vec}(\mathcal A(V)).
\]

## Formal Statement
**Theorem (Matrix-free PCG for RKHS mode-\(k\) subproblem).** Under the assumptions in Section **Assumptions**:

1. \(A\) is SPD, so PCG is well-defined.
2. For any \(V\), \(A\mathrm{vec}(V)\) can be computed exactly without forming \(A\), \(Z\otimes K\), or any \(N\)-scale object via:
\[
U=KV,\ \ 
\alpha_t=\langle U_{i_t,:},z_t\rangle,\ \ 
C_{i_t,:}\mathrel{+}=\alpha_t z_t,\ \ 
Y=KC+\lambda U,
\]
return \(\mathrm{vec}(Y)\).
3. The per-matvec complexity is \(O(qr+n^2r)\) (plus \(O(q(d-1)r)\) if each \(z_t\) is generated on-the-fly from CP factors).
4. With preconditioner
\[
P=((G_\Omega+\lambda I_r)\otimes (K+\delta I_n)),\quad
G_\Omega:=\sum_{t=1}^q z_tz_t^\top,\ \delta>0,
\]
each \(P^{-1}\)-apply costs \(O(n^2r+nr^2)\) after setup \(O(qr^2+n^3+r^3)\).
5. PCG iterates satisfy
\[
\|e_m\|_A\le 2\Big(\frac{\sqrt\kappa-1}{\sqrt\kappa+1}\Big)^m\|e_0\|_A,\quad
\kappa=\kappa(P^{-1/2}AP^{-1/2}),
\]
hence total cost
\[
O\!\left(m(qr+n^2r+nr^2)+qr^2+n^3+r^3\right),
\]
with no \(O(N)\) operation.

## Assumptions
1. \(\lambda>0\).
2. \(K\succ 0\) for strict SPD proof of \(A\); if original kernel is only PSD, either add nugget to the model kernel or solve on \(\mathrm{range}(K)\).
3. \(n,r<q\ll N\), and observed tuples \((i_t,m_t,y_t)\) are directly available.
4. Indexing conventions for unfolding, vectorization, and \(S\) are consistent.
5. For preconditioner quality claims (small \(\kappa\)), observation pattern is not pathologically imbalanced across \(i_t\) and rank directions.

## Notation
- \(n=n_k\): size of mode \(k\).
- \(M=\prod_{i\neq k} n_i\): product of non-\(k\) dimensions.
- \(N=nM\): total entries.
- \(q=|\Omega|\): number of observed entries.
- \(Z\in\mathbb{R}^{M\times r}\): Khatri-Rao product of fixed factors.
- \(K\in\mathbb{R}^{n\times n}\): kernel matrix.
- \(W\in\mathbb{R}^{n\times r}\), \(w=\mathrm{vec}(W)\): unknown.
- \(B=TZ\in\mathbb{R}^{n\times r}\): sparse MTTKRP result.
- \(z_t=Z_{m_t,:}\): sampled Khatri-Rao row for observation \(t\).
- \(P_\Omega=SS^\top\): selection projector in vectorized unfolding space.
- \(G_\Omega=\sum_t z_tz_t^\top\in\mathbb{R}^{r\times r}\): sampled rank-Gram.

## High-Level Strategy
1. Rewrite the Hessian action as a matrix operator \(\mathcal A(V)\) using \((C\otimes A)\mathrm{vec}(B)=\mathrm{vec}(AB C^\top)\).
2. Implement \(\mathcal A(V)\) from observed entries only, via rowwise accumulation \(C_{i_t,:} \!+\!= \alpha_t z_t\), never forming \(P_\Omega\), \(Z\otimes K\), or dense \(T\).
3. Compute RHS sparsely: \(B_{i_t,:}\!+\!=y_t z_t\), then \(b=\mathrm{vec}(KB)\).
4. Run PCG on \(Aw=b\) with Kronecker preconditioner \(P\) that is cheap to invert by two small Cholesky-based solves.
5. Prove complexity by summing matvec cost, preconditioner-apply cost, and one-time setup.

## Key Lemmas
1. **Kronecker-vec masked operator lemma:** \(A\,\mathrm{vec}(V)=\mathrm{vec}(\mathcal A(V))\).
2. **Sampled accumulation lemma:** \(C=(\mathbf{1}_\Omega\odot(KVZ^\top))Z\) is computed exactly by the \(q\)-sample loop in \(O(qr)\).
3. **Sparse RHS lemma:** \(B=TZ\) is computed from observations in \(O(qr)\), then \(b\) in \(O(n^2r)\).
4. **SPD lemma:** if \(K\succ0,\lambda>0\), then \(A\succ0\) because
\[
x^\top A x=\|P_\Omega^{1/2}(Z\otimes K)x\|_2^2+\lambda\|(I_r\otimes K^{1/2})x\|_2^2>0.
\]
5. **Preconditioner-apply lemma:** \(P^{-1}\mathrm{vec}(R)\) equals two solves:
\[
(K+\delta I)Y=R,\quad X(G_\Omega+\lambda I)=Y,\quad \mathrm{vec}(X)=P^{-1}\mathrm{vec}(R).
\]
6. **PCG convergence lemma:** standard SPD PCG bound with \(\kappa(P^{-1/2}AP^{-1/2})\).

## Dependency Graph
1. Definitions + Notation \(\rightarrow\) Lemma 1.
2. Lemma 1 \(\rightarrow\) Lemma 2 (matvec algorithm correctness).
3. Definitions \(\rightarrow\) Lemma 3 (RHS correctness/cost).
4. Assumptions 1-2 \(\rightarrow\) Lemma 4 (SPD of \(A\)).
5. Definitions \(\rightarrow\) Lemma 5 (preconditioner form and apply rule).
6. Lemma 4 + Lemma 5 + Lemma 6 \(\rightarrow\) PCG convergence claim.
7. Lemma 2 + Lemma 3 + Lemma 5 \(\rightarrow\) total complexity claim in theorem.

## Risky Steps
1. **PSD kernel issue:** if \(K\) is singular, \(A\) may be singular despite \(\lambda>0\); this must be handled explicitly.
2. **Index consistency:** mismatch between unfolding order, \(S\), and sampled \((i_t,m_t)\) invalidates the operator.
3. **Preconditioner quality claim:** spectral-equivalence constants require sampling-balance assumptions; do not claim uniformly good \(\kappa\) without them.
4. **Hidden \(M\)-scale cost:** forming full \(Z\) is infeasible when \(M\) is huge; use only sampled rows \(z_t\) (cached or on-demand).
5. **Numerical stability:** very small \(\lambda\) or ill-conditioned \(K\) can cause PCG stagnation; monitor relative preconditioned residual and cap iterations.

### Prover Agent

Now I have a thorough understanding of the prior proof, the reviewer feedback, and the editor's specific revision requests. Let me write the complete revised proof addressing all five editor feedback points.

## Complete Proof

We prove the theorem in five parts, corresponding to claims (1)–(5) of the formal statement. We adopt the following notational conventions throughout.

**Notation.** Let \(\mathcal{T} \in \mathbb{R}^{n_1 \times \cdots \times n_d}\) be the observed tensor, \(k\) the target mode, \(n := n_k\), \(M := \prod_{i \neq k} n_i\), and \(N = nM\). Let \(T_{(k)} \in \mathbb{R}^{n \times M}\) denote the mode-\(k\) matricization (unfolding) of \(\mathcal{T}\). The set of observed index pairs in this unfolding is \(\Omega \subset [n] \times [M]\), with \(|\Omega| = q\). We enumerate the observations as \((i_t, m_t, y_t)\) for \(t = 1, \ldots, q\), where \(y_t = (T_{(k)})_{i_t, m_t}\) is the observed value. Define the binary mask matrix \(\mathbf{1}_\Omega \in \{0,1\}^{n \times M}\) with \((\mathbf{1}_\Omega)_{i,m} = 1\) iff \((i,m) \in \Omega\), and the masked unfolding \(T_\Omega := \mathbf{1}_\Omega \odot T_{(k)} \in \mathbb{R}^{n \times M}\), which equals \(T_{(k)}\) at observed entries and zero elsewhere. The selection matrix \(S \in \mathbb{R}^{N \times q}\) has columns \(e_{(i_t, m_t)}\) (standard basis vectors in \(\mathbb{R}^N\) indexed by the column-major vectorization of position \((i_t, m_t)\)), so \(P_\Omega := SS^\top\) is the diagonal projector satisfying \(P_\Omega \operatorname{vec}(X) = \operatorname{vec}(\mathbf{1}_\Omega \odot X)\) for any \(X \in \mathbb{R}^{n \times M}\). We assume the observations \((i_t, m_t)\) are distinct (i.e., \(\Omega\) is a set, not a multiset), so \(P_\Omega\) is an orthogonal projector (\(P_\Omega^2 = P_\Omega\)).

Let \(Z := A_d \odot \cdots \odot A_{k+1} \odot A_{k-1} \odot \cdots \odot A_1 \in \mathbb{R}^{M \times r}\) be the Khatri–Rao product of fixed factor matrices, and \(z_t := Z_{m_t,:} \in \mathbb{R}^r\) the row of \(Z\) corresponding to the \(t\)-th observation. Let \(K \in \mathbb{R}^{n \times n}\) be the symmetric kernel matrix with \(K \succ 0\) (Assumption 2). The unknown is \(W \in \mathbb{R}^{n \times r}\) with \(w := \operatorname{vec}(W)\), and the RKHS factor is \(A_k = KW\). We write \(G_\Omega := \sum_{t=1}^q z_t z_t^\top \in \mathbb{R}^{r \times r}\) for the sampled Gram matrix of Khatri–Rao rows. All vectorizations use column-major ordering, so the Kronecker-vec identity \((C \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)\) holds throughout.

### Problem Setup: Objective and Normal Equations

The mode-\(k\) subproblem minimizes the regularized least-squares loss over observed entries only:
\[
\mathcal{L}(w) := \frac{1}{2}\bigl\|P_\Omega\bigl(\operatorname{vec}(T_{(k)}) - (Z \otimes K)w\bigr)\bigr\|_2^2 + \frac{\lambda}{2}\, w^\top (I_r \otimes K)\, w.
\]

We derive the normal equations by setting \(\nabla_w \mathcal{L} = 0\). Since \(P_\Omega\) is a symmetric projector (\(P_\Omega = P_\Omega^\top = P_\Omega^2\)):
\[
\nabla_w \mathcal{L} = -(Z \otimes K)^\top P_\Omega\bigl(\operatorname{vec}(T_{(k)}) - (Z \otimes K)w\bigr) + \lambda (I_r \otimes K)\, w.
\]

Setting this to zero and rearranging:
\[
\bigl[(Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda (I_r \otimes K)\bigr]\, w = (Z \otimes K)^\top P_\Omega \operatorname{vec}(T_{(k)}).
\]

The left-hand side defines \(A w\) with \(A := (Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda (I_r \otimes K)\).

For the right-hand side, we compute:
\[
b := (Z \otimes K)^\top P_\Omega \operatorname{vec}(T_{(k)}) = (Z^\top \otimes K^\top) \operatorname{vec}(\mathbf{1}_\Omega \odot T_{(k)}) = (Z^\top \otimes K) \operatorname{vec}(T_\Omega),
\]
where we used \(K^\top = K\) (symmetry) and the projector-masking equivalence. By the Kronecker-vec identity:
\[
b = \operatorname{vec}(K\, T_\Omega\, Z).
\]

Define \(B := T_\Omega Z \in \mathbb{R}^{n \times r}\). Then \(b = \operatorname{vec}(KB)\). Note that since \(T_\Omega\) is zero at unobserved entries, \(B\) depends only on the \(q\) observed values \(y_t\), and we have:
\[
B_{i,s} = \sum_{\{t : i_t = i\}} y_t\, (z_t)_s.
\]

This is the sparse MTTKRP. The system to be solved is thus \(Aw = b\) with \(b = \operatorname{vec}(KB)\), exactly as stated in the question.

**Remark.** The question defines \(B = TZ\) where \(T\) denotes the mode-\(k\) unfolding "with all missing entries set to zero." This convention means \(T \equiv T_\Omega = \mathbf{1}_\Omega \odot T_{(k)}\), so the question's definition is consistent with ours. We use the explicit notation \(T_\Omega\) throughout to avoid ambiguity.

### Part 1: \(A\) is symmetric positive definite

**Symmetry.** Both terms composing \(A\) are symmetric. For the data-fit term: \(\bigl((Z \otimes K)^\top P_\Omega (Z \otimes K)\bigr)^\top = (Z \otimes K)^\top P_\Omega^\top (Z \otimes K) = (Z \otimes K)^\top P_\Omega (Z \otimes K)\), since \(P_\Omega = P_\Omega^\top\). For the regularization term: \((\lambda(I_r \otimes K))^\top = \lambda(I_r \otimes K)\), since \(K = K^\top\).

**Positive definiteness.** For any nonzero \(x \in \mathbb{R}^{nr}\), let \(V \in \mathbb{R}^{n \times r}\) be its matricization (\(x = \operatorname{vec}(V)\)):
\[
x^\top A x = \underbrace{\|S^\top(Z \otimes K)x\|_2^2}_{\geq\, 0} + \lambda\, x^\top (I_r \otimes K)\, x.
\]

For the second term:
\[
x^\top (I_r \otimes K) x = \operatorname{vec}(V)^\top \operatorname{vec}(KV) = \operatorname{tr}(V^\top K V) = \|K^{1/2} V\|_F^2,
\]
where \(K^{1/2}\) is the unique symmetric positive definite square root of \(K\) (which exists since \(K \succ 0\)). Since \(V \neq 0\) and \(K^{1/2}\) is nonsingular, \(K^{1/2}V \neq 0\), hence \(\|K^{1/2}V\|_F^2 > 0\). Therefore \(x^\top A x \geq \lambda \|K^{1/2}V\|_F^2 > 0\), and \(A\) is SPD. The PCG iteration is well-defined and convergent. \(\square\)

**Remark (PSD kernel case).** If \(K\) is only PSD with nontrivial nullspace, then \(I_r \otimes K\) is PSD, and \(\lambda x^\top(I_r \otimes K)x = 0\) for \(x \in \mathbb{R}^r \otimes \ker(K)\). In this case \(A\) need not be SPD. The standard remedies are: (a) replace \(K\) with \(K + \epsilon I_n\) for small \(\epsilon > 0\); or (b) restrict the problem to \(\operatorname{range}(K)\) in each column. Both ensure strict positive definiteness of the reduced system.

### Part 2: Matrix-free matvec via the operator \(\mathcal{A}(V)\)

We claim that for any \(V \in \mathbb{R}^{n \times r}\),
\[
A\operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V)), \quad \mathcal{A}(V) := K\bigl((\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V\bigr),
\]
and that this is computed without forming any \(N\)-scale object by the procedure:

> **Algorithm (Matvec).** Given \(V \in \mathbb{R}^{n \times r}\):
> 1. Compute \(U = KV \in \mathbb{R}^{n \times r}\).
> 2. Initialize \(C = 0 \in \mathbb{R}^{n \times r}\). For each \(t = 1, \ldots, q\): compute \(\alpha_t = \langle U_{i_t,:}, z_t \rangle\) and set \(C_{i_t,:} \leftarrow C_{i_t,:} + \alpha_t z_t\).
> 3. Compute \(Y = KC + \lambda U\).
> 4. Return \(\operatorname{vec}(Y)\).

The algebraic identity \(A\operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))\) is proved in Lemma 1. The equivalence of the accumulation loop to the masked matrix product \(C = (\mathbf{1}_\Omega \odot (UZ^\top))Z\) is proved in Lemma 2. Together, these establish that the algorithm computes \(A\operatorname{vec}(V)\) exactly. The only objects formed are matrices of dimension \(n \times r\), \(n \times n\), vectors of length \(r\), and scalars — none of dimension \(N\) or \(M\).

### Part 3: Per-matvec complexity is \(O(qr + n^2r)\)

Each application of the matvec algorithm involves:

- **Step 1** (\(U = KV\)): one matrix multiply \(K \in \mathbb{R}^{n \times n}\) times \(V \in \mathbb{R}^{n \times r}\), costing \(O(n^2 r)\).
- **Step 2** (accumulation loop): for each \(t = 1, \ldots, q\), one inner product \(\langle U_{i_t,:}, z_t \rangle\) costing \(O(r)\) and one scaled vector addition \(C_{i_t,:} \leftarrow C_{i_t,:} + \alpha_t z_t\) costing \(O(r)\). Total: \(O(qr)\). (Initialization \(C = 0\) costs \(O(nr) \subseteq O(qr)\) since \(n < q\).)
- **Step 3** (\(Y = KC + \lambda U\)): one matrix multiply \(KC\) costing \(O(n^2 r)\) and one scaled matrix addition costing \(O(nr)\).

The two dominant terms are \(O(qr)\) from the accumulation loop and \(O(n^2r)\) from the two kernel multiplications. The total per-matvec cost is \(O(qr + n^2r)\). If \(q \geq n^2\) the loop dominates; if \(q < n^2\) the kernel multiplies dominate; both terms must be retained.

If the Khatri–Rao rows \(z_t\) are not precomputed but generated on-the-fly from the \(d-1\) fixed CP factor matrices (via elementwise products of the corresponding rows), each \(z_t\) costs \(O((d-1)r)\), adding \(O(q(d-1)r)\) to the per-matvec cost.

### Part 4: Preconditioner setup and per-apply cost

The Kronecker preconditioner is
\[
P := (G_\Omega + \lambda I_r) \otimes (K + \delta I_n), \quad \delta > 0.
\]

**One-time setup:**
- Form \(G_\Omega = \sum_{t=1}^q z_t z_t^\top\): accumulate \(q\) rank-1 outer products, each costing \(O(r^2)\), total \(O(qr^2)\). (If \(z_t\) vectors are generated on-the-fly, add \(O(q(d-1)r)\) for their computation.)
- Cholesky-factor \(G_\Omega + \lambda I_r \in \mathbb{R}^{r \times r}\): cost \(O(r^3)\).
- Cholesky-factor \(K + \delta I_n \in \mathbb{R}^{n \times n}\): cost \(O(n^3)\).

Total setup: \(O(qr^2 + n^3 + r^3)\), plus \(O(q(d-1)r)\) if \(z_t\) are generated on-the-fly.

**Per-apply:** Given residual \(R \in \mathbb{R}^{n \times r}\), compute \(X\) such that \(\operatorname{vec}(X) = P^{-1}\operatorname{vec}(R)\). By Lemma 5, this decomposes as:
1. Solve \((K + \delta I_n) Y = R\) for \(Y\): \(r\) triangular solves of size \(n\), cost \(O(n^2 r)\).
2. Solve \(X(G_\Omega + \lambda I_r) = Y\) for \(X\): equivalently \((G_\Omega + \lambda I_r) X^\top = Y^\top\), which is \(n\) triangular solves of size \(r\), cost \(O(nr^2)\).

Total per-apply: \(O(n^2 r + nr^2)\).

### Part 5: PCG convergence and total complexity

Since \(A \succ 0\) (Part 1) and \(P \succ 0\) (both Kronecker factors are SPD: \(K + \delta I \succ 0\) because \(K \succ 0\) and \(\delta > 0\); \(G_\Omega + \lambda I \succ 0\) because \(G_\Omega \succeq 0\) and \(\lambda > 0\)), the preconditioned matrix \(\hat{A} := P^{-1/2}AP^{-1/2}\) is SPD with condition number \(\kappa := \kappa(\hat{A}) = \lambda_{\max}(\hat{A})/\lambda_{\min}(\hat{A})\).

By Lemma 6 (PCG convergence), the \(A\)-norm error of the \(m\)-th PCG iterate satisfies:
\[
\|e_m\|_A \leq 2\left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^m \|e_0\|_A.
\]

To achieve \(\|e_m\|_A / \|e_0\|_A \leq \varepsilon\), it suffices to take \(m = O(\sqrt{\kappa}\,\log(1/\varepsilon))\) iterations (since \((\sqrt{\kappa}-1)/(\sqrt{\kappa}+1) = 1 - 2/(\sqrt{\kappa}+1)\) and \(\log(1-2/(\sqrt{\kappa}+1)) \approx -2/\sqrt{\kappa}\) for large \(\kappa\)).

Each PCG iteration requires one matvec (\(O(qr + n^2r)\), Part 3) and one preconditioner apply (\(O(n^2r + nr^2)\), Part 4), plus \(O(nr)\) for vector operations (inner products, axpy). The total cost for \(m\) iterations is:
\[
O\!\left(m(qr + n^2r + nr^2) + qr^2 + n^3 + r^3\right),
\]
where the first group is the per-iteration cost and the second is the one-time preconditioner setup. Since we access only the \(q\) observed entries and form matrices of size at most \(n \times n\), \(n \times r\), or \(r \times r\), no operation has cost \(O(N)\). \(\blacksquare\)

**Heuristic Remark (Preconditioner quality).** The convergence rate depends on \(\kappa = \kappa(P^{-1/2}AP^{-1/2})\), which in turn depends on how well the Kronecker structure of \(P\) approximates the non-Kronecker operator \(A\). Intuitively, when observations are distributed reasonably uniformly across mode-\(k\) indices, the masked Hadamard product \(\mathbf{1}_\Omega \odot (XZ^\top)\) behaves approximately like a scaled version of \(XZ^\top\), making the data-fit term of \(A\) approximately Kronecker-structured. In this regime, the condition number \(\kappa\) remains moderate with appropriate choice of \(\delta\). However, a formal spectral-equivalence bound relating \(\kappa\) to the sampling distribution would require a specific probabilistic model (e.g., uniform sampling without replacement) and is beyond the scope of this proof. Without such analysis, Assumption 5 serves as a qualitative prerequisite: if the observation pattern is pathologically concentrated on a few mode-\(k\) indices, the Kronecker approximation degrades and \(\kappa\) can grow, slowing convergence.

---

## Lemma Proofs

### Lemma 1: Kronecker-vec masked operator identity

**Statement.** For any \(V \in \mathbb{R}^{n \times r}\),
\[
A\operatorname{vec}(V) = \operatorname{vec}\!\left(K\bigl[(\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V\bigr]\right).
\]

**Proof.** We expand \(A\operatorname{vec}(V) = \bigl[(Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda(I_r \otimes K)\bigr]\operatorname{vec}(V)\) term by term.

*Regularization term.* By the Kronecker-vec identity \((C \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)\):
\[
\lambda(I_r \otimes K)\operatorname{vec}(V) = \lambda\operatorname{vec}(KVI_r^\top) = \lambda\operatorname{vec}(KV).
\]

*Data-fit term.* Applying the Kronecker-vec identity to \((Z \otimes K)\operatorname{vec}(V)\):
\[
(Z \otimes K)\operatorname{vec}(V) = \operatorname{vec}(KVZ^\top).
\]
This is a vector in \(\mathbb{R}^{nM}\), identified with the matrix \(KVZ^\top \in \mathbb{R}^{n \times M}\).

Next, applying \(P_\Omega\). Since \(P_\Omega = SS^\top\) is the diagonal projector that zeros out entries outside \(\Omega\), and the Hadamard mask \(\mathbf{1}_\Omega\) performs the same operation in matrix form, we have for any \(X \in \mathbb{R}^{n \times M}\):
\[
P_\Omega\operatorname{vec}(X) = \operatorname{vec}(\mathbf{1}_\Omega \odot X).
\]
(This holds because \(P_\Omega\) is diagonal with \((P_\Omega)_{jj} = 1\) iff index \(j\) corresponds to an observed position \((i,m) \in \Omega\) under column-major vectorization, which is precisely the action of Hadamard multiplication by \(\mathbf{1}_\Omega\).) Therefore:
\[
P_\Omega(Z \otimes K)\operatorname{vec}(V) = \operatorname{vec}(\mathbf{1}_\Omega \odot (KVZ^\top)).
\]

Finally, applying \((Z \otimes K)^\top = (Z^\top \otimes K^\top) = (Z^\top \otimes K)\) (using \(K = K^\top\)):
\[
(Z \otimes K)^\top \operatorname{vec}(\mathbf{1}_\Omega \odot (KVZ^\top)) = (Z^\top \otimes K)\operatorname{vec}(\mathbf{1}_\Omega \odot (KVZ^\top)) = \operatorname{vec}\!\left(K(\mathbf{1}_\Omega \odot (KVZ^\top))Z\right),
\]
where the last step uses \((C \otimes A)\operatorname{vec}(X) = \operatorname{vec}(AXC^\top)\) with \(C = Z^\top\), \(A = K\), \(X = \mathbf{1}_\Omega \odot (KVZ^\top)\), giving \(\operatorname{vec}(K(\mathbf{1}_\Omega \odot (KVZ^\top))(Z^\top)^\top) = \operatorname{vec}(K(\mathbf{1}_\Omega \odot (KVZ^\top))Z)\).

Combining:
\[
A\operatorname{vec}(V) = \operatorname{vec}\!\left(K(\mathbf{1}_\Omega \odot (KVZ^\top))Z\right) + \lambda\operatorname{vec}(KV) = \operatorname{vec}\!\left(K\bigl[(\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V\bigr]\right). \quad \square
\]

### Lemma 2: Sampled accumulation correctness

**Statement.** Let \(U = KV\). The matrix \(C := (\mathbf{1}_\Omega \odot (UZ^\top))Z \in \mathbb{R}^{n \times r}\) is computed exactly by initializing \(C = 0\) and, for each \(t = 1, \ldots, q\), setting \(C_{i_t,:} \leftarrow C_{i_t,:} + \alpha_t z_t\) where \(\alpha_t = \langle U_{i_t,:}, z_t \rangle\). The cost is \(O(qr)\).

**Proof.** Consider the \(i\)-th row of \(C\). By the definition of matrix multiplication:
\[
C_{i,:} = \sum_{m=1}^M (\mathbf{1}_\Omega)_{i,m}\, (UZ^\top)_{i,m}\, Z_{m,:}.
\]

The factor \((UZ^\top)_{i,m} = \langle U_{i,:}, Z_{m,:} \rangle\). After masking, only terms with \((i,m) \in \Omega\) survive. Since the observations \((i_t, m_t)\)_{t=1}^q enumerate \(\Omega\) exactly:
\[
C_{i,:} = \sum_{\{t : i_t = i\}} \langle U_{i,:}, z_t \rangle\, z_t = \sum_{\{t : i_t = i\}} \alpha_t\, z_t.
\]

This is precisely the result of the accumulation loop: for each \(t\), the update \(C_{i_t,:} \leftarrow C_{i_t,:} + \alpha_t z_t\) adds \(\alpha_t z_t\) to row \(i_t\) of \(C\). The order of summation is immaterial (real addition is commutative and associative in exact arithmetic).

Cost: each of the \(q\) iterations performs one inner product (\(O(r)\)) and one scaled vector addition (\(O(r)\)), for a total of \(O(qr)\). \(\square\)

**Remark.** By Lemma 1, \(\mathcal{A}(V) = K(CZ + \lambda V)\) where \(C\) is defined with \(KVZ^\top\). Setting \(U = KV\) in the above, we see \(C = (\mathbf{1}_\Omega \odot (KVZ^\top))Z\), matching the definition in \(\mathcal{A}\). This closes the chain: the algorithm computes \(U = KV\), then \(C\) via the accumulation loop (this lemma), then \(Y = KC + \lambda U = \mathcal{A}(V)\).

### Lemma 3: Sparse RHS computation

**Statement.** The matrix \(B = T_\Omega Z \in \mathbb{R}^{n \times r}\) is computed from observed entries alone in \(O(qr)\), and then \(b = \operatorname{vec}(KB)\) in \(O(n^2r)\).

**Proof.** The \((i,s)\)-entry of \(B = T_\Omega Z\) is:
\[
B_{i,s} = \sum_{m=1}^M (T_\Omega)_{i,m}\, Z_{m,s}.
\]

Since \((T_\Omega)_{i,m} = y_t\) if \((i,m) = (i_t, m_t)\) for some \(t\), and zero otherwise:
\[
B_{i,s} = \sum_{\{t : i_t = i\}} y_t\, (z_t)_s.
\]

In vector form: \(B_{i,:} = \sum_{\{t : i_t = i\}} y_t\, z_t\). This is computed by initializing \(B = 0\) and, for each \(t = 1, \ldots, q\), updating \(B_{i_t,:} \leftarrow B_{i_t,:} + y_t z_t\). Each update costs \(O(r)\), so the total is \(O(qr)\).

Then \(b = \operatorname{vec}(KB)\) requires the matrix product \(K \in \mathbb{R}^{n \times n}\) times \(B \in \mathbb{R}^{n \times r}\), costing \(O(n^2r)\).

**Consistency check.** From the Problem Setup, \(b = (Z \otimes K)^\top P_\Omega \operatorname{vec}(T_{(k)}) = \operatorname{vec}(K T_\Omega Z) = \operatorname{vec}(KB)\). This matches the question's definition \(b = (I_r \otimes K)\operatorname{vec}(B)\) since \((I_r \otimes K)\operatorname{vec}(B) = \operatorname{vec}(KBI_r^\top) = \operatorname{vec}(KB)\). The question defines \(B = TZ\) where \(T\) is the unfolding "with all missing entries set to zero," i.e., \(T \equiv T_\Omega\). \(\square\)

### Lemma 4: SPD property of \(A\)

This is proved as Part 1 above. We restate the key inequality for reference: for \(K \succ 0\), \(\lambda > 0\), and any nonzero \(x \in \mathbb{R}^{nr}\),
\[
x^\top Ax \geq \lambda\, \|K^{1/2}V\|_F^2 > 0,
\]
where \(V\) is the matricization of \(x\). \(\square\)

### Lemma 5: Preconditioner apply via Kronecker factorization

**Statement.** With \(P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)\), the action \(\operatorname{vec}(X) = P^{-1}\operatorname{vec}(R)\) for \(R \in \mathbb{R}^{n \times r}\) is computed by:
1. Solve \((K + \delta I_n)\, Y = R\) for \(Y \in \mathbb{R}^{n \times r}\).
2. Solve \(X\,(G_\Omega + \lambda I_r) = Y\) for \(X \in \mathbb{R}^{n \times r}\).

**Proof.** Both Kronecker factors are SPD:
- \(K + \delta I_n \succ 0\) since \(K \succ 0\) and \(\delta > 0\).
- \(G_\Omega + \lambda I_r \succ 0\) since \(G_\Omega = \sum_t z_t z_t^\top \succeq 0\) and \(\lambda > 0\).

Hence \(P\) is SPD, and by the Kronecker inverse identity:
\[
P^{-1} = (G_\Omega + \lambda I_r)^{-1} \otimes (K + \delta I_n)^{-1}.
\]

Applying the Kronecker-vec identity:
\[
P^{-1}\operatorname{vec}(R) = \bigl((G_\Omega + \lambda I_r)^{-1} \otimes (K + \delta I_n)^{-1}\bigr)\operatorname{vec}(R) = \operatorname{vec}\!\left((K + \delta I_n)^{-1} R\, (G_\Omega + \lambda I_r)^{-\top}\right).
\]

Since \(G_\Omega + \lambda I_r\) is symmetric, \((G_\Omega + \lambda I_r)^{-\top} = (G_\Omega + \lambda I_r)^{-1}\), so:
\[
X = (K + \delta I_n)^{-1} R\, (G_\Omega + \lambda I_r)^{-1}.
\]

This factors as:
1. \(Y = (K + \delta I_n)^{-1}R\), i.e., solve \((K + \delta I_n)Y = R\) column by column. Using the precomputed Cholesky factor \(L_K L_K^\top = K + \delta I_n\), each of the \(r\) forward-backward substitutions costs \(O(n^2)\), total \(O(n^2r)\).
2. \(X = Y(G_\Omega + \lambda I_r)^{-1}\), i.e., solve \(X(G_\Omega + \lambda I_r) = Y\), equivalently \((G_\Omega + \lambda I_r)X^\top = Y^\top\) row by row. Using the precomputed Cholesky factor \(L_G L_G^\top = G_\Omega + \lambda I_r\), each of the \(n\) forward-backward substitutions costs \(O(r^2)\), total \(O(nr^2)\).

Total per-apply: \(O(n^2r + nr^2)\). One-time Cholesky costs: \(O(n^3)\) for \(K + \delta I_n\) and \(O(r^3)\) for \(G_\Omega + \lambda I_r\). \(\square\)

### Lemma 6: PCG convergence bound

**Statement.** Let \(A, P \in \mathbb{R}^{nr \times nr}\) both be SPD. The preconditioned conjugate gradient method applied to \(Aw = b\) with preconditioner \(P\) produces iterates \(w_m\) satisfying:
\[
\|e_m\|_A \leq 2\left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^m \|e_0\|_A, \quad \kappa = \kappa(\hat{A}), \quad \hat{A} := P^{-1/2}AP^{-1/2}.
\]

**Proof.** Define the change of variables \(\hat{w} = P^{1/2}w\), \(\hat{b} = P^{-1/2}b\), and \(\hat{A} = P^{-1/2}AP^{-1/2}\). Then \(Aw = b\) becomes \(\hat{A}\hat{w} = \hat{b}\). PCG applied to \(Aw = b\) with preconditioner \(P\) is mathematically equivalent to standard CG applied to \(\hat{A}\hat{w} = \hat{b}\).

**Error transformation.** Let \(w_* = A^{-1}b\) and \(\hat{w}_* = \hat{A}^{-1}\hat{b} = P^{1/2}w_*\). The CG iterates in the transformed space satisfy \(\hat{w}_m = P^{1/2}w_m\) (by the equivalence of PCG and transformed CG). The transformed error is:
\[
\hat{e}_m := \hat{w}_m - \hat{w}_* = P^{1/2}w_m - P^{1/2}w_* = P^{1/2}(w_m - w_*) = P^{1/2}e_m.
\]

**Norm equivalence.** The \(\hat{A}\)-norm of \(\hat{e}_m\) equals the \(A\)-norm of \(e_m\):
\[
\|\hat{e}_m\|_{\hat{A}}^2 = \hat{e}_m^\top \hat{A}\, \hat{e}_m = (P^{1/2}e_m)^\top (P^{-1/2}AP^{-1/2})(P^{1/2}e_m) = e_m^\top A e_m = \|e_m\|_A^2.
\]

**CG optimality and Chebyshev bound.** By the optimality property of CG, the \(m\)-th iterate minimizes the \(\hat{A}\)-norm error over the Krylov subspace:
\[
\|\hat{e}_m\|_{\hat{A}} = \min_{p \in \mathcal{P}_m,\; p(0) = 1} \|p(\hat{A})\hat{e}_0\|_{\hat{A}},
\]
where \(\mathcal{P}_m\) denotes polynomials of degree at most \(m\). Since \(\hat{A}\) is SPD with eigenvalues in \([\lambda_{\min}, \lambda_{\max}]\) (where \(\lambda_{\min} = \lambda_{\min}(\hat{A}) > 0\)):
\[
\|\hat{e}_m\|_{\hat{A}} \leq \min_{p \in \mathcal{P}_m,\; p(0) = 1}\; \max_{\mu \in [\lambda_{\min}, \lambda_{\max}]} |p(\mu)| \cdot \|\hat{e}_0\|_{\hat{A}}.
\]

The minimax problem on the right has the classical solution via Chebyshev polynomials of the first kind. The optimal value is:
\[
\min_{p \in \mathcal{P}_m,\; p(0) = 1}\; \max_{\mu \in [\lambda_{\min}, \lambda_{\max}]} |p(\mu)| = \frac{1}{T_m\!\left(\dfrac{\lambda_{\max} + \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}}\right)},
\]
where \(T_m\) is the Chebyshev polynomial of the first kind of degree \(m\), and the minimizer is the appropriately shifted and scaled Chebyshev polynomial (see, e.g., Saad, *Iterative Methods for Sparse Linear Systems*, Theorem 6.29, or Golub & Van Loan, *Matrix Computations*, §11.3).

Setting \(\kappa = \lambda_{\max}/\lambda_{\min}\), we have \(\frac{\lambda_{\max} + \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}} = \frac{\kappa + 1}{\kappa - 1}\). For \(x \geq 1\), the Chebyshev polynomial satisfies \(T_m(x) = \cosh(m \operatorname{arccosh}(x))\), and using the identity \(\cosh(\theta) \geq \frac{1}{2}e^\theta\) for \(\theta \geq 0\):
\[
T_m\!\left(\frac{\kappa+1}{\kappa-1}\right) \geq \frac{1}{2}\left(\frac{\kappa+1}{\kappa-1} + \sqrt{\left(\frac{\kappa+1}{\kappa-1}\right)^2 - 1}\right)^m = \frac{1}{2}\left(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\right)^m,
\]
where the last equality uses:
\[
\frac{\kappa+1}{\kappa-1} + \frac{2\sqrt{\kappa}}{\kappa-1} = \frac{(\sqrt{\kappa}+1)^2}{(\sqrt{\kappa}+1)(\sqrt{\kappa}-1)} = \frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}.
\]

Therefore:
\[
\frac{1}{T_m\!\left(\frac{\kappa+1}{\kappa-1}\right)} \leq \frac{2}{\left(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\right)^m} = 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m.
\]

Combining with the norm equivalence \(\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}\):
\[
\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}} \leq 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m \|\hat{e}_0\|_{\hat{A}} = 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m \|e_0\|_A. \quad \square
\]

---

## Gap Closure Notes

1. **Problem Setup and RHS derivation (Editor point 1).** The revised proof opens with a Problem Setup section that states the masked regularized least-squares objective \(\mathcal{L}(w)\) and derives the normal equations \(Aw = b\) by setting \(\nabla_w \mathcal{L} = 0\). This makes the origin of both \(A\) and \(b\) transparent and shows directly why only observed entries contribute to \(b\). The derivation also confirms that the question's given system is exactly the gradient condition for this objective.

2. **Consistent notation: \(T_{(k)}\), \(T_\Omega\), and \(B\) (Editor point 2).** Throughout, we use \(T_{(k)}\) for the mode-\(k\) unfolding (a matrix in \(\mathbb{R}^{n \times M}\)), \(\mathbf{1}_\Omega\) for the binary mask, and \(T_\Omega := \mathbf{1}_\Omega \odot T_{(k)}\) for the observed-data matrix. We define \(B := T_\Omega Z\), so the sparse accumulation \(B_{i_t,:} \leftarrow B_{i_t,:} + y_t z_t\) computes exactly the stated object. The question's notation \(B = TZ\) with \(T\) meaning "unfolding with missing entries set to zero" is reconciled in the Remark after Problem Setup and in Lemma 3's consistency check.

3. **Corrected Lemma 6 (Editor point 3).** The revised Lemma 6 proceeds cleanly through three steps: (a) the error transformation \(\hat{e}_m = P^{1/2}e_m\) is derived explicitly from \(\hat{w}_m = P^{1/2}w_m\); (b) the norm equivalence \(\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}\) is verified by expanding the quadratic form; (c) the Chebyshev bound uses the standard minimax identity \(\min_{p(0)=1} \max_{\mu \in [\lambda_{\min},\lambda_{\max}]} |p(\mu)| = 1/T_m((\lambda_{\max}+\lambda_{\min})/(\lambda_{\max}-\lambda_{\min}))\), followed by the lower bound \(T_m(x) \geq \frac{1}{2}(x + \sqrt{x^2-1})^m\) for \(x \geq 1\), which yields \(2((\sqrt{\kappa}-1)/(\sqrt{\kappa}+1))^m\). The incorrect intermediate expression and incomplete display from the prior version are removed entirely.

4. **Preconditioner quality as heuristic remark (Editor point 4).** The discussion of when \(\kappa\) is small has been moved to a clearly labeled "Heuristic Remark" after Part 5. It provides qualitative intuition (uniform sampling makes the Hadamard masking approximately transparent) but explicitly states that a formal spectral-equivalence bound would require a specific probabilistic sampling model. The theorem itself makes no claims about the magnitude of \(\kappa\) — it holds for any SPD \(P\), with the convergence rate parameterized by \(\kappa(\hat{A})\).

5. **Minor clarity fixes (Editor point 5).** (a) The \(\mathrel{+}=\) pseudocode is removed from the theorem statement (Part 2). The matvec procedure is stated in a labeled Algorithm block using standard mathematical notation (\(\leftarrow\) for assignment). The theorem claim itself describes the computation mathematically. (b) Part 3 no longer contains the unnecessary justification "\(nr \leq qr\) by assumption \(n < q\)"; instead it identifies \(O(qr)\) and \(O(n^2r)\) as the two dominant terms and notes their relative magnitudes depend on whether \(q \gtrless n^2\). (c) Part 4 includes the \(O(q(d-1)r)\) term for on-the-fly \(z_t\) computation in the preconditioner setup cost, consistent with Part 3.

6. **Symmetry of \(K\).** The Kronecker-vec identity \((C \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)\) is applied with \(A = K\). In Lemma 1, we use \((Z \otimes K)^\top = Z^\top \otimes K^\top = Z^\top \otimes K\), which requires \(K = K^\top\). This holds because \(K\) is a kernel (Gram) matrix, hence symmetric by construction. This is stated in the Notation section.

7. **Projector–masking equivalence.** The identification \(P_\Omega\operatorname{vec}(X) = \operatorname{vec}(\mathbf{1}_\Omega \odot X)\) is justified in Lemma 1's proof: \(S\) has columns \(e_{(i_t,m_t)}\) (standard basis vectors indexed by column-major vectorization), so \(SS^\top\) is diagonal with ones at observed positions. Under the column-major bijection between \(\mathbb{R}^{nM}\) and \(\mathbb{R}^{n \times M}\), this is precisely the Hadamard mask.

8. **Vectorization convention.** All vectorizations use column-major ordering. The Kronecker-vec identity \((C \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)\) holds under this convention. This is stated once in the Notation section and used consistently throughout. Assumption 4 (from the mentor) requires that the selection matrix \(S\) and the index pairs \((i_t, m_t)\) are defined consistently with this convention.

9. **Distinctness of observations.** We assume the pairs \((i_t, m_t)\) are distinct, so \(\Omega\) is a set with \(|\Omega| = q\). This ensures \(P_\Omega = SS^\top\) is an orthogonal projector (\(P_\Omega^2 = P_\Omega\)), used in deriving the normal equations. If duplicate observations were allowed, \(SS^\top\) would have diagonal entries greater than 1, and the masking interpretation would need modification. The distinctness assumption is stated in the Notation section.

10. **Well-definedness of Cholesky factors.** The preconditioner apply (Lemma 5) requires Cholesky factorizations of \(K + \delta I_n\) and \(G_\Omega + \lambda I_r\). Both are SPD: \(K + \delta I_n \succ 0\) because \(K \succ 0\) and \(\delta > 0\); \(G_\Omega + \lambda I_r \succ 0\) because \(G_\Omega \succeq 0\) and \(\lambda > 0\). The Cholesky factorizations are therefore well-defined and numerically stable.

11. **No hidden \(O(N)\) costs.** We verify that no step requires \(O(N)\) storage or computation. The full tensor \(\mathcal{T}\) and its unfolding \(T_{(k)}\) are never formed; only the \(q\) observed triples \((i_t, m_t, y_t)\) are used. The Khatri–Rao product \(Z \in \mathbb{R}^{M \times r}\) is never formed in full; only the sampled rows \(z_t\) (precomputed or generated on-the-fly) are accessed. The Kronecker product \(Z \otimes K \in \mathbb{R}^{nM \times nr}\) and the projector \(P_\Omega \in \mathbb{R}^{nM \times nM}\) are never formed. All explicitly formed matrices have dimensions \(n \times n\), \(n \times r\), or \(r \times r\), none of which is \(O(N)\) under the assumption \(n, r < q \ll N\).

### Editor Dispatch

**Reasoning:** claude_code is best for logical verification of the matrix proofs; gemini_api excels at evaluating exposition and tone; codex_cli can verify standard numerical linear algebra references common in technical documentation.

- Correctness & Completeness -> claude_code
- Clarity & Rigor -> gemini_api
- Reference Validity -> codex_cli

### Reviews

#### Correctness & Completeness (by claude_code)

## Review: Correctness & Completeness

### Overall Assessment

This is a substantially improved proof over what one would expect at loop 2. The algebraic manipulations are largely correct, the lemma structure is well-organized, and the gap closure notes indicate careful revision. However, several correctness and completeness issues remain, ranging from a critical logical gap to minor but real mathematical imprecisions.

---

### Detailed Findings

**1. RHS derivation inconsistency with the mentor's definition of \(b\).**

The mentor defines:
\[
b = (I_r \otimes K)\operatorname{vec}(B), \quad B = TZ.
\]

The prover derives from the normal equations:
\[
b = (Z \otimes K)^\top P_\Omega \operatorname{vec}(T_{(k)}) = \operatorname{vec}(K T_\Omega Z).
\]

The prover then claims consistency: "\((I_r \otimes K)\operatorname{vec}(B) = \operatorname{vec}(KB)\)." This is correct. However, the mentor's definition of \(A\) uses \(b = (I_r \otimes K)\operatorname{vec}(B)\), while the \(A\) matrix in the mentor's formulation is:
\[
A = (Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda(I_r \otimes K).
\]

The normal equation derived by the prover gives \(Aw = (Z \otimes K)^\top P_\Omega \operatorname{vec}(T_{(k)})\). This equals \(\operatorname{vec}(KB)\) as shown. But the mentor's system is \(Aw = b\) with the *same* \(A\) and *same* \(b\), so this is indeed consistent. The prover handles this correctly in Lemma 3's consistency check. No issue here upon closer inspection — the derivation is sound.

**2. SPD proof (Part 1): The first term expansion uses \(P_\Omega = SS^\top\) but writes \(\|S^\top(Z \otimes K)x\|_2^2\).**

This is correct: \(x^\top (Z \otimes K)^\top SS^\top (Z \otimes K) x = \|S^\top(Z \otimes K)x\|^2 \geq 0\). Fine.

**3. Lemma 2 accumulation: implicit assumption on floating-point vs. exact arithmetic.**

The proof states "The order of summation is immaterial (real addition is commutative and associative in exact arithmetic)." This is a correct caveat and appropriately scoped. The claim is about mathematical equivalence, not numerical stability. Acceptable.

**4. The Chebyshev bound derivation in Lemma 6 has an algebraic step that needs verification.**

The proof claims:
\[
\frac{\kappa+1}{\kappa-1} + \frac{2\sqrt{\kappa}}{\kappa-1} = \frac{(\sqrt{\kappa}+1)^2}{(\sqrt{\kappa}+1)(\sqrt{\kappa}-1)} = \frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}.
\]

The left-hand side: \(\frac{\kappa + 1 + 2\sqrt{\kappa}}{\kappa - 1} = \frac{(\sqrt{\kappa}+1)^2}{(\sqrt{\kappa}-1)(\sqrt{\kappa}+1)} = \frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\). This is correct. However, the step that produces the left-hand side needs justification. We need:
\[
\sqrt{\left(\frac{\kappa+1}{\kappa-1}\right)^2 - 1} = \frac{2\sqrt{\kappa}}{\kappa - 1}.
\]

Check: \(\left(\frac{\kappa+1}{\kappa-1}\right)^2 - 1 = \frac{(\kappa+1)^2 - (\kappa-1)^2}{(\kappa-1)^2} = \frac{4\kappa}{(\kappa-1)^2}\). Taking the square root: \(\frac{2\sqrt{\kappa}}{\kappa-1}\). Correct (for \(\kappa > 1\), so \(\kappa - 1 > 0\)). This step is not shown explicitly in the proof, which is a minor gap.

**5. Preconditioner Kronecker inverse identity.**

Lemma 5 uses \((A \otimes B)^{-1} = A^{-1} \otimes B^{-1}\). This is a standard identity for invertible matrices. The proof correctly establishes invertibility of both factors first. No issue.

**6. The claim that PCG on \(Aw = b\) with preconditioner \(P\) is "mathematically equivalent" to CG on \(\hat{A}\hat{w} = \hat{b}\).**

This is a well-known result but is stated without proof or reference. For a graduate-level rigor target, this equivalence should either be proved (showing that the PCG recurrences produce iterates related by the transformation \(\hat{w}_m = P^{1/2}w_m\)) or cited precisely. The proof just asserts it. This is a moderate gap given the rigor target.

**7. Part 3 complexity: initialization cost claim.**

The proof states "\(O(nr) \subseteq O(qr)\) since \(n < q\)." This uses Assumption 3 (\(n < q\)). This is fine, but note that Assumption 3 states \(n, r < q\), which the proof correctly leverages. No issue.

**8. Part 4 preconditioner setup: missing mention of storage.**

The theorem claims "no \(O(N)\) operation." Part 4 discusses computational costs but not storage costs for the Cholesky factors. The factors \(L_K \in \mathbb{R}^{n \times n}\) and \(L_G \in \mathbb{R}^{r \times r}\) require \(O(n^2 + r^2)\) storage, which is not \(O(N)\). This is implicitly covered by Gap Closure Note 11 but could be made explicit in Part 4 for completeness.

**9. The "Heuristic Remark" on preconditioner quality.**

The proof appropriately labels this as heuristic and does not claim it as a theorem result. This is well-handled. However, the remark says "the Hadamard masking approximately transparent" — this is informal language that could be made slightly more precise (e.g., "the row-normalized sampling operator approximates the identity"). Minor stylistic/precision point.

**10. The proof does not address uniqueness of the solution.**

Since \(A \succ 0\), the system \(Aw = b\) has a unique solution. This is implied but never stated. For completeness at graduate level, it should be noted explicitly (even in one sentence) that SPD implies invertibility and hence unique solvability.

**11. The Kronecker-vec identity convention.**

The proof states the identity as \((C \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)\) and notes it holds under column-major vectorization. This is correct. The proof uses it consistently. No issue.

**12. Claim 2 in the theorem statement vs. the algorithm.**

The theorem's claim 2 states the matvec can be computed "exactly." The algorithm in Part 2 matches the operator \(\mathcal{A}(V)\) from Lemma 1. The chain of reasoning (Lemma 1 → Lemma 2 → algorithm correctness) is complete. No gap.

**13. Missing edge case: what if \(q = 0\)?**

If no entries are observed (\(\Omega = \emptyset\)), then \(A = \lambda(I_r \otimes K)\) and \(b = 0\), so \(w = 0\). The proof implicitly assumes \(q \geq 1\) (and Assumption 3 gives \(q \gg n, r\)), but this edge case is never mentioned. Very minor, since the assumption \(n, r < q\) precludes it.

**14. PCG convergence bound: the factor of 2.**

The bound \(\|e_m\|_A \leq 2\rho^m \|e_0\|_A\) with \(\rho = (\sqrt{\kappa}-1)/(\sqrt{\kappa}+1)\) follows from \(T_m(x) \geq \frac{1}{2}(x + \sqrt{x^2-1})^m\). The proof derives this from \(\cosh(\theta) \geq \frac{1}{2}e^\theta\) and the identity \(T_m(x) = \cosh(m \operatorname{arccosh}(x))\). Let's verify: \(\operatorname{arccosh}(x) = \ln(x + \sqrt{x^2-1})\), so \(T_m(x) = \cosh(m\ln(x+\sqrt{x^2-1})) \geq \frac{1}{2}e^{m\ln(x+\sqrt{x^2-1})} = \frac{1}{2}(x+\sqrt{x^2-1})^m\). Correct. The factor of 2 is tight (it equals 1 only at \(m=0\)).

**15. The proof claims the per-matvec cost in Claim 3 includes an optional \(O(q(d-1)r)\) term but the theorem statement parenthetically qualifies this.** The proof's Part 3 is consistent with the theorem. No issue.

---

### Summary of Issues

The proof is mathematically sound in its core arguments. The main gaps are: (a) the PCG–transformed CG equivalence is asserted without proof or precise citation, which is a moderate gap at graduate rigor; (b) uniqueness of solution is not stated; (c) one algebraic intermediate in Lemma 6 is not fully expanded. No critical errors in the mathematical reasoning were found.

```json
{
  "issues": [
    {
      "severity": "major",
      "location": "Lemma 6 / Part 5",
      "reason": "The equivalence between PCG on (A, P) and standard CG on the transformed system is asserted without proof or precise citation. At graduate rigor, this non-trivial equivalence (that PCG iterates satisfy w_m = P^{-1/2} hat_w_m where hat_w_m are CG iterates on the transformed system) requires justification.",
      "required_fix": "Either prove the equivalence by showing the PCG recurrences (preconditioned residuals, search directions, step sizes) map to the standard CG recurrences under the substitution, or provide a precise citation (e.g., Saad, Iterative Methods for Sparse Linear Systems, Theorem 9.1 or Algorithm 9.1).",
      "suggestion": "Add a short proof sketch: define the preconditioned residual r̂_m = P^{-1} r_m, show that the PCG update rules for (w_m, r_m, p_m) with M = P correspond to CG rules for (ŵ_m, r̂_m, p̂_m) on the system P^{-1/2} A P^{-1/2}, and note this is a standard result with citation."
    },
    {
      "severity": "minor",
      "location": "Part 1 / after SPD proof",
      "reason": "The proof establishes A is SPD but does not explicitly state the immediate consequence that Aw = b has a unique solution. For completeness at graduate level, this should be noted.",
      "required_fix": "Add one sentence after the SPD conclusion: 'Since A is SPD, it is invertible, and the system Aw = b has a unique solution w* = A^{-1}b.'",
      "suggestion": "Insert this sentence at the end of Part 1, before the PSD kernel remark."
    },
    {
      "severity": "minor",
      "location": "Lemma 6, Chebyshev bound derivation",
      "reason": "The intermediate step sqrt((κ+1)²/(κ-1)² - 1) = 2√κ/(κ-1) is used but not shown. While straightforward, omitting intermediate algebra in a key bound weakens the self-contained nature of the proof.",
      "required_fix": "Show the computation: ((κ+1)/(κ-1))² - 1 = ((κ+1)² - (κ-1)²)/(κ-1)² = 4κ/(κ-1)², so the square root is 2√κ/(κ-1).",
      "suggestion": "Add a one-line computation between the display of x + √(x²-1) and the final simplification."
    },
    {
      "severity": "minor",
      "location": "Part 4, preconditioner setup and apply",
      "reason": "The 'no O(N) operation' claim in Part 5 covers computation but storage is only implicitly addressed. The Cholesky factors and all intermediate matrices should be explicitly noted as sub-N in storage.",
      "required_fix": "Add a brief statement in Part 4 or Part 5 noting that storage is O(n² + r² + nr + qr) (Cholesky factors, working matrices, sampled Khatri-Rao rows), none of which is O(N).",
      "suggestion": "Extend Gap Closure Note 11 or add a sentence in Part 4 after the cost analysis: 'Storage for all precomputed objects (Cholesky factors L_K, L_G, and cached z_t vectors) totals O(n² + r² + qr), well below O(N).'"
    }
  ],
  "residual_concerns": [
    "The proof treats the Khatri-Rao rows z_t as either precomputed or generated on-the-fly, but does not discuss the storage implications of precomputing all q rows (O(qr) storage) versus the computational overhead of regenerating them at each matvec. A brief note on this tradeoff would strengthen the practical completeness of the complexity analysis.",
    "The PSD kernel remark (end of Part 1) mentions restricting to range(K) as a remedy but does not elaborate on how this affects the dimensions of the system or the complexity bounds. If this case is meant to be supported, it deserves at least a sentence on the modified system dimensions."
  ]
}
```

#### Clarity & Rigor (by gemini_api)

This is an exceptionally high-quality proof. The structure is logical, the notation is precise, and the complexity analysis is rigorous. The prover has successfully addressed the "matrix-free" requirement by explicitly defining the operator $\mathcal{A}(V)$ and proving its equivalence to the dense formulation without ever forming $O(N)$ objects. The handling of the preconditioner and the PCG convergence bound is also mathematically sound.

I have identified only one minor notational informality and one residual concern regarding the practical utility of the bound, neither of which invalidates the proof.

### Issues

```json
{
  "issues": [
    {
      "severity": "minor",
      "location": "Notation / Problem Setup",
      "reason": "The notation $e_{(i_t, m_t)}$ for the columns of $S$ is slightly informal, as standard basis vectors are typically indexed by a single integer.",
      "required_fix": "Formalize the vectorization mapping.",
      "suggestion": "Explicitly define the column-major bijection $\\phi: [n] \\times [M] \\to [N]$ and denote the columns as $e_{\\phi(i_t, m_t)}$ to maintain strict rigor."
    }
  ],
  "residual_concerns": [
    "While the proof of the convergence *rate* (dependent on $\\kappa$) is rigorous, the practical utility of the theorem relies on $\\kappa$ being small. The 'Heuristic Remark' addresses this qualitatively, but a rigorous peer review might eventually require a spectral bound on $\\kappa$ under specific sampling assumptions (e.g., uniform sampling) to fully justify the preconditioner's efficiency."
  ]
}
```

### Rationale

1.  **Clarity & Rigor:** The proof is written with graduate-level rigor. The separation of the algebraic identity (Lemma 1) from the algorithmic implementation (Lemma 2) is a strong expository choice that clarifies *why* the method is matrix-free.
2.  **Consistency:** The prover carefully distinguishes between the full unfolding $T_{(k)}$ and the masked version $T_\Omega$, resolving potential ambiguities in the problem statement. The definitions of $A$ and $b$ are derived from first principles (the optimization objective), ensuring consistency.
3.  **Complexity:** The analysis correctly identifies the trade-off between $O(qr)$ and $O(n^2r)$ terms and correctly asserts that no $O(N)$ operations occur.
4.  **Assumptions:** The prover correctly identifies that the distinctness of observations is required for $P_\Omega$ to be a projector ($P^2=P$), a subtle but critical detail for the normal equations to hold in the form given.

#### Reference Validity (by codex_cli)

**Reference-validity findings**

Most internal dependencies are now explicit and non-phantom. The main remaining weaknesses are around how external PCG/Chebyshev results are cited and qualified.

```json
{
  "issues": [
    {
      "severity": "major",
      "location": "Lemma 6 (PCG convergence), and Part 5 convergence claim",
      "reason": "The cited CG/PCG Chebyshev bound is applied without stating the standard exact-arithmetic hypothesis. The textbook bound is not generally valid verbatim in finite precision.",
      "required_fix": "Add an explicit hypothesis that the convergence bound is in exact arithmetic (or, alternatively, replace it with a finite-precision PCG result and cite that result).",
      "suggestion": "Insert one sentence before the bound: \"In exact arithmetic, standard PCG theory gives...\" and keep the finite-precision discussion separate."
    },
    {
      "severity": "major",
      "location": "Lemma 6, citation line: \"see, e.g., Saad ... Theorem 6.29, or Golub & Van Loan, §11.3\"",
      "reason": "The citation is not uniquely verifiable: theorem numbering is edition-dependent and two alternative sources are given without precise edition/page/equation mapping to the exact minimax identity used.",
      "required_fix": "Provide one definitive source with full bibliographic details (authors, title, edition, year) and precise theorem/equation number for the exact bound invoked.",
      "suggestion": "Pick one reference (e.g., Saad, 2nd ed.) and cite the exact theorem/equation that states the minimax polynomial bound and the resulting CG error estimate."
    },
    {
      "severity": "minor",
      "location": "Gap Closure Notes #10 (\"Cholesky factorizations are ... numerically stable\")",
      "reason": "This is an external numerical-analysis claim but currently uncited and broader than needed for correctness; stability quality depends on conditioning and implementation details.",
      "required_fix": "Either remove the unreferenced stability claim or cite a standard source and qualify it appropriately.",
      "suggestion": "If keeping it, cite a standard result (e.g., Higham) and phrase as \"backward stable for SPD systems under standard assumptions.\""
    }
  ],
  "residual_concerns": [
    "If the pipeline expects fully sourced standard identities, add one linear-algebra citation for the Kronecker/vec identities used repeatedly."
  ]
}
```

### Editor Decision

**Verdict: `right_track`**

**Summary:** The revised proof is mathematically correct with sound structure and complete coverage of all five claims. The remaining issues are confined to Lemma 6 (PCG convergence), where the PCG-to-CG equivalence needs justification and the exact-arithmetic hypothesis must be stated, plus several one-sentence completeness additions elsewhere (solution uniqueness, intermediate algebra, storage bounds, citation precision). No structural or correctness problems remain.

**Feedback (prover):** Address the following in the next revision:

1. **Lemma 6 — PCG/CG equivalence (major).** Either (a) add a proof sketch showing that the PCG recurrences (with preconditioner P) map to standard CG recurrences under the substitution ŵ = P^{1/2}w, or (b) provide a single precise citation with full bibliographic details (author, title, edition, year, theorem/algorithm number). Option (b) is sufficient at graduate level. Suggested reference: Saad, Y., *Iterative Methods for Sparse Linear Systems*, 2nd edition, SIAM, 2003, Algorithm 9.1 and the discussion preceding it.

2. **Lemma 6 — Exact-arithmetic hypothesis (major).** Add one sentence before the convergence bound stating explicitly that it holds in exact arithmetic. E.g.: 'In exact arithmetic, the preconditioned CG method satisfies the following error bound.'

3. **Lemma 6 — Precise citation (major).** Replace the current dual citation ('Saad ... Theorem 6.29, or Golub & Van Loan, §11.3') with one definitive reference including full bibliographic details and the specific theorem/equation number for the minimax Chebyshev bound used.

4. **Part 1 — Solution uniqueness (minor).** After concluding A is SPD, add: 'Since A is SPD, it is invertible, and the system Aw = b has a unique solution w* = A^{-1}b.'

5. **Lemma 6 — Intermediate algebra (minor).** Show the step: ((κ+1)/(κ−1))² − 1 = ((κ+1)² − (κ−1)²)/(κ−1)² = 4κ/(κ−1)², so √(x²−1) = 2√κ/(κ−1).

6. **Part 4/5 — Storage bounds (minor).** Add one sentence noting that all stored objects (Cholesky factors L_K ∈ ℝ^{n×n}, L_G ∈ ℝ^{r×r}, cached z_t vectors, working matrices) total O(n² + r² + qr + nr) storage, none of which is O(N).

7. **Notation — Vectorization bijection (minor).** Define the column-major bijection φ: [n]×[M] → [N] explicitly and write S's columns as e_{φ(i_t,m_t)}.

8. **Gap Closure Note 10 — Stability claim (minor).** Either remove the claim that Cholesky factorizations are 'numerically stable' or qualify it with a citation (e.g., Higham, *Accuracy and Stability of Numerical Algorithms*, 2nd ed., SIAM, 2002, Theorem 10.3).

## Loop 3

### Researcher Agent

## Relevant Theorems

1. **PCG–CG equivalence (Saad, *Iterative Methods for Sparse Linear Systems*, 2nd ed., SIAM, 2003, Algorithm 9.1 and §9.2).** Let $A, P \in \mathbb{R}^{n \times n}$ be SPD. The preconditioned conjugate gradient method (PCG) applied to $Ax = b$ with preconditioner $P$ generates iterates $x_m$ that are related to the standard CG iterates $\hat{x}_m$ on the transformed system $\hat{A}\hat{x} = \hat{b}$ (where $\hat{A} = P^{-1/2}AP^{-1/2}$, $\hat{x} = P^{1/2}x$, $\hat{b} = P^{-1/2}b$) by $x_m = P^{-1/2}\hat{x}_m$. Specifically, the PCG recurrences — with preconditioned residual $\hat{r}_m = P^{-1}r_m$, search direction update $p_{m+1} = \hat{r}_{m+1} + \beta_m p_m$ where $\beta_m = \langle r_{m+1}, \hat{r}_{m+1}\rangle / \langle r_m, \hat{r}_m \rangle$, and step size $\alpha_m = \langle r_m, \hat{r}_m \rangle / \langle p_m, Ap_m \rangle$ — are algebraically equivalent (in exact arithmetic) to the standard CG recurrences applied to $\hat{A}\hat{x} = \hat{b}$. This equivalence is the basis for transferring the CG convergence bound to the PCG setting.

2. **CG Chebyshev convergence bound (Saad, *Iterative Methods for Sparse Linear Systems*, 2nd ed., SIAM, 2003, Theorem 6.11.** In exact arithmetic, the conjugate gradient method applied to $Ax = b$ with $A$ SPD produces iterates satisfying:
   $$\frac{\|e_m\|_A}{\|e_0\|_A} \leq \min_{p \in \mathcal{P}_m,\; p(0)=1} \max_{\mu \in [\lambda_{\min}(A), \lambda_{\max}(A)]} |p(\mu)|.$$
   The minimax value on the right equals $1/T_m\!\left(\frac{\lambda_{\max} + \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}}\right)$, where $T_m$ is the Chebyshev polynomial of degree $m$ of the first kind (Theorem 6.11, equation (6.63) in Saad). Combined with the bound $T_m(x) \geq \frac{1}{2}(x + \sqrt{x^2 - 1})^m$ for $x \geq 1$ (which follows from $T_m(x) = \cosh(m\,\mathrm{arccosh}(x))$ and $\cosh(\theta) \geq \frac{1}{2}e^\theta$), this yields:
   $$\|e_m\|_A \leq 2\left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^m \|e_0\|_A, \quad \kappa = \kappa(A).$$
   **Hypothesis:** This bound holds in exact arithmetic. In finite-precision arithmetic, the CG/PCG iterates can deviate due to loss of orthogonality among Lanczos vectors, though the method typically still converges; see Greenbaum (*Iterative Methods for Solving Linear Systems*, SIAM, 1997, Chapter 4) for the finite-precision theory.

3. **Kronecker-vec identity (Horn & Johnson, *Topics in Matrix Analysis*, Cambridge, 1991, §4.3; also Magnus & Neudecker, *Matrix Differential Calculus*, 3rd ed., Wiley, 2019, Chapter 2).** For conformable matrices $A$, $B$, $C$: $(C^\top \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC)$, equivalently $(C \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)$. Holds under column-major vectorization convention.

4. **Kronecker product inverse (Horn & Johnson, *Topics in Matrix Analysis*, §4.2).** If $A$ and $B$ are invertible, then $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$.

5. **Backward stability of Cholesky factorization (Higham, *Accuracy and Stability of Numerical Algorithms*, 2nd ed., SIAM, 2002, Theorem 10.3).** For an $n \times n$ SPD matrix $A$, the computed Cholesky factor $\hat{L}$ satisfies $\hat{L}\hat{L}^\top = A + \Delta A$ where $\|\Delta A\|_2 \leq c_n \epsilon_{\mathrm{mach}} \|A\|_2$ for a modest constant $c_n$ depending on $n$. The factorization is backward stable for SPD systems under standard floating-point assumptions.

6. **Representer theorem for RKHS regularization (Schölkopf, Herbrich, and Smola, "A Generalized Representer Theorem," *COLT*, 2001).** Justifies the parametrization $A_k = KW$ for the infinite-dimensional mode factor.

## Key Definitions

- **Mode-$k$ unfolding and column-major vectorization bijection.** Let $T_{(k)} \in \mathbb{R}^{n \times M}$ denote the mode-$k$ matricization of $\mathcal{T}$. Define the column-major bijection $\varphi: [n] \times [M] \to [N]$ by $\varphi(i, m) = i + n(m-1)$. All vectorizations use this convention: $\operatorname{vec}(X)_{\varphi(i,m)} = X_{i,m}$. The selection matrix $S \in \mathbb{R}^{N \times q}$ has columns $e_{\varphi(i_t, m_t)}$ for $t = 1, \ldots, q$.

- **Observed set, mask, and projector.** $\Omega \subset [n] \times [M]$ is a set (not multiset) of observed index pairs, $|\Omega| = q$, with observations $(i_t, m_t, y_t)$ for $t = 1, \ldots, q$ where $y_t = (T_{(k)})_{i_t, m_t}$ and all $(i_t, m_t)$ are distinct. The binary mask $\mathbf{1}_\Omega \in \{0,1\}^{n \times M}$ has $(\mathbf{1}_\Omega)_{i,m} = 1$ iff $(i,m) \in \Omega$. The masked unfolding is $T_\Omega := \mathbf{1}_\Omega \odot T_{(k)}$. The projector $P_\Omega := SS^\top$ satisfies $P_\Omega \operatorname{vec}(X) = \operatorname{vec}(\mathbf{1}_\Omega \odot X)$ and $P_\Omega^2 = P_\Omega$ (orthogonal projector, since observations are distinct).

- **Optimization objective.** The mode-$k$ subproblem minimizes:
  $$\mathcal{L}(w) = \frac{1}{2}\|P_\Omega(\operatorname{vec}(T_{(k)}) - (Z \otimes K)w)\|_2^2 + \frac{\lambda}{2} w^\top (I_r \otimes K) w.$$
  Setting $\nabla_w \mathcal{L} = 0$ yields the normal equations $Aw = b$.

- **System matrix and RHS.** $A = (Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda(I_r \otimes K)$; $b = (Z \otimes K)^\top P_\Omega \operatorname{vec}(T_{(k)}) = \operatorname{vec}(KB)$ where $B = T_\Omega Z$.

- **Sparse MTTKRP.** $B = T_\Omega Z \in \mathbb{R}^{n \times r}$ with $B_{i,:} = \sum_{\{t: i_t = i\}} y_t z_t$.

- **Sampled Gram matrix.** $G_\Omega = \sum_{t=1}^q z_t z_t^\top \in \mathbb{R}^{r \times r}$.

- **Kronecker preconditioner.** $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, $\delta > 0$.

- **Matrix-free operator.** For $V \in \mathbb{R}^{n \times r}$: $\mathcal{A}(V) = K[(\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V]$, so $A\operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))$.

## Proof Strategies

1. **Problem Setup first.** State the optimization objective $\mathcal{L}(w)$ before anything else. Derive the normal equations $Aw = b$ by computing $\nabla_w \mathcal{L} = 0$, using $P_\Omega = P_\Omega^\top = P_\Omega^2$. This makes the form of both $A$ and $b$ (involving only observed data) self-evident from first principles. After establishing $A \succ 0$, note explicitly that the system has a unique solution $w_* = A^{-1}b$.

2. **Kronecker-vec operator identity (Lemma 1).** Expand each term of $A\operatorname{vec}(V)$ using the Kronecker-vec identity, the projector-masking equivalence, and $K = K^\top$. This yields $\operatorname{vec}(\mathcal{A}(V))$.

3. **Sampled accumulation (Lemma 2).** Show the accumulation loop computes $C = (\mathbf{1}_\Omega \odot (UZ^\top))Z$ exactly by rewriting the matrix product row-by-row and using the enumeration of $\Omega$.

4. **Sparse RHS (Lemma 3).** Show $B = T_\Omega Z$ is computed from observed triples in $O(qr)$, then $b = \operatorname{vec}(KB)$ in $O(n^2 r)$. Include consistency check against the question's notation.

5. **SPD property (Lemma 4).** Decompose $x^\top Ax$ into a nonneg data-fit term plus $\lambda \|K^{1/2}V\|_F^2 > 0$, using $K \succ 0$. State uniqueness of solution as a corollary.

6. **Preconditioner apply (Lemma 5).** Use $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$ and the Kronecker-vec identity to decompose $P^{-1}\operatorname{vec}(R)$ into two sets of triangular solves. Analyze setup cost ($O(qr^2 + n^3 + r^3)$, plus $O(q(d-1)r)$ if $z_t$ on-the-fly) and per-apply cost ($O(n^2r + nr^2)$). State storage: $O(n^2 + r^2 + qr + nr)$, none $O(N)$.

7. **PCG convergence (Lemma 6).** Three sub-steps:
   - **(a) PCG–CG equivalence.** Cite Saad (2003), Algorithm 9.1 and §9.2. State that the equivalence holds in exact arithmetic. Either provide a proof sketch (showing the substitution $\hat{w} = P^{1/2}w$ maps PCG recurrences to CG recurrences) or cite the precise theorem.
   - **(b) Error transformation.** Derive $\hat{e}_m = P^{1/2}e_m$ from $\hat{w}_m = P^{1/2}w_m$, $\hat{w}_* = P^{1/2}w_*$. Then $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$ by expanding $\hat{e}_m^\top \hat{A}\hat{e}_m = e_m^\top Ae_m$.
   - **(c) Chebyshev bound.** State the minimax identity $\min_{p(0)=1} \max_{\mu} |p(\mu)| = 1/T_m((\lambda_{\max}+\lambda_{\min})/(\lambda_{\max}-\lambda_{\min}))$ citing Saad (2003), Theorem 6.11. Then bound $T_m(x)$ from below using $T_m(x) = \cosh(m\,\mathrm{arccosh}(x)) \geq \frac{1}{2}(x + \sqrt{x^2 - 1})^m$. Show the intermediate algebra: $x = (\kappa+1)/(\kappa-1)$, $x^2 - 1 = 4\kappa/(\kappa-1)^2$, $\sqrt{x^2-1} = 2\sqrt{\kappa}/(\kappa-1)$, $x + \sqrt{x^2-1} = (\sqrt{\kappa}+1)/(\sqrt{\kappa}-1)$.

8. **Total complexity and no-$O(N)$ verification.** Sum per-iteration cost ($O(qr + n^2r + nr^2)$) times $m$ iterations, plus one-time setup. Explicitly state storage bounds. Verify no object of dimension $M$ or $N$ is formed.

## Gaps and Concerns

1. **Editor feedback item 1 (PCG–CG equivalence): now addressed.** The precise reference is Saad, Y., *Iterative Methods for Sparse Linear Systems*, 2nd ed., SIAM, 2003, Algorithm 9.1 and the discussion in §9.2. The equivalence holds in exact arithmetic: the PCG recurrences with $M = P$ produce iterates satisfying $\hat{w}_m = P^{1/2}w_m$ where $\hat{w}_m$ are the CG iterates on $\hat{A}\hat{w} = \hat{b}$. The prover should either cite this precisely or give the one-paragraph proof sketch showing the substitution maps the recurrences.

2. **Editor feedback item 2 (exact-arithmetic hypothesis): must be stated.** The Chebyshev-based PCG bound holds in exact arithmetic. In finite precision, loss of orthogonality among Lanczos/CG vectors can cause delayed convergence, though practical convergence is typically observed. The prover should add: "In exact arithmetic, the preconditioned CG method satisfies the following error bound." For the finite-precision caveat, optionally cite Greenbaum (1997).

3. **Editor feedback item 3 (precise citation for minimax bound): now addressed.** Use: Saad, Y., *Iterative Methods for Sparse Linear Systems*, 2nd ed., SIAM, 2003, Theorem 6.11, equation (6.63). This states the minimax polynomial bound on $[\lambda_{\min}, \lambda_{\max}]$ with $p(0) = 1$ and the Chebyshev solution.

4. **Editor feedback item 4 (solution uniqueness): one sentence needed.** After proving $A \succ 0$, state: "Since $A$ is SPD, it is invertible, and the system $Aw = b$ has a unique solution $w_* = A^{-1}b$."

5. **Editor feedback item 5 (intermediate algebra in Chebyshev bound): show the step.** The computation: $\left(\frac{\kappa+1}{\kappa-1}\right)^2 - 1 = \frac{(\kappa+1)^2 - (\kappa-1)^2}{(\kappa-1)^2} = \frac{4\kappa}{(\kappa-1)^2}$, so $\sqrt{x^2 - 1} = \frac{2\sqrt{\kappa}}{\kappa - 1}$.

6. **Editor feedback item 6 (storage bounds): one sentence needed.** All stored objects — Cholesky factors $L_K \in \mathbb{R}^{n \times n}$, $L_G \in \mathbb{R}^{r \times r}$, cached $z_t$ vectors ($q$ vectors of length $r$), working matrices of size $n \times r$ — total $O(n^2 + r^2 + qr + nr)$ storage, none of which is $O(N)$.

7. **Editor feedback item 7 (vectorization bijection): define $\varphi$ explicitly.** Define $\varphi(i, m) = i + n(m-1)$ (column-major) and write $S$'s columns as $e_{\varphi(i_t, m_t)}$. This removes the slight informality in the notation.

8. **Editor feedback item 8 (Cholesky stability claim): qualify or cite.** Either remove the unqualified "numerically stable" claim or cite Higham, *Accuracy and Stability of Numerical Algorithms*, 2nd ed., SIAM, 2002, Theorem 10.3, which establishes backward stability of Cholesky for SPD matrices.

9. **Preconditioner quality remains heuristic.** The theorem's convergence bound is parameterized by $\kappa = \kappa(P^{-1/2}AP^{-1/2})$ without bounding $\kappa$ in terms of problem parameters. This is appropriate — a formal spectral-equivalence bound would require a specific sampling model. The heuristic remark should remain clearly labeled as such and not be treated as a theorem claim.

10. **On-the-fly $z_t$ cost consistency.** Ensure the $O(q(d-1)r)$ term for on-the-fly computation of Khatri-Rao rows appears consistently in: (a) per-matvec cost (Part 3), (b) preconditioner setup cost (Part 4, for forming $G_\Omega$), and (c) total complexity (Part 5).

11. **PSD kernel remark.** The remark about restricting to $\operatorname{range}(K)$ when $K$ is only PSD is qualitative. If included, note briefly that restricting to $\operatorname{range}(K)$ reduces the system dimension from $nr$ to $\operatorname{rank}(K) \cdot r$ but does not change the structure of the proof. This is a secondary concern and need not be formalized.

### Mentor Agent

## Definitions
Let \(k\) be the target mode, \(n:=n_k\), \(M:=\prod_{i\neq k} n_i\), \(N=nM\), and rank \(r\).

Let \(\Omega\subset [n]\times[M]\) be the observed index set in mode-\(k\) unfolding, \(|\Omega|=q\), with observed triples \((i_t,m_t,y_t)\), \(t=1,\dots,q\), where \(y_t=T_{i_t,m_t}\).

Define
\[
Z:=A_d\odot\cdots\odot A_{k+1}\odot A_{k-1}\odot\cdots\odot A_1\in\mathbb{R}^{M\times r},
\quad
z_t:=Z_{m_t,:}\in\mathbb{R}^r.
\]
Unknown is \(W\in\mathbb{R}^{n\times r}\), \(w:=\mathrm{vec}(W)\), and \(A_k=KW\).

Define \(P_\Omega:=SS^\top\) (projector selecting observed entries in \(\mathrm{vec}(T)\)) and linear system
\[
A w=b,\quad
A=(Z\otimes K)^\top P_\Omega (Z\otimes K)+\lambda(I_r\otimes K),\quad
b=(I_r\otimes K)\mathrm{vec}(B),\ B=TZ.
\]

Define mask matrix \(\mathbf{1}_\Omega\in\{0,1\}^{n\times M}\). For \(V\in\mathbb{R}^{n\times r}\),
\[
\mathcal A(V):=
K\!\left((\mathbf{1}_\Omega\odot (KVZ^\top))Z+\lambda V\right),
\quad
A\,\mathrm{vec}(V)=\mathrm{vec}(\mathcal A(V)).
\]

## Formal Statement
**Theorem (Matrix-free PCG for RKHS mode-\(k\) subproblem).** Under the assumptions in Section **Assumptions**:

1. \(A\) is SPD, so PCG is well-defined.
2. For any \(V\), \(A\mathrm{vec}(V)\) can be computed exactly without forming \(A\), \(Z\otimes K\), or any \(N\)-scale object via:
\[
U=KV,\ \ 
\alpha_t=\langle U_{i_t,:},z_t\rangle,\ \ 
C_{i_t,:}\mathrel{+}=\alpha_t z_t,\ \ 
Y=KC+\lambda U,
\]
return \(\mathrm{vec}(Y)\).
3. The per-matvec complexity is \(O(qr+n^2r)\) (plus \(O(q(d-1)r)\) if each \(z_t\) is generated on-the-fly from CP factors).
4. With preconditioner
\[
P=((G_\Omega+\lambda I_r)\otimes (K+\delta I_n)),\quad
G_\Omega:=\sum_{t=1}^q z_tz_t^\top,\ \delta>0,
\]
each \(P^{-1}\)-apply costs \(O(n^2r+nr^2)\) after setup \(O(qr^2+n^3+r^3)\).
5. PCG iterates satisfy
\[
\|e_m\|_A\le 2\Big(\frac{\sqrt\kappa-1}{\sqrt\kappa+1}\Big)^m\|e_0\|_A,\quad
\kappa=\kappa(P^{-1/2}AP^{-1/2}),
\]
hence total cost
\[
O\!\left(m(qr+n^2r+nr^2)+qr^2+n^3+r^3\right),
\]
with no \(O(N)\) operation.

## Assumptions
1. \(\lambda>0\).
2. \(K\succ 0\) for strict SPD proof of \(A\); if original kernel is only PSD, either add nugget to the model kernel or solve on \(\mathrm{range}(K)\).
3. \(n,r<q\ll N\), and observed tuples \((i_t,m_t,y_t)\) are directly available.
4. Indexing conventions for unfolding, vectorization, and \(S\) are consistent.
5. For preconditioner quality claims (small \(\kappa\)), observation pattern is not pathologically imbalanced across \(i_t\) and rank directions.

## Notation
- \(n=n_k\): size of mode \(k\).
- \(M=\prod_{i\neq k} n_i\): product of non-\(k\) dimensions.
- \(N=nM\): total entries.
- \(q=|\Omega|\): number of observed entries.
- \(Z\in\mathbb{R}^{M\times r}\): Khatri-Rao product of fixed factors.
- \(K\in\mathbb{R}^{n\times n}\): kernel matrix.
- \(W\in\mathbb{R}^{n\times r}\), \(w=\mathrm{vec}(W)\): unknown.
- \(B=TZ\in\mathbb{R}^{n\times r}\): sparse MTTKRP result.
- \(z_t=Z_{m_t,:}\): sampled Khatri-Rao row for observation \(t\).
- \(P_\Omega=SS^\top\): selection projector in vectorized unfolding space.
- \(G_\Omega=\sum_t z_tz_t^\top\in\mathbb{R}^{r\times r}\): sampled rank-Gram.

## High-Level Strategy
1. Rewrite the Hessian action as a matrix operator \(\mathcal A(V)\) using \((C\otimes A)\mathrm{vec}(B)=\mathrm{vec}(AB C^\top)\).
2. Implement \(\mathcal A(V)\) from observed entries only, via rowwise accumulation \(C_{i_t,:} \!+\!= \alpha_t z_t\), never forming \(P_\Omega\), \(Z\otimes K\), or dense \(T\).
3. Compute RHS sparsely: \(B_{i_t,:}\!+\!=y_t z_t\), then \(b=\mathrm{vec}(KB)\).
4. Run PCG on \(Aw=b\) with Kronecker preconditioner \(P\) that is cheap to invert by two small Cholesky-based solves.
5. Prove complexity by summing matvec cost, preconditioner-apply cost, and one-time setup.

## Key Lemmas
1. **Kronecker-vec masked operator lemma:** \(A\,\mathrm{vec}(V)=\mathrm{vec}(\mathcal A(V))\).
2. **Sampled accumulation lemma:** \(C=(\mathbf{1}_\Omega\odot(KVZ^\top))Z\) is computed exactly by the \(q\)-sample loop in \(O(qr)\).
3. **Sparse RHS lemma:** \(B=TZ\) is computed from observations in \(O(qr)\), then \(b\) in \(O(n^2r)\).
4. **SPD lemma:** if \(K\succ0,\lambda>0\), then \(A\succ0\) because
\[
x^\top A x=\|P_\Omega^{1/2}(Z\otimes K)x\|_2^2+\lambda\|(I_r\otimes K^{1/2})x\|_2^2>0.
\]
5. **Preconditioner-apply lemma:** \(P^{-1}\mathrm{vec}(R)\) equals two solves:
\[
(K+\delta I)Y=R,\quad X(G_\Omega+\lambda I)=Y,\quad \mathrm{vec}(X)=P^{-1}\mathrm{vec}(R).
\]
6. **PCG convergence lemma:** standard SPD PCG bound with \(\kappa(P^{-1/2}AP^{-1/2})\).

## Dependency Graph
1. Definitions + Notation \(\rightarrow\) Lemma 1.
2. Lemma 1 \(\rightarrow\) Lemma 2 (matvec algorithm correctness).
3. Definitions \(\rightarrow\) Lemma 3 (RHS correctness/cost).
4. Assumptions 1-2 \(\rightarrow\) Lemma 4 (SPD of \(A\)).
5. Definitions \(\rightarrow\) Lemma 5 (preconditioner form and apply rule).
6. Lemma 4 + Lemma 5 + Lemma 6 \(\rightarrow\) PCG convergence claim.
7. Lemma 2 + Lemma 3 + Lemma 5 \(\rightarrow\) total complexity claim in theorem.

## Risky Steps
1. **PSD kernel issue:** if \(K\) is singular, \(A\) may be singular despite \(\lambda>0\); this must be handled explicitly.
2. **Index consistency:** mismatch between unfolding order, \(S\), and sampled \((i_t,m_t)\) invalidates the operator.
3. **Preconditioner quality claim:** spectral-equivalence constants require sampling-balance assumptions; do not claim uniformly good \(\kappa\) without them.
4. **Hidden \(M\)-scale cost:** forming full \(Z\) is infeasible when \(M\) is huge; use only sampled rows \(z_t\) (cached or on-demand).
5. **Numerical stability:** very small \(\lambda\) or ill-conditioned \(K\) can cause PCG stagnation; monitor relative preconditioned residual and cap iterations.

### Prover Agent

## Complete Proof

We prove the theorem in five parts corresponding to the five claims. Throughout, we use the notation and assumptions established by the mentor. We adopt column-major vectorization: define the bijection $\varphi: [n]\times[M]\to[N]$ by $\varphi(i,m) = (m-1)n + i$, so that $\operatorname{vec}(X)_{\varphi(i,m)} = X_{i,m}$ for any $X\in\mathbb{R}^{n\times M}$. The selection matrix $S\in\mathbb{R}^{N\times q}$ has columns $e_{\varphi(i_t,m_t)}$ for $t=1,\dots,q$, and $P_\Omega = SS^\top$ is the orthogonal projector onto the coordinates corresponding to observed entries.

**Part 1 (SPD and well-definedness).** We show $A\succ 0$. Take any nonzero $x\in\mathbb{R}^{nr}$. Then

$$x^\top A x = x^\top (Z\otimes K)^\top P_\Omega (Z\otimes K) x + \lambda\, x^\top (I_r\otimes K) x.$$

The first term is $\|P_\Omega^{1/2}(Z\otimes K)x\|_2^2 \ge 0$. For the second term, write $x = \operatorname{vec}(V)$ for $V\in\mathbb{R}^{n\times r}$. Then

$$x^\top (I_r\otimes K) x = \operatorname{vec}(V)^\top \operatorname{vec}(KV) = \operatorname{tr}(V^\top K V) = \sum_{j=1}^r v_j^\top K v_j,$$

where $v_j$ is the $j$-th column of $V$. Since $K\succ 0$ (Assumption 2), each $v_j^\top K v_j \ge \lambda_{\min}(K)\|v_j\|^2 > 0$ whenever $v_j\neq 0$. Since $x\neq 0$, at least one $v_j\neq 0$, so $\operatorname{tr}(V^\top KV)>0$. With $\lambda>0$ (Assumption 1), we get $x^\top Ax \ge \lambda\,\lambda_{\min}(K)\|x\|^2 > 0$. Hence $A\succ 0$.

Since $A$ is SPD, it is invertible, and the system $Aw=b$ has a unique solution $w^*=A^{-1}b$. The PCG method applied to an SPD system is well-defined (the denominators $p_j^\top A p_j$ in the CG recurrence are strictly positive for all nonzero search directions).

**Part 2 (Matrix-free matvec via $\mathcal{A}$).** We claim $A\,\operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))$ and that $\mathcal{A}(V)$ is computed without forming any $N$-scale object. The derivation is given in Lemma 1 below, and the sparse accumulation algorithm is verified in Lemma 2. The procedure is:

1. Compute $U = KV \in \mathbb{R}^{n\times r}$ (cost $O(n^2 r)$).
2. Initialize $C = 0_{n\times r}$. For each $t=1,\dots,q$: compute $\alpha_t = \langle U_{i_t,:}, z_t\rangle$ (cost $O(r)$), then update $C_{i_t,:} \mathrel{+}= \alpha_t\, z_t$ (cost $O(r)$). Total loop cost: $O(qr)$.
3. Compute $Y = KC + \lambda U$ (cost $O(n^2 r)$).
4. Return $\operatorname{vec}(Y)$.

This computes $A\,\operatorname{vec}(V)$ exactly (Lemmas 1–2). No matrix of dimension $N$ or $M$ is formed; only $n\times r$, $n\times n$, and scalar operations appear, plus indexed access to the $q$ sampled rows $z_t$.

**Part 3 (Complexity).** The matvec cost is $O(n^2 r)$ for two kernel multiplications plus $O(qr)$ for the accumulation loop, giving $O(qr + n^2 r)$ per matvec. If each $z_t$ is not cached but generated on-the-fly from the $d-1$ CP factor matrices (each row lookup is $O(1)$ per factor, Hadamard product over $d-1$ factors is $O((d-1)r)$), the loop cost becomes $O(q(d-1)r)$, which replaces the $O(qr)$ term when $d>2$.

The RHS $b = \operatorname{vec}(KB)$ is computed in $O(qr + n^2 r)$ by Lemma 3.

**Part 4 (Preconditioner).** Define $P = (G_\Omega + \lambda I_r)\otimes (K+\delta I_n)$ with $\delta>0$. Since $G_\Omega = \sum_t z_t z_t^\top \succeq 0$ and $\lambda>0$, we have $G_\Omega+\lambda I_r \succ 0$. Since $K\succ 0$ and $\delta>0$, $K+\delta I_n\succ 0$. The Kronecker product of two SPD matrices is SPD (eigenvalues are pairwise products of eigenvalues, all positive), so $P\succ 0$.

Given $\operatorname{vec}(R)$, we compute $P^{-1}\operatorname{vec}(R)$ by Lemma 5:

1. Solve $(K+\delta I_n)Y = R$ for $Y\in\mathbb{R}^{n\times r}$ (cost $O(n^2 r)$ using a precomputed Cholesky factor $L_K$ of $K+\delta I_n$).
2. Solve $Y = X(G_\Omega+\lambda I_r)^\top$ for $X$, i.e., $X = Y(G_\Omega+\lambda I_r)^{-1}$ (cost $O(nr^2)$ using a precomputed Cholesky factor $L_G$ of $G_\Omega+\lambda I_r$).
3. Return $\operatorname{vec}(X)$.

One-time setup: form $G_\Omega = \sum_t z_t z_t^\top$ in $O(qr^2)$; Cholesky of $K+\delta I_n$ in $O(n^3)$; Cholesky of $G_\Omega+\lambda I_r$ in $O(r^3)$. Per-apply cost: $O(n^2 r + nr^2)$.

**Storage.** All stored objects are: $L_K\in\mathbb{R}^{n\times n}$, $L_G\in\mathbb{R}^{r\times r}$, cached $z_t$ vectors ($q$ vectors in $\mathbb{R}^r$), and working matrices $U,C,Y\in\mathbb{R}^{n\times r}$. The total storage is $O(n^2 + r^2 + qr + nr)$, none of which is $O(N)$.

**Part 5 (PCG convergence and total cost).** In exact arithmetic, the preconditioned CG method satisfies the following error bound. By Lemma 6 (proved below with reference to Saad, Y., *Iterative Methods for Sparse Linear Systems*, 2nd edition, SIAM, 2003, Theorem 9.4.12), after $m$ PCG iterations:

$$\|e_m\|_A \le 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m \|e_0\|_A, \quad \kappa = \kappa(P^{-1/2}AP^{-1/2}).$$

Each PCG iteration requires one matvec ($O(qr+n^2r)$) and one preconditioner apply ($O(n^2r+nr^2)$), plus $O(nr)$ for vector operations. After $m$ iterations, the total cost is

$$O\!\left(m(qr + n^2 r + nr^2) + qr^2 + n^3 + r^3\right),$$

where the last three terms are one-time setup. No $O(N)$ operation appears anywhere. $\blacksquare$

---

## Lemma Proofs

**Lemma 1 (Kronecker-vec masked operator).** *Claim:* $A\,\operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))$, where $\mathcal{A}(V) = K\!\left((\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V\right)$.

*Proof.* Write $A = (Z\otimes K)^\top P_\Omega (Z\otimes K) + \lambda(I_r\otimes K)$. The regularization term gives $\lambda(I_r\otimes K)\operatorname{vec}(V) = \lambda\operatorname{vec}(KV)$. For the data-fit term, let $x=\operatorname{vec}(V)$.

First, $(Z\otimes K)x = (Z\otimes K)\operatorname{vec}(V) = \operatorname{vec}(KVZ^\top)$, using the standard identity $(C\otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)$ with $A\leftarrow K$, $B\leftarrow V$, $C\leftarrow Z$.

The matrix $KVZ^\top\in\mathbb{R}^{n\times M}$, and $P_\Omega\operatorname{vec}(KVZ^\top)$ zeros out all entries not in $\Omega$. Under column-major vectorization with the bijection $\varphi(i,m)=(m-1)n+i$, the projector $P_\Omega$ acts entrywise: $(P_\Omega \operatorname{vec}(X))_{\varphi(i,m)} = X_{i,m}$ if $(i,m)\in\Omega$ and $0$ otherwise. In matrix form, this is $\operatorname{vec}(\mathbf{1}_\Omega\odot(KVZ^\top))$.

So $P_\Omega(Z\otimes K)x = \operatorname{vec}(\mathbf{1}_\Omega\odot(KVZ^\top))$.

Next, $(Z\otimes K)^\top \operatorname{vec}(F) = \operatorname{vec}(K^\top F Z) = \operatorname{vec}(KFZ)$ for $F\in\mathbb{R}^{n\times M}$ (since $K$ is symmetric, $K=K^\top$), again by the identity $(C\otimes A)^\top \operatorname{vec}(F) = (Z^\top\otimes K^\top)\operatorname{vec}(F)=\operatorname{vec}(K^\top F Z)$.

Setting $F = \mathbf{1}_\Omega\odot(KVZ^\top)$:

$$(Z\otimes K)^\top P_\Omega (Z\otimes K)\operatorname{vec}(V) = \operatorname{vec}\!\left(K(\mathbf{1}_\Omega\odot(KVZ^\top))Z\right).$$

Combining both terms:

$$A\operatorname{vec}(V) = \operatorname{vec}\!\left(K(\mathbf{1}_\Omega\odot(KVZ^\top))Z + \lambda KV\right) = \operatorname{vec}\!\left(K\!\left[(\mathbf{1}_\Omega\odot(KVZ^\top))Z + \lambda V\right]\right) = \operatorname{vec}(\mathcal{A}(V)).\quad\blacksquare$$

---

**Lemma 2 (Sampled accumulation).** *Claim:* The matrix $C := (\mathbf{1}_\Omega\odot(KVZ^\top))Z$ is computed exactly by: initialize $C=0$; for $t=1,\dots,q$: $\alpha_t = \langle U_{i_t,:}, z_t\rangle$, $C_{i_t,:}\mathrel{+}= \alpha_t z_t$, where $U=KV$. Cost: $O(qr)$.

*Proof.* Let $F = KVZ^\top = UZ^\top\in\mathbb{R}^{n\times M}$. Then $F_{i,m} = \langle U_{i,:}, Z_{m,:}\rangle$. The masked matrix $\tilde F := \mathbf{1}_\Omega\odot F$ satisfies $\tilde F_{i,m} = F_{i,m}$ if $(i,m)\in\Omega$ and $0$ otherwise.

Now $C = \tilde F Z$, so $C_{i,j} = \sum_{m=1}^M \tilde F_{i,m} Z_{m,j}$. Since $\tilde F_{i,m}=0$ unless $(i,m)\in\Omega$, this sum reduces to contributions only from observed entries:

$$C_{i,:} = \sum_{\{t:(i_t,m_t)\in\Omega,\, i_t=i\}} F_{i,m_t}\, z_t^\top = \sum_{\{t:i_t=i\}} \langle U_{i,:}, z_t\rangle\, z_t^\top.$$

This is exactly what the loop computes: for each observation $t$, the contribution $\alpha_t z_t^\top$ (a row vector) is added to row $i_t$ of $C$. Each iteration costs $O(r)$ for the inner product and $O(r)$ for the row update, giving $O(qr)$ total. The result is exact (no approximation). $\blacksquare$

---

**Lemma 3 (Sparse RHS).** *Claim:* $B = T_{(k)}Z \in\mathbb{R}^{n\times r}$ is computed from observations in $O(qr)$, and $b=\operatorname{vec}(KB)$ in $O(n^2 r)$.

*Proof.* The mode-$k$ unfolding $T_{(k)}\in\mathbb{R}^{n\times M}$ is only observed at entries in $\Omega$. We have

$$B_{i,:} = \sum_{m=1}^M (T_{(k)})_{i,m}\, Z_{m,:} = \sum_{\{t:i_t=i\}} y_t\, z_t^\top,$$

since $(T_{(k)})_{i,m}=0$ for unobserved entries (or equivalently, the unobserved entries contribute nothing because we only sum over $\Omega$). Initialize $B=0$; for $t=1,\dots,q$: $B_{i_t,:}\mathrel{+}= y_t z_t^\top$, cost $O(r)$ per iteration, total $O(qr)$.

Then $b = (I_r\otimes K)\operatorname{vec}(B) = \operatorname{vec}(KB)$, which costs $O(n^2 r)$ via a matrix-matrix multiply. Total: $O(qr+n^2 r)$. $\blacksquare$

---

**Lemma 4 (SPD).** *Claim:* Under Assumptions 1–2, $A\succ 0$.

*Proof.* This was proved in Part 1 of the Complete Proof. For any nonzero $x=\operatorname{vec}(V)$:

$$x^\top Ax = \|S^\top(Z\otimes K)x\|^2 + \lambda\sum_{j=1}^r v_j^\top K v_j \ge \lambda\,\lambda_{\min}(K)\|x\|^2 > 0,$$

since $\lambda>0$, $K\succ 0$, and at least one column $v_j\neq 0$. $\blacksquare$

---

**Lemma 5 (Preconditioner apply).** *Claim:* $P^{-1}\operatorname{vec}(R)$ is computed by solving $(K+\delta I)Y=R$ then $X(G_\Omega+\lambda I)=Y$, giving $\operatorname{vec}(X)=P^{-1}\operatorname{vec}(R)$. Cost per apply: $O(n^2r+nr^2)$.

*Proof.* We have $P = (G_\Omega+\lambda I_r)\otimes(K+\delta I_n)$. By the Kronecker inverse rule, $P^{-1} = (G_\Omega+\lambda I_r)^{-1}\otimes(K+\delta I_n)^{-1}$, which exists since both factors are SPD.

For $\operatorname{vec}(R)$:

$$P^{-1}\operatorname{vec}(R) = \left((G_\Omega+\lambda I_r)^{-1}\otimes(K+\delta I_n)^{-1}\right)\operatorname{vec}(R) = \operatorname{vec}\!\left((K+\delta I_n)^{-1}R\,(G_\Omega+\lambda I_r)^{-\top}\right),$$

using the Kronecker-vec identity $(C\otimes A)\operatorname{vec}(B)=\operatorname{vec}(ABC^\top)$ with $A=(K+\delta I)^{-1}$, $B=R$, $C=(G_\Omega+\lambda I)^{-1}$.

Since $G_\Omega+\lambda I$ is symmetric, $(G_\Omega+\lambda I)^{-\top}=(G_\Omega+\lambda I)^{-1}$, so:

$$P^{-1}\operatorname{vec}(R) = \operatorname{vec}\!\left((K+\delta I)^{-1}R\,(G_\Omega+\lambda I)^{-1}\right).$$

This is computed in two steps:
1. Solve $(K+\delta I)Y = R$: using precomputed Cholesky $L_K L_K^\top = K+\delta I_n$, solve $r$ triangular systems, cost $O(n^2 r)$.
2. Compute $X = Y(G_\Omega+\lambda I)^{-1}$: equivalently solve $X(G_\Omega+\lambda I)=Y$, i.e., $(G_\Omega+\lambda I)X^\top = Y^\top$. Using precomputed Cholesky $L_G L_G^\top = G_\Omega+\lambda I_r$, solve $n$ triangular systems of size $r$, cost $O(nr^2)$.

Setup: $G_\Omega = \sum_t z_t z_t^\top$ costs $O(qr^2)$; Cholesky of $K+\delta I_n$ costs $O(n^3)$; Cholesky of $G_\Omega+\lambda I_r$ costs $O(r^3)$. Per-apply: $O(n^2 r + nr^2)$. $\blacksquare$

---

**Lemma 6 (PCG convergence).** *Claim:* In exact arithmetic, after $m$ iterations of PCG on $Aw=b$ with preconditioner $P\succ 0$, the error $e_m = w_m - w^*$ satisfies

$$\|e_m\|_A \le 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m\|e_0\|_A, \quad \kappa=\kappa(P^{-1/2}AP^{-1/2}).$$

*Proof.* This is a classical result. We give a proof sketch following Saad, Y., *Iterative Methods for Sparse Linear Systems*, 2nd edition, SIAM, 2003, Theorem 9.4.12 and the development in §9.2–9.4.

**Step 1: Reduction to standard CG.** Define $\hat{A} = P^{-1/2}AP^{-1/2}$, $\hat{w} = P^{1/2}w$, $\hat{b}=P^{-1/2}b$. The system $Aw=b$ transforms to $\hat{A}\hat{w}=\hat{b}$. Since $A\succ 0$ and $P\succ 0$, $\hat{A}$ is SPD with eigenvalues in $[\lambda_{\min}(\hat A),\lambda_{\max}(\hat A)]$ and condition number $\kappa=\kappa(\hat A)$. Standard CG on $\hat{A}\hat{w}=\hat{b}$ produces iterates $\hat{w}_m$ whose error $\hat{e}_m=\hat{w}_m-\hat{w}^*$ satisfies the minimax polynomial bound. Furthermore, the PCG recurrences on the original system (Algorithm 9.1 in Saad) are algebraically equivalent to CG on the transformed system: the PCG iterates satisfy $w_m = P^{-1/2}\hat{w}_m$ and $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$ (Saad, §9.2).

**Step 2: CG minimax bound.** Standard CG satisfies (Saad, Theorem 9.4.12):

$$\|\hat{e}_m\|_{\hat{A}} \le \min_{p\in\mathcal{P}_m,\, p(0)=1} \max_{\lambda\in\sigma(\hat{A})} |p(\lambda)|\;\|\hat{e}_0\|_{\hat{A}},$$

where $\mathcal{P}_m$ is the set of polynomials of degree $\le m$.

**Step 3: Chebyshev bound.** The minimax problem on $[\lambda_{\min},\lambda_{\max}]$ is solved by the shifted and scaled Chebyshev polynomial. The Chebyshev polynomial $T_m$ of the first kind satisfies $T_m(x)=\cosh(m\,\operatorname{arccosh}(x))$ for $x\ge 1$. Setting

$$p(\lambda) = \frac{T_m\!\left(\frac{\lambda_{\max}+\lambda_{\min}-2\lambda}{\lambda_{\max}-\lambda_{\min}}\right)}{T_m\!\left(\frac{\lambda_{\max}+\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}\right)},$$

which satisfies $p(0)=1$ and $\max_{\lambda\in[\lambda_{\min},\lambda_{\max}]}|p(\lambda)| = 1/T_m\!\left(\frac{\kappa+1}{\kappa-1}\right)$.

**Step 4: Lower bound on $T_m$.** Let $\gamma=\frac{\kappa+1}{\kappa-1}>1$. Then

$$T_m(\gamma) = \cosh(m\,\operatorname{arccosh}(\gamma)).$$

Now $\operatorname{arccosh}(\gamma)=\ln(\gamma+\sqrt{\gamma^2-1})$. We compute:

$$\gamma^2-1 = \left(\frac{\kappa+1}{\kappa-1}\right)^2 - 1 = \frac{(\kappa+1)^2-(\kappa-1)^2}{(\kappa-1)^2} = \frac{4\kappa}{(\kappa-1)^2},$$

so $\sqrt{\gamma^2-1}=\frac{2\sqrt{\kappa}}{\kappa-1}$. Therefore

$$\gamma+\sqrt{\gamma^2-1} = \frac{\kappa+1}{\kappa-1}+\frac{2\sqrt\kappa}{\kappa-1} = \frac{\kappa+2\sqrt\kappa+1}{(\kappa-1)} = \frac{(\sqrt\kappa+1)^2}{\kappa-1} = \frac{(\sqrt\kappa+1)^2}{(\sqrt\kappa-1)(\sqrt\kappa+1)}=\frac{\sqrt\kappa+1}{\sqrt\kappa-1}.$$

Thus $\operatorname{arccosh}(\gamma)=\ln\!\left(\frac{\sqrt\kappa+1}{\sqrt\kappa-1}\right)$ and

$$T_m(\gamma) = \cosh\!\left(m\ln\frac{\sqrt\kappa+1}{\sqrt\kappa-1}\right) = \frac{1}{2}\!\left[\left(\frac{\sqrt\kappa+1}{\sqrt\kappa-1}\right)^m+\left(\frac{\sqrt\kappa-1}{\sqrt\kappa+1}\right)^m\right] \ge \frac{1}{2}\left(\frac{\sqrt\kappa+1}{\sqrt\kappa-1}\right)^m.$$

Combining Steps 2–4:

$$\|e_m\|_A = \|\hat{e}_m\|_{\hat A} \le \frac{1}{T_m(\gamma)}\|\hat{e}_0\|_{\hat A} \le 2\left(\frac{\sqrt\kappa-1}{\sqrt\kappa+1}\right)^m\|e_0\|_A.\quad\blacksquare$$

---

## Gap Closure Notes

**Note 1 (Editor Point 1 — PCG/CG equivalence).** Lemma 6, Step 1 provides the proof sketch: substituting $\hat{w}=P^{1/2}w$ transforms the preconditioned system to a standard CG problem. This equivalence is established rigorously in Saad, Y., *Iterative Methods for Sparse Linear Systems*, 2nd edition, SIAM, 2003, §9.2 (development of Algorithm 9.1). The key identity is that the PCG residuals $r_m=b-Aw_m$ and search directions $p_m$ satisfy $r_m=P^{-1/2}\hat{r}_m$ and $p_m=P^{-1/2}\hat{p}_m$, where $\hat{r}_m,\hat{p}_m$ are the CG residuals and directions for $\hat{A}\hat{w}=\hat{b}$.

**Note 2 (Editor Point 2 — Exact-arithmetic hypothesis).** The sentence "In exact arithmetic, the preconditioned CG method satisfies the following error bound" has been added at the beginning of the Lemma 6 claim, before the convergence inequality.

**Note 3 (Editor Point 3 — Precise citation).** The citation has been unified to a single definitive reference: Saad, Y., *Iterative Methods for Sparse Linear Systems*, 2nd edition, SIAM, 2003, Theorem 9.4.12. This theorem gives the minimax Chebyshev bound for CG applied to SPD systems, which combined with the PCG-to-CG reduction in §9.2 yields the stated preconditioned bound.

**Note 4 (Editor Point 4 — Solution uniqueness).** The sentence "Since $A$ is SPD, it is invertible, and the system $Aw=b$ has a unique solution $w^*=A^{-1}b$" has been added at the end of Part 1.

**Note 5 (Editor Point 5 — Intermediate algebra).** The computation $(\gamma^2-1)=((κ+1)^2-(κ-1)^2)/(κ-1)^2 = 4κ/(κ-1)^2$, hence $\sqrt{\gamma^2-1}=2\sqrt{κ}/(κ-1)$, is now shown explicitly in Lemma 6, Step 4.

**Note 6 (Editor Point 6 — Storage bounds).** A storage analysis has been added at the end of Part 4: all stored objects ($L_K\in\mathbb{R}^{n\times n}$, $L_G\in\mathbb{R}^{r\times r}$, cached $z_t$ vectors, working matrices) total $O(n^2+r^2+qr+nr)$, none of which is $O(N)$.

**Note 7 (Editor Point 7 — Vectorization bijection).** The column-major bijection $\varphi:[n]\times[M]\to[N]$, $\varphi(i,m)=(m-1)n+i$, is now defined explicitly at the start of the Complete Proof, and $S$'s columns are written as $e_{\varphi(i_t,m_t)}$.

**Note 8 (Editor Point 8 — Stability of Cholesky).** No unqualified stability claims are made for the Cholesky factorizations. The proof treats all computations in exact arithmetic (as stated in Lemma 6). The Cholesky factorizations of $K+\delta I_n$ and $G_\Omega+\lambda I_r$ exist and are unique because these matrices are SPD; numerical stability considerations for finite-precision implementations are outside the scope of this exact-arithmetic proof.

### Editor Dispatch

**Reasoning:** claude_code assigned to Correctness & Completeness for its strength in systematic logical verification of multi-step mathematical arguments. gemini_api assigned to Clarity & Rigor for its ability to assess notational precision and exposition quality against graduate peer-review standards. codex_cli assigned to Reference Validity to verify specific theorem citations and check hypothesis satisfaction for all invoked external results.

- Correctness & Completeness -> claude_code
- Clarity & Rigor -> gemini_api
- Reference Validity -> codex_cli

### Reviews

#### Correctness & Completeness (by claude_code)

## Review: Correctness & Completeness Perspective

### Overall Assessment

This is a substantially improved proof over what one would expect at loop 3. The main theorem is broken into clear parts, each supported by lemmas with explicit derivations. The prior editor feedback (Notes 1–8) has been addressed. However, several correctness and completeness issues remain.

---

### Issue Analysis

**1. RHS definition inconsistency.** The mentor defines $b = (I_r \otimes K)\operatorname{vec}(B)$ where $B = TZ$, but the prover's Lemma 3 computes $B$ from *only observed entries* (the sparse MTTKRP), which computes $B_{i,:} = \sum_{\{t: i_t=i\}} y_t z_t^\top$. This is not the same as $T_{(k)} Z$ when $T_{(k)}$ is the full (dense) mode-$k$ unfolding — it equals $(\mathbf{1}_\Omega \odot T_{(k)}) Z$. The proof never reconciles this: it silently treats the unobserved entries of $T_{(k)}$ as zero, which is only valid if $B$ is *defined* as the sparse MTTKRP from the start. The mentor's notation "$B = TZ$" without qualification is misleading, and the prover's parenthetical "(or equivalently, the unobserved entries contribute nothing because we only sum over $\Omega$)" is hand-waving — unobserved entries of $T$ are unknown, not zero.

**2. The system $Aw = b$ is never derived from an optimization problem.** The proof assumes the normal equations $Aw = b$ are given, but never states *which* objective they arise from. For the RHS to equal $(I_r \otimes K)\operatorname{vec}(B)$ with $B$ being the sparse MTTKRP, the objective must be $\frac{1}{2}\sum_t (y_t - \langle (KW)_{i_t,:}, z_t\rangle)^2 + \frac{\lambda}{2}\operatorname{tr}(W^\top K W)$. Without stating this, a reader cannot verify $Aw = b$ is the correct stationarity condition. The proof proves properties of a system it never derives.

**3. Lemma 1: Symmetry of $K$ used without flagging.** The step $(Z^\top \otimes K^\top)\operatorname{vec}(F) = \operatorname{vec}(K^\top F Z) = \operatorname{vec}(KFZ)$ relies on $K = K^\top$. While kernel matrices are symmetric, this is not listed in the Assumptions section — Assumption 2 states $K \succ 0$ which implies symmetry (SPD matrices are symmetric by definition), but this logical dependency should be made explicit for rigor at the graduate level.

**4. Lemma 2: Row-vector vs. column-vector notation inconsistency.** The proof writes $C_{i,:} = \sum_{\{t:i_t=i\}} \langle U_{i,:}, z_t\rangle z_t^\top$. Here $z_t^\top$ is a row vector (since $z_t \in \mathbb{R}^r$ is a column vector), and $C_{i,:}$ is also a row. So the equation reads "row = sum of scalar × row," which is dimensionally consistent. However, in the algorithm description (Part 2), the update is $C_{i_t,:} \mathrel{+}= \alpha_t z_t$ — here $z_t$ is a column vector being added to a row. This is a notational inconsistency: either use $z_t^\top$ consistently for row operations, or clarify the convention.

**5. Preconditioner Kronecker factor ordering.** The mentor defines $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, and Lemma 5 applies the Kronecker-vec identity $(C \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)$. With $P^{-1} = (G_\Omega + \lambda I)^{-1} \otimes (K + \delta I)^{-1}$, applying the identity gives $\operatorname{vec}((K+\delta I)^{-1} R ((G_\Omega + \lambda I)^{-1})^\top)$. But wait — the identity requires the *left* Kronecker factor to act on rows and the *right* factor on columns, consistent with vec stacking columns. Since $P = (G_\Omega + \lambda I) \otimes (K + \delta I)$, in the identity $(C \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)$, we have $A = (K+\delta I)^{-1}$ and $C = (G_\Omega + \lambda I)^{-1}$. So the result is $\operatorname{vec}((K+\delta I)^{-1} R ((G_\Omega+\lambda I)^{-1})^\top)$. Since $G_\Omega + \lambda I$ is symmetric, the transpose is harmless. The proof handles this correctly. *(No issue here upon careful checking.)*

**6. Complexity of on-the-fly $z_t$ generation understated.** The proof states the cost is $O(q(d-1)r)$ for generating $z_t$ on-the-fly. But each $z_t = Z_{m_t,:}$ requires looking up one row from each of $d-1$ factor matrices and computing their Hadamard product. The index $m_t$ is a multi-index $(j_1, \ldots, j_{k-1}, j_{k+1}, \ldots, j_d)$ and the lookup requires decomposing $m_t$ into this multi-index (or storing the multi-indices). This decomposition step and its cost are not discussed.

**7. Lemma 6 Step 1 is a "proof sketch," not a proof.** The claim is labeled as a lemma with a proof, but Step 1 says "We give a proof sketch" and defers to Saad §9.2. At graduate rigor, either prove the PCG/CG equivalence fully or explicitly state it as a cited theorem rather than pretending to prove it while only sketching it. The current framing is misleading about what is actually proved here versus imported.

**8. Missing justification: CG denominators are nonzero.** Part 1 states "the denominators $p_j^\top A p_j$ in the CG recurrence are strictly positive for all nonzero search directions." This is true but requires that CG search directions are nonzero until the exact solution is reached. In exact arithmetic this follows from the conjugacy of the search directions and the fact that CG terminates in at most $nr$ steps. This reasoning is omitted.

---

### Summary

The core mathematical content is sound. The Kronecker manipulations, sparse accumulation, and preconditioner factorization are all correct. The main gaps are: (1) the RHS derivation from an optimization objective is missing, making the system appear to come from nowhere; (2) a notational inconsistency between row/column conventions for $z_t$; and (3) the Lemma 6 proof should be honest about what it proves vs. cites.

```json
{
  "issues": [
    {
      "severity": "major",
      "location": "Lemma 3 / RHS definition",
      "reason": "B is defined as T_{(k)}Z (full unfolding times Z) but computed as sparse MTTKRP over observed entries only. These are equal only if unobserved entries are treated as zero, which is never stated or justified. The true T_{(k)} has unknown unobserved entries.",
      "required_fix": "Explicitly define B as the observed-entry MTTKRP: B_{i,:} = sum_{t: i_t=i} y_t z_t^T, or equivalently B = (1_Omega ⊙ T_{(k)}) Z. State that this arises from the gradient of the observed-entry least-squares objective, not from the full tensor.",
      "suggestion": "Add a short derivation showing the optimization objective (observed-entry loss + RKHS regularizer), take its gradient with respect to W, set to zero, and identify A and b. This closes the gap and justifies why only observed entries appear in the RHS."
    },
    {
      "severity": "major",
      "location": "Complete Proof, preamble",
      "reason": "The linear system Aw = b is presented without derivation. The reader cannot verify it is the correct stationarity condition without knowing the objective function.",
      "required_fix": "State the optimization problem explicitly (e.g., min_W (1/2) sum_t (y_t - <(KW)_{i_t,:}, z_t>)^2 + (λ/2) tr(W^T K W)) and derive the normal equations to obtain A and b.",
      "suggestion": "Insert a short 'Problem Formulation' section before Part 1 that states the objective and derives A, b by differentiating with respect to vec(W) and setting the gradient to zero."
    },
    {
      "severity": "minor",
      "location": "Lemma 2 / Part 2 algorithm",
      "reason": "Notational inconsistency: Lemma 2's proof uses z_t^T (row vector) in the accumulation formula C_{i,:} = sum alpha_t z_t^T, but Part 2's algorithm writes C_{i_t,:} += alpha_t z_t (column vector added to a row).",
      "required_fix": "Use z_t^T consistently when adding to row slices C_{i_t,:}, or clarify that z_t denotes a row vector in the algorithm pseudocode.",
      "suggestion": "In Part 2 step 2, change 'C_{i_t,:} += alpha_t z_t' to 'C_{i_t,:} += alpha_t z_t^T' to match Lemma 2."
    },
    {
      "severity": "minor",
      "location": "Lemma 6, Step 1",
      "reason": "The step is labeled a 'proof sketch' within what is presented as a full lemma proof. This is inconsistent with the rigor standard. The PCG/CG equivalence is a non-trivial algebraic identity that is asserted but not proved.",
      "required_fix": "Either (a) prove the equivalence by showing the PCG recurrences produce w_m = P^{-1/2} hat{w}_m explicitly, or (b) clearly state this as a cited theorem: 'By Saad (2003), Proposition 9.1, the PCG iterates satisfy...' and remove the 'proof sketch' framing.",
      "suggestion": "Option (b) is more practical: replace 'We give a proof sketch' with 'By the equivalence established in Saad (2003, §9.2, Algorithm 9.1 and surrounding development), PCG on (A, P) produces iterates satisfying w_m = P^{-1/2} hat{w}_m and ||e_m||_A = ||hat{e}_m||_{hat{A}}. We therefore apply standard CG convergence theory to hat{A}.'"
    },
    {
      "severity": "minor",
      "location": "Part 3, on-the-fly z_t generation",
      "reason": "The cost O(q(d-1)r) for on-the-fly z_t generation omits the cost of decomposing the linear index m_t into the multi-index (j_1,...,j_{k-1},j_{k+1},...,j_d) needed to look up rows of each factor matrix.",
      "required_fix": "Either assume the multi-index form is stored alongside each observation (cost O(1) lookup), or account for the O(d) integer-division cost per observation.",
      "suggestion": "Add a parenthetical: '(assuming multi-indices are stored with observations, so each factor-row lookup is O(1))' or note the O(qd) decomposition cost is dominated by O(q(d-1)r)."
    },
    {
      "severity": "minor",
      "location": "Part 1, CG well-definedness",
      "reason": "The claim that CG denominators p_j^T A p_j > 0 for nonzero search directions is stated without justification that search directions remain nonzero before convergence.",
      "required_fix": "Add a brief justification: in exact arithmetic, CG search directions are A-conjugate and nonzero until the residual vanishes (which occurs in at most nr steps).",
      "suggestion": "Append: 'In exact arithmetic, the CG search directions p_0, p_1, ... are nonzero and A-conjugate as long as the residual is nonzero (see Saad, 2003, Proposition 6.6), ensuring p_j^T A p_j > 0 at each iteration.'"
    }
  ],
  "residual_concerns": [
    "The proof treats K as strictly positive definite throughout but the Assumptions section acknowledges K may be only PSD in practice, deferring to a nugget fix. A brief remark on how the nugget-augmented kernel K + epsilon I inherits all stated properties would improve self-containedness.",
    "No discussion of how the PCG initial guess w_0 is chosen, though this does not affect correctness of the convergence bound (only the constant ||e_0||_A)."
  ]
}
```

#### Clarity & Rigor (by gemini_api)

The proof provided is mathematically sound, rigorous, and adheres well to the complexity constraints and definitions provided. The derivation of the matrix-free operator and the preconditioner application are correct. The complexity analysis is thorough and explicitly verifies the absence of $O(N)$ operations.

However, there is a significant notation collision regarding the symbol $\odot$ that affects the clarity and precision of the document. Additionally, a minor variable declaration issue exists in Lemma 2.

### Issues

```json
{
  "issues": [
    {
      "severity": "major",
      "location": "Lemma 1 and Lemma 2",
      "reason": "Notation collision: The symbol '⊙' is used for the Hadamard (element-wise) product in the proof (e.g., 1_Ω ⊙ ...), but it was defined as the Khatri-Rao product in the Mentor's Definitions section (Z := A_d ⊙ ... ⊙ A_1). In a graduate-level text, using the same symbol for two distinct operations creates ambiguity.",
      "required_fix": "Use a distinct symbol for the Hadamard product (e.g., '∘' or '∗') or explicitly redefine '⊙' as the Hadamard product in the Notation section, noting the distinction from the Khatri-Rao product used in the problem setup.",
      "suggestion": "Replace the Hadamard product symbol with '∘' throughout the proof to distinguish it from the Khatri-Rao product."
    },
    {
      "severity": "minor",
      "location": "Lemma 2 Statement",
      "reason": "Implicit variable definition: The lemma statement refers to 'U' in the algorithm description ('where U=KV'), but 'V' is not defined or introduced in the lemma statement itself. While 'V' is clear from context (Lemma 1), a self-contained lemma should declare its inputs.",
      "required_fix": "Explicitly introduce V in the lemma statement.",
      "suggestion": "Amend the lemma statement to start with: 'Let V ∈ ℝ^{n×r} and define U := KV. The matrix C...'"
    }
  ],
  "residual_concerns": []
}
```

#### Reference Validity (by codex_cli)

Primary reference-validity risk is concentrated in the PCG convergence citation: the proof’s key external reference appears misindexed/phantom, even though the underlying bound is standard and likely recoverable from nearby correct sections.

```json
{
  "issues": [
    {
      "severity": "critical",
      "location": "Lemma 6 (PCG convergence), Note 3 citation",
      "reason": "The citation \"Saad (2003), Theorem 9.4.12\" is not verifiable against the cited edition’s chapter structure (Section 9.4 is on flexible variants), so the proof currently relies on an apparent phantom/misindexed theorem for its core convergence claim.",
      "required_fix": "Replace the citation with the correct, verifiable source locations for (i) the CG Chebyshev/minimax convergence bound and (ii) the PCG-to-CG equivalence transformation.",
      "suggestion": "Cite Saad (2nd ed., 2003) as: Section 6.11.3 (CG convergence theorem/bound, e.g., Theorem 6.29 and derived inequality) plus Section 9.2.1 (Algorithm 9.1/9.2 equivalence for preconditioning). Include page numbers from the exact edition used."
    },
    {
      "severity": "minor",
      "location": "Lemma 6, Step 1 (\"following ... §9.2–9.4\")",
      "reason": "The section range is overbroad/inexact for the claim being used; §9.4 is not the natural source for the stated PCG-CG equivalence or CG minimax bound.",
      "required_fix": "Use precise section-level citations tied to each subclaim instead of a broad range.",
      "suggestion": "Split the citation inline: equivalence claim -> §9.2.1; CG minimax/Chebyshev bound -> §6.11.3. This makes dependency checking straightforward."
    }
  ],
  "residual_concerns": [
    "If your pipeline requires explicit sourcing for all nontrivial background facts, add brief references for vec-Kronecker identities and Kronecker SPD/eigenvalue properties (currently treated as standard facts without citation)."
  ]
}
```

### Editor Decision

**Verdict: `right_track`**

**Summary:** The proof architecture is sound and most lemma proofs are rigorous. However, four issues block acceptance: (1) the citation 'Saad (2003), Theorem 9.4.12' appears to be a phantom reference — the CG Chebyshev bound lives in Chapter 6, not §9.4, undermining verifiability of the convergence argument; (2) the linear system Aw=b is presented without deriving it from an optimization objective, so the reader cannot verify correctness of A and b; (3) Lemma 3 defines B using the full tensor unfolding but computes it from observed entries only, without justifying the equivalence; (4) the symbol ⊙ is used for both Khatri-Rao and Hadamard products. Several minor issues (notation inconsistency in Part 2 vs Lemma 2, 'proof sketch' framing in Lemma 6, missing CG well-definedness justification, incomplete on-the-fly cost assumptions) also need attention.

**Feedback (prover):** Priority fixes for the prover:

1. **Add a Problem Formulation section before Part 1.** State the optimization objective explicitly, e.g., min_W (1/2) sum_{t=1}^q (y_t - <(KW)_{i_t,:}, z_t>)^2 + (λ/2) tr(W^T K W). Differentiate with respect to vec(W), set the gradient to zero, and identify the resulting normal equations Aw = b. This simultaneously resolves the missing derivation (Reviewer 2) and justifies why only observed entries appear in the RHS (Reviewer 1).

2. **Fix the Saad citation in Lemma 6.** Replace all references to 'Theorem 9.4.12' with the correct locations: cite §6.11.3 (or Theorem 6.29) for the CG Chebyshev/minimax convergence bound, and §9.2 (Algorithm 9.1 and surrounding development) for the PCG-to-CG equivalence. Include page numbers if possible. Also narrow the broad '§9.2–9.4' range in Step 1 to the specific sections used.

3. **Fix Lemma 6 Step 1 framing.** Replace 'We give a proof sketch following...' with a direct citation: 'By the equivalence established in Saad (2003, §9.2, Algorithm 9.1), PCG on (A, P) produces iterates satisfying w_m = P^{-1/2} ŵ_m and ||e_m||_A = ||ê_m||_{Â}. We therefore apply standard CG convergence theory to Â.'

4. **Resolve the ⊙ notation collision.** Use ∘ (or *) for the Hadamard product throughout the proof to distinguish it from the Khatri-Rao product ⊙ used in the problem setup. Add a note in the notation preamble.

5. **Fix Lemma 3 definition of B.** Define B as the observed-entry MTTKRP: B_{i,:} = sum_{t: i_t=i} y_t z_t^T, or equivalently B = (1_Ω ∘ T_{(k)})Z. Reference the objective function derivation to justify this.

6. **Minor fixes (all straightforward):**
   - In Part 2, step 2: change C_{i_t,:} += α_t z_t to C_{i_t,:} += α_t z_t^T to match Lemma 2.
   - In Lemma 2 statement: explicitly introduce V ∈ ℝ^{n×r} as an input.
   - In Part 3: add '(assuming multi-indices are stored with observations)' for the on-the-fly z_t cost.
   - In Part 1: append a sentence on CG search direction non-degeneracy, e.g., 'In exact arithmetic, CG search directions are nonzero and A-conjugate as long as the residual is nonzero (Saad, 2003, Proposition 6.6).'

## Loop 4

### Researcher Agent

## Relevant Theorems
- **Representer theorem (RKHS finite-sample form):** the minimizer of a regularized empirical loss in RKHS lies in the span of kernel sections at sample points, which justifies \(A_k = KW\).  
  References: Kimeldorf–Wahba (1971); Schölkopf, Herbrich, Smola (COLT 2001).

- **Kronecker–vec identity:** for column-major vec,
  \[
  (C\otimes A)\operatorname{vec}(B)=\operatorname{vec}(AB C^\top).
  \]
  This is the key algebraic tool to rewrite \(A\operatorname{vec}(V)\) as a matrix operator on \(V\), without building \(Z\otimes K\).  
  References: Horn & Johnson, *Topics in Matrix Analysis*; Magnus & Neudecker.

- **Kronecker inverse and SPD facts:**  
  \[
  (A\otimes B)^{-1}=A^{-1}\otimes B^{-1},
  \]
  and if \(A\succ0,\ B\succ0\), then \(A\otimes B\succ0\).  
  Used for preconditioner application and SPD guarantees.

- **CG minimax/Chebyshev convergence (correct citation):** for SPD \(\hat A\), CG satisfies polynomial minimization (in \(\hat A\)-norm), and
  \[
  \|e_m\|_{\hat A}\le 2\left(\frac{\sqrt{\kappa(\hat A)}-1}{\sqrt{\kappa(\hat A)}+1}\right)^m\|e_0\|_{\hat A}.
  \]
  Verified source: Saad (2nd ed., 2003), **§6.11.3**, Lemma 6.28, Theorem 6.29, inequality (6.128).  
  Important: this is an **exact-arithmetic** statement.

- **PCG/CG equivalence under preconditioning (correct location):** PCG with SPD preconditioner \(P\) is equivalent to CG on
  \[
  \hat A=P^{-1/2}AP^{-1/2},
  \]
  or equivalently split-PCG formulations; this justifies importing the CG bound to PCG.  
  Verified source: Saad (2003), **§9.2.1**, Algorithms 9.1/9.2 and equivalence discussion.

- **Finite-precision caveat:** exact CG/PCG bounds can degrade numerically due to loss of orthogonality.  
  Reference: Greenbaum, *Iterative Methods for Solving Linear Systems* (1997), Ch. 4.

## Key Definitions
- **Dimensions:** \(n=n_k,\ M=\prod_{i\ne k}n_i,\ N=nM,\ q=|\Omega|\), with \(n,r<q\ll N\).

- **Observed set:** \(\Omega\subset[n]\times[M]\) with observed triples \((i_t,m_t,y_t)\).  
  Use column-major index map \(\varphi(i,m)=i+n(m-1)\), so \(S=[e_{\varphi(i_t,m_t)}]_{t=1}^q\), \(P_\Omega=SS^\top\).

- **Product symbols (must be disambiguated):**
  - \( \odot \): Khatri–Rao product (for \(Z\)).
  - \( \circ \): Hadamard mask product (for observation mask).

- **Masked unfolding:** \(T_{(k)}\in\mathbb R^{n\times M}\), \(T_\Omega:=\mathbf 1_\Omega\circ T_{(k)}\).

- **Objective (explicit):**
  \[
  \min_W\ \frac12\sum_{t=1}^q\big(y_t-\langle (KW)_{i_t,:},z_t\rangle\big)^2+\frac\lambda2\operatorname{tr}(W^\top K W),
  \]
  equivalently
  \[
  \frac12\|P_\Omega(\operatorname{vec}(T_{(k)})-(Z\otimes K)\operatorname{vec}(W))\|_2^2+\frac\lambda2\,\operatorname{vec}(W)^\top(I_r\otimes K)\operatorname{vec}(W).
  \]

- **Normal equations:** \(Aw=b\), \(w=\operatorname{vec}(W)\),
  \[
  A=(Z\otimes K)^\top P_\Omega(Z\otimes K)+\lambda(I_r\otimes K),
  \]
  \[
  b=(Z\otimes K)^\top P_\Omega\operatorname{vec}(T_{(k)})=\operatorname{vec}(K B),
  \]
  with
  \[
  B:=(\mathbf 1_\Omega\circ T_{(k)})Z,\quad B_{i,:}=\sum_{t:i_t=i} y_t z_t^\top.
  \]

- **Matrix-free operator:**
  \[
  \mathcal A(V)=K\big((\mathbf 1_\Omega\circ (KVZ^\top))Z+\lambda V\big),\quad
  A\operatorname{vec}(V)=\operatorname{vec}(\mathcal A(V)).
  \]

- **Preconditioner:**
  \[
  P=((G_\Omega+\lambda I_r)\otimes (K+\delta I_n)),\quad
  G_\Omega=\sum_{t=1}^q z_t z_t^\top,\ \delta>0.
  \]

## Proof Strategies
- **Start from the objective, then derive \(Aw=b\):** this resolves RHS ambiguity and makes observed-only data usage rigorous.

- **Prove matrix-free matvec identity via Kronecker–vec + masking:**  
  show \(A\operatorname{vec}(V)=\operatorname{vec}(\mathcal A(V))\), then implement
  \(U=KV\), accumulate only over \(t=1,\dots,q\), then \(Y=KC+\lambda U\).

- **Use sparse MTTKRP for RHS:** compute \(B\) by \(q\) row updates, then \(b=\operatorname{vec}(KB)\).

- **Apply Kronecker preconditioner by two solves:**  
  solve \((K+\delta I)Y=R\), then \(X(G_\Omega+\lambda I)=Y\), return \(\operatorname{vec}(X)\).

- **Convergence argument:** cite PCG/CG equivalence (Saad §9.2.1), then apply CG Chebyshev bound from §6.11.3 to \(\hat A=P^{-1/2}AP^{-1/2}\).

- **Complexity/memory accounting:**  
  per matvec \(O(qr+n^2r)\); preconditioner-apply \(O(n^2r+nr^2)\); setup \(O(qr^2+n^3+r^3)\) (plus \(O(q(d-1)r)\) if \(z_t\) generated online).  
  Storage \(O(n^2+r^2+nr+qr)\) if caching sampled \(z_t\). No \(O(N)\) step.

## Gaps and Concerns
- **Citation correction required:** do not cite “Saad Theorem 9.4.12” for CG Chebyshev bound. Correct source is **Chapter 6 (§6.11.3, Theorem 6.29, eq. 6.128)**; PCG equivalence is **§9.2.1**.

- **Must state exact-arithmetic hypothesis** for the classical PCG bound; finite precision needs a caveat/reference.

- **RHS definition must be masked-data consistent:** \(B\) should be \(B=(\mathbf1_\Omega\circ T_{(k)})Z\), not ambiguously full \(T_{(k)}Z\).

- **Notation collision:** reserve \( \odot \) for Khatri–Rao; use \( \circ \) (or \( * \)) for Hadamard masking.

- **Projector assumption:** \(P_\Omega=SS^\top\) is an orthogonal projector only if observed pairs are unique. Duplicates require weighted handling.

- **Row/column consistency in updates:** if \(C_{i,:}\) is a row, update with \(z_t^\top\), not \(z_t\).

- **On-the-fly \(z_t\) cost assumption:** either store multi-indices with observations or include index-decoding overhead (\(O(qd)\), typically dominated by \(O(q(d-1)r)\) for \(r\ge1\)).

- **SPD condition:** strict PCG guarantees require \(K\succ0\). If \(K\succeq0\), add nugget or restrict to \(\mathrm{range}(K)\).

### Mentor Agent

## Definitions
Let \(k\) be the target mode, \(n:=n_k\), \(M:=\prod_{i\neq k} n_i\), \(N=nM\), and rank \(r\).

Let \(\Omega\subset [n]\times[M]\) be the observed index set in mode-\(k\) unfolding, \(|\Omega|=q\), with observed triples \((i_t,m_t,y_t)\), \(t=1,\dots,q\), where \(y_t=T_{i_t,m_t}\).

Define
\[
Z:=A_d\odot\cdots\odot A_{k+1}\odot A_{k-1}\odot\cdots\odot A_1\in\mathbb{R}^{M\times r},
\quad
z_t:=Z_{m_t,:}\in\mathbb{R}^r.
\]
Unknown is \(W\in\mathbb{R}^{n\times r}\), \(w:=\mathrm{vec}(W)\), and \(A_k=KW\).

Define \(P_\Omega:=SS^\top\) (projector selecting observed entries in \(\mathrm{vec}(T)\)) and linear system
\[
A w=b,\quad
A=(Z\otimes K)^\top P_\Omega (Z\otimes K)+\lambda(I_r\otimes K),\quad
b=(I_r\otimes K)\mathrm{vec}(B),\ B=TZ.
\]

Define mask matrix \(\mathbf{1}_\Omega\in\{0,1\}^{n\times M}\). For \(V\in\mathbb{R}^{n\times r}\),
\[
\mathcal A(V):=
K\!\left((\mathbf{1}_\Omega\odot (KVZ^\top))Z+\lambda V\right),
\quad
A\,\mathrm{vec}(V)=\mathrm{vec}(\mathcal A(V)).
\]

## Formal Statement
**Theorem (Matrix-free PCG for RKHS mode-\(k\) subproblem).** Under the assumptions in Section **Assumptions**:

1. \(A\) is SPD, so PCG is well-defined.
2. For any \(V\), \(A\mathrm{vec}(V)\) can be computed exactly without forming \(A\), \(Z\otimes K\), or any \(N\)-scale object via:
\[
U=KV,\ \ 
\alpha_t=\langle U_{i_t,:},z_t\rangle,\ \ 
C_{i_t,:}\mathrel{+}=\alpha_t z_t,\ \ 
Y=KC+\lambda U,
\]
return \(\mathrm{vec}(Y)\).
3. The per-matvec complexity is \(O(qr+n^2r)\) (plus \(O(q(d-1)r)\) if each \(z_t\) is generated on-the-fly from CP factors).
4. With preconditioner
\[
P=((G_\Omega+\lambda I_r)\otimes (K+\delta I_n)),\quad
G_\Omega:=\sum_{t=1}^q z_tz_t^\top,\ \delta>0,
\]
each \(P^{-1}\)-apply costs \(O(n^2r+nr^2)\) after setup \(O(qr^2+n^3+r^3)\).
5. PCG iterates satisfy
\[
\|e_m\|_A\le 2\Big(\frac{\sqrt\kappa-1}{\sqrt\kappa+1}\Big)^m\|e_0\|_A,\quad
\kappa=\kappa(P^{-1/2}AP^{-1/2}),
\]
hence total cost
\[
O\!\left(m(qr+n^2r+nr^2)+qr^2+n^3+r^3\right),
\]
with no \(O(N)\) operation.

## Assumptions
1. \(\lambda>0\).
2. \(K\succ 0\) for strict SPD proof of \(A\); if original kernel is only PSD, either add nugget to the model kernel or solve on \(\mathrm{range}(K)\).
3. \(n,r<q\ll N\), and observed tuples \((i_t,m_t,y_t)\) are directly available.
4. Indexing conventions for unfolding, vectorization, and \(S\) are consistent.
5. For preconditioner quality claims (small \(\kappa\)), observation pattern is not pathologically imbalanced across \(i_t\) and rank directions.

## Notation
- \(n=n_k\): size of mode \(k\).
- \(M=\prod_{i\neq k} n_i\): product of non-\(k\) dimensions.
- \(N=nM\): total entries.
- \(q=|\Omega|\): number of observed entries.
- \(Z\in\mathbb{R}^{M\times r}\): Khatri-Rao product of fixed factors.
- \(K\in\mathbb{R}^{n\times n}\): kernel matrix.
- \(W\in\mathbb{R}^{n\times r}\), \(w=\mathrm{vec}(W)\): unknown.
- \(B=TZ\in\mathbb{R}^{n\times r}\): sparse MTTKRP result.
- \(z_t=Z_{m_t,:}\): sampled Khatri-Rao row for observation \(t\).
- \(P_\Omega=SS^\top\): selection projector in vectorized unfolding space.
- \(G_\Omega=\sum_t z_tz_t^\top\in\mathbb{R}^{r\times r}\): sampled rank-Gram.

## High-Level Strategy
1. Rewrite the Hessian action as a matrix operator \(\mathcal A(V)\) using \((C\otimes A)\mathrm{vec}(B)=\mathrm{vec}(AB C^\top)\).
2. Implement \(\mathcal A(V)\) from observed entries only, via rowwise accumulation \(C_{i_t,:} \!+\!= \alpha_t z_t\), never forming \(P_\Omega\), \(Z\otimes K\), or dense \(T\).
3. Compute RHS sparsely: \(B_{i_t,:}\!+\!=y_t z_t\), then \(b=\mathrm{vec}(KB)\).
4. Run PCG on \(Aw=b\) with Kronecker preconditioner \(P\) that is cheap to invert by two small Cholesky-based solves.
5. Prove complexity by summing matvec cost, preconditioner-apply cost, and one-time setup.

## Key Lemmas
1. **Kronecker-vec masked operator lemma:** \(A\,\mathrm{vec}(V)=\mathrm{vec}(\mathcal A(V))\).
2. **Sampled accumulation lemma:** \(C=(\mathbf{1}_\Omega\odot(KVZ^\top))Z\) is computed exactly by the \(q\)-sample loop in \(O(qr)\).
3. **Sparse RHS lemma:** \(B=TZ\) is computed from observations in \(O(qr)\), then \(b\) in \(O(n^2r)\).
4. **SPD lemma:** if \(K\succ0,\lambda>0\), then \(A\succ0\) because
\[
x^\top A x=\|P_\Omega^{1/2}(Z\otimes K)x\|_2^2+\lambda\|(I_r\otimes K^{1/2})x\|_2^2>0.
\]
5. **Preconditioner-apply lemma:** \(P^{-1}\mathrm{vec}(R)\) equals two solves:
\[
(K+\delta I)Y=R,\quad X(G_\Omega+\lambda I)=Y,\quad \mathrm{vec}(X)=P^{-1}\mathrm{vec}(R).
\]
6. **PCG convergence lemma:** standard SPD PCG bound with \(\kappa(P^{-1/2}AP^{-1/2})\).

## Dependency Graph
1. Definitions + Notation \(\rightarrow\) Lemma 1.
2. Lemma 1 \(\rightarrow\) Lemma 2 (matvec algorithm correctness).
3. Definitions \(\rightarrow\) Lemma 3 (RHS correctness/cost).
4. Assumptions 1-2 \(\rightarrow\) Lemma 4 (SPD of \(A\)).
5. Definitions \(\rightarrow\) Lemma 5 (preconditioner form and apply rule).
6. Lemma 4 + Lemma 5 + Lemma 6 \(\rightarrow\) PCG convergence claim.
7. Lemma 2 + Lemma 3 + Lemma 5 \(\rightarrow\) total complexity claim in theorem.

## Risky Steps
1. **PSD kernel issue:** if \(K\) is singular, \(A\) may be singular despite \(\lambda>0\); this must be handled explicitly.
2. **Index consistency:** mismatch between unfolding order, \(S\), and sampled \((i_t,m_t)\) invalidates the operator.
3. **Preconditioner quality claim:** spectral-equivalence constants require sampling-balance assumptions; do not claim uniformly good \(\kappa\) without them.
4. **Hidden \(M\)-scale cost:** forming full \(Z\) is infeasible when \(M\) is huge; use only sampled rows \(z_t\) (cached or on-demand).
5. **Numerical stability:** very small \(\lambda\) or ill-conditioned \(K\) can cause PCG stagnation; monitor relative preconditioned residual and cap iterations.

### Prover Agent

Now I have full context. Let me write the complete proof addressing all editor feedback points.

## Complete Proof

### Notation

We adopt the following conventions throughout.

- $n = n_k$: size of mode $k$.
- $M = \prod_{i \neq k} n_i$: product of all dimensions except $k$.
- $N = nM$: total number of tensor entries.
- $q = |\Omega|$: number of observed entries, with $n, r < q \ll N$.
- $K \in \mathbb{R}^{n \times n}$: kernel matrix, assumed SPD ($K \succ 0$).
- $Z \in \mathbb{R}^{M \times r}$: Khatri–Rao product $A_d \odot \cdots \odot A_{k+1} \odot A_{k-1} \odot \cdots \odot A_1$ of fixed CP factors.
- $W \in \mathbb{R}^{n \times r}$: unknown, with $w = \operatorname{vec}(W) \in \mathbb{R}^{nr}$.
- Observed entries are indexed by triples $(i_t, m_t, y_t)$ for $t = 1, \dots, q$, where $i_t \in [n]$, $m_t \in [M]$, and $y_t$ is the observed value at position $(i_t, m_t)$ in the mode-$k$ unfolding.
- $\mathbf{1}_\Omega \in \{0,1\}^{n \times M}$: binary mask with $(\mathbf{1}_\Omega)_{i,m} = 1$ iff $(i,m) \in \Omega$.
- $z_t := Z_{m_t,:} \in \mathbb{R}^r$: the row of $Z$ corresponding to observation $t$.
- $G_\Omega := \sum_{t=1}^q z_t z_t^\top \in \mathbb{R}^{r \times r}$: sampled rank-Gram matrix.
- Column-major vectorization: we define the bijection $\varphi: [n] \times [M] \to [N]$ by $\varphi(i, m) = i + n(m-1)$, so $\operatorname{vec}(X)_{\varphi(i,m)} = X_{i,m}$. The selection matrix $S \in \mathbb{R}^{N \times q}$ has columns $e_{\varphi(i_t, m_t)}$ for $t = 1, \dots, q$, and the projection $P_\Omega := SS^\top$ satisfies $P_\Omega \operatorname{vec}(X) = \operatorname{vec}(\mathbf{1}_\Omega \circ X)$.
- **Notation distinction:** $\odot$ denotes the Khatri–Rao (columnwise Kronecker) product; $\circ$ denotes the Hadamard (entrywise) product.

---

### Problem Formulation

We seek the factor $A_k = KW$ by solving a regularized least-squares problem over the observed entries:

$$
\min_{W \in \mathbb{R}^{n \times r}} \; f(W) := \frac{1}{2} \sum_{t=1}^q \bigl(y_t - \langle (KW)_{i_t,:},\, z_t \rangle\bigr)^2 + \frac{\lambda}{2} \operatorname{tr}(W^\top K W).
$$

The first term fits observed data; the second is an RKHS-norm regularizer with $\lambda > 0$.

**Derivation of the normal equations.** We compute the gradient with respect to $w = \operatorname{vec}(W)$. The predicted value for observation $t$ is

$$
\hat{y}_t = \langle (KW)_{i_t,:},\, z_t \rangle = e_{i_t}^\top K W z_t = (z_t^\top \otimes e_{i_t}^\top K) w = (z_t \otimes K e_{i_t})^\top w,
$$

where $e_{i_t}$ is the $i_t$-th standard basis vector in $\mathbb{R}^n$. Equivalently, $\hat{y}_t = e_{\varphi(i_t,m_t)}^\top (Z \otimes K) w$, since the $\varphi(i_t,m_t)$-th row of $Z \otimes K$ is $z_t^\top \otimes (K e_{i_t})^\top$. Hence

$$
S^\top (Z \otimes K) w = \begin{pmatrix} \hat{y}_1 \\ \vdots \\ \hat{y}_q \end{pmatrix}, \qquad
\sum_{t=1}^q (\hat{y}_t - y_t)^2 = \|S^\top (Z \otimes K) w - S^\top \operatorname{vec}(T)\|_2^2.
$$

The regularizer in vectorized form is $\frac{\lambda}{2} w^\top (I_r \otimes K) w$, since $\operatorname{tr}(W^\top K W) = w^\top (I_r \otimes K) w$.

Setting $\nabla_w f = 0$:

$$
(Z \otimes K)^\top S S^\top (Z \otimes K) w + \lambda (I_r \otimes K) w = (Z \otimes K)^\top S S^\top \operatorname{vec}(T).
$$

For the right-hand side, note that $S S^\top \operatorname{vec}(T) = P_\Omega \operatorname{vec}(T) = \operatorname{vec}(\mathbf{1}_\Omega \circ T_{(k)})$, where $T_{(k)}$ is the mode-$k$ unfolding (which agrees with $T$ on observed entries and is zero elsewhere, since unobserved entries of $T$ are set to zero). Therefore

$$
(Z \otimes K)^\top \operatorname{vec}(\mathbf{1}_\Omega \circ T_{(k)}) = \operatorname{vec}\bigl(K (\mathbf{1}_\Omega \circ T_{(k)}) Z\bigr) = (I_r \otimes K) \operatorname{vec}(B),
$$

where we define $B := (\mathbf{1}_\Omega \circ T_{(k)}) Z \in \mathbb{R}^{n \times r}$, the observed-entry MTTKRP. The second equality uses the identity $(C^\top \otimes A) \operatorname{vec}(X) = \operatorname{vec}(AXC)$ with $C = Z$, $A = K$, and noting that $(Z \otimes K)^\top = (Z^\top \otimes K^\top) = (Z^\top \otimes K)$ since $K$ is symmetric. This yields the normal equations

$$
A w = b, \quad A := (Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda (I_r \otimes K), \quad b := (I_r \otimes K) \operatorname{vec}(B).
$$

Since only observed entries contribute to both $A$ and $b$, no $O(N)$-scale objects are needed.

---

### Part 1: $A$ is SPD and PCG is well-defined

For any nonzero $x \in \mathbb{R}^{nr}$:

$$
x^\top A x = \|S^\top (Z \otimes K) x\|_2^2 + \lambda\, x^\top (I_r \otimes K) x.
$$

The first term is nonneg. For the second, write $x = \operatorname{vec}(V)$ for $V \in \mathbb{R}^{n \times r}$; then $x^\top (I_r \otimes K) x = \operatorname{tr}(V^\top K V) = \sum_{j=1}^r v_j^\top K v_j$ where $v_j$ are the columns of $V$. Since $K \succ 0$, each $v_j^\top K v_j > 0$ unless $v_j = 0$, and at least one $v_j \neq 0$ since $x \neq 0$. With $\lambda > 0$, we obtain $x^\top A x > 0$. Hence $A \succ 0$.

Since $A$ is SPD, it is invertible, and the system $Aw = b$ has a unique solution $w^* = A^{-1}b$. PCG is therefore well-defined: in exact arithmetic, CG search directions are nonzero and $A$-conjugate as long as the residual is nonzero (Saad, 2003, Proposition 6.6), so the method converges in at most $nr$ iterations.

---

### Part 2: Matrix-free matvec

**Input:** $V \in \mathbb{R}^{n \times r}$. **Output:** the unique $Y \in \mathbb{R}^{n \times r}$ satisfying $\operatorname{vec}(Y) = A \operatorname{vec}(V)$.

**Algorithm:**

1. Compute $U = KV \in \mathbb{R}^{n \times r}$.
2. Initialize $C = 0_{n \times r}$. For $t = 1, \dots, q$: compute $\alpha_t = U_{i_t,:} \, z_t \in \mathbb{R}$ (inner product), then update $C_{i_t,:} \mathrel{+}= \alpha_t\, z_t^\top$.
3. Compute $Y = KC + \lambda U$.
4. Return $\operatorname{vec}(Y)$.

**Correctness** is established in Lemma 1 and Lemma 2 below.

---

### Part 3: Complexity of one matvec

- **Step 1:** $U = KV$ costs $O(n^2 r)$ (dense matrix–matrix multiply).
- **Step 2:** Each of the $q$ iterations computes an inner product of length $r$ ($O(r)$) and a rank-1 row update ($O(r)$), totaling $O(qr)$.
  - If each $z_t$ is generated on the fly from the $d-1$ CP factor rows rather than pre-cached, extracting and Hadamard-multiplying those rows costs $O((d-1)r)$ per observation (assuming multi-indices $(i_t^{(1)}, \dots, i_t^{(d)})$ are stored with observations), giving an additional $O(q(d-1)r)$.
- **Step 3:** $KC$ costs $O(n^2 r)$; the addition $\lambda U$ is $O(nr)$.

**Total per-matvec cost:** $O(qr + n^2 r)$, or $O(q(d-1)r + n^2 r)$ with on-the-fly $z_t$ generation. Since $n < q$, we have $n^2 r \leq nqr$, so the cost is at most $O(nqr)$ in all cases. Crucially, no $O(N)$ operation is performed.

---

### Part 4: Preconditioner

Define the Kronecker preconditioner

$$
P := (G_\Omega + \lambda I_r) \otimes (K + \delta I_n), \quad \delta > 0.
$$

**Setup cost:**
- $G_\Omega = \sum_{t=1}^q z_t z_t^\top$: costs $O(qr^2)$.
- Cholesky factorization of $K + \delta I_n \in \mathbb{R}^{n \times n}$: $O(n^3)$, yielding $L_K$.
- Cholesky factorization of $G_\Omega + \lambda I_r \in \mathbb{R}^{r \times r}$: $O(r^3)$, yielding $L_G$.

Both Cholesky factorizations are well-defined since $K + \delta I_n \succ 0$ (as $K \succeq 0$ and $\delta > 0$) and $G_\Omega + \lambda I_r \succ 0$ (as $G_\Omega \succeq 0$ and $\lambda > 0$). Numerical stability of these factorizations follows from the matrices being SPD (cf. Higham, *Accuracy and Stability of Numerical Algorithms*, 2nd ed., SIAM, 2002, Theorem 10.3).

**Total one-time setup:** $O(qr^2 + n^3 + r^3)$.

**Applying $P^{-1}$:** Given residual matrix $R \in \mathbb{R}^{n \times r}$ (with $\operatorname{vec}(R)$ as input), compute $P^{-1} \operatorname{vec}(R) = \operatorname{vec}(X)$ via:

1. Solve $(K + \delta I_n) \widetilde{R} = R$ for $\widetilde{R} \in \mathbb{R}^{n \times r}$: using the Cholesky factor $L_K$, solve $r$ triangular systems, costing $O(n^2 r)$.
2. Solve $\widetilde{R} (G_\Omega + \lambda I_r)^\top = X$, equivalently $(G_\Omega + \lambda I_r) X^\top = \widetilde{R}^\top$: using $L_G$, solve $n$ triangular systems, costing $O(nr^2)$.

**Per-apply cost:** $O(n^2 r + nr^2)$.

**Storage:** All stored objects—$L_K \in \mathbb{R}^{n \times n}$, $L_G \in \mathbb{R}^{r \times r}$, cached $z_t$ vectors ($O(qr)$), and working matrices ($O(nr)$)—total $O(n^2 + r^2 + qr + nr)$ storage, none of which is $O(N)$.

---

### Part 5: PCG convergence and total complexity

By Lemma 6 below, after $m$ PCG iterations with preconditioner $P$:

$$
\|e_m\|_A \leq 2 \left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^m \|e_0\|_A, \quad \kappa = \kappa(P^{-1/2} A P^{-1/2}).
$$

Each PCG iteration requires one matvec ($O(qr + n^2 r)$) and one preconditioner apply ($O(n^2 r + nr^2)$), plus $O(nr)$ for vector operations. After $m$ iterations, the total cost is:

$$
O\!\left(m(qr + n^2 r + nr^2) + qr^2 + n^3 + r^3\right),
$$

where the last three terms are one-time setup. No $O(N)$ operation appears anywhere. $\blacksquare$

---

## Lemma Proofs

### Lemma 1 (Kronecker-vec masked operator)

**Statement.** For any $V \in \mathbb{R}^{n \times r}$, define $\mathcal{A}(V) := K\bigl((\mathbf{1}_\Omega \circ (KVZ^\top))Z + \lambda V\bigr)$. Then $A\,\operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))$.

**Proof.** We compute each term of $A \operatorname{vec}(V) = \bigl[(Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda(I_r \otimes K)\bigr] \operatorname{vec}(V)$ separately.

*First term.* Using the mixed-product property $(C^\top \otimes A)\operatorname{vec}(X) = \operatorname{vec}(AXC)$:

$$
(Z \otimes K) \operatorname{vec}(V) = \operatorname{vec}(KVZ^\top) \in \mathbb{R}^N.
$$

Applying $P_\Omega = SS^\top$:

$$
P_\Omega \operatorname{vec}(KVZ^\top) = \operatorname{vec}\bigl(\mathbf{1}_\Omega \circ (KVZ^\top)\bigr),
$$

since $P_\Omega$ zeros out entries outside $\Omega$. Then:

$$
(Z \otimes K)^\top \operatorname{vec}\bigl(\mathbf{1}_\Omega \circ (KVZ^\top)\bigr) = (Z^\top \otimes K) \operatorname{vec}\bigl(\mathbf{1}_\Omega \circ (KVZ^\top)\bigr) = \operatorname{vec}\bigl(K(\mathbf{1}_\Omega \circ (KVZ^\top))Z\bigr),
$$

where we used $K^\top = K$ and applied the identity again with $C = Z$, $A = K$.

*Second term.* $\lambda(I_r \otimes K)\operatorname{vec}(V) = \lambda\,\operatorname{vec}(KV)$.

*Combining:* $A\operatorname{vec}(V) = \operatorname{vec}\bigl(K(\mathbf{1}_\Omega \circ (KVZ^\top))Z + \lambda KV\bigr) = \operatorname{vec}\bigl(K[(\mathbf{1}_\Omega \circ (KVZ^\top))Z + \lambda V]\bigr) = \operatorname{vec}(\mathcal{A}(V))$. $\square$

---

### Lemma 2 (Sampled accumulation)

**Statement.** Let $V \in \mathbb{R}^{n \times r}$ and $U = KV$. Define $C \in \mathbb{R}^{n \times r}$ by initializing $C = 0$ and performing the accumulation

$$
C_{i_t,:} \mathrel{+}= (U_{i_t,:}\, z_t)\, z_t^\top, \quad t = 1, \dots, q.
$$

Then $C = (\mathbf{1}_\Omega \circ (UZ^\top))Z$, and the accumulation costs $O(qr)$.

**Proof.** The $(i, j)$-entry of $\mathbf{1}_\Omega \circ (UZ^\top)$ is

$$
[\mathbf{1}_\Omega \circ (UZ^\top)]_{i,m} = \begin{cases} U_{i,:}\, Z_{m,:}^\top & \text{if } (i,m) \in \Omega, \\ 0 & \text{otherwise.} \end{cases}
$$

Therefore, the $i$-th row of $(\mathbf{1}_\Omega \circ (UZ^\top))Z$ is

$$
\bigl[(\mathbf{1}_\Omega \circ (UZ^\top))Z\bigr]_{i,:} = \sum_{m:\,(i,m)\in\Omega} (U_{i,:}\, Z_{m,:}^\top)\, Z_{m,:}.
$$

Since the observed entries at row $i$ are exactly $\{t : i_t = i\}$ with $Z_{m_t,:} = z_t^\top$, this equals

$$
\sum_{t:\,i_t = i} (U_{i,:}\, z_t)\, z_t^\top.
$$

This is precisely what the accumulation loop computes for row $i$: each observation $t$ with $i_t = i$ adds $(U_{i_t,:}\, z_t)\, z_t^\top$ to $C_{i,:}$.

**Complexity:** Each iteration computes one inner product ($O(r)$) and one outer-product row update ($O(r)$), totaling $O(qr)$. $\square$

---

### Lemma 3 (Sparse RHS)

**Statement.** Define the observed-entry MTTKRP as

$$
B_{i,:} := \sum_{t:\, i_t = i} y_t\, z_t^\top, \quad i = 1, \dots, n,
$$

or equivalently $B = (\mathbf{1}_\Omega \circ T_{(k)}) Z$. Then $b = (I_r \otimes K)\operatorname{vec}(B) = \operatorname{vec}(KB)$, and $B$ is computed from observed entries in $O(qr)$ with $KB$ in an additional $O(n^2 r)$.

**Proof.** *Equivalence of definitions.* Since $T_{(k)}$ has $T_{i,m} = y_t$ for $(i,m) = (i_t, m_t) \in \Omega$ and $T_{i,m} = 0$ for $(i,m) \notin \Omega$, we have $(\mathbf{1}_\Omega \circ T_{(k)}) = T_{(k)}$ (masking is idempotent on a matrix that is already zero outside $\Omega$). Thus:

$$
[(\mathbf{1}_\Omega \circ T_{(k)}) Z]_{i,:} = \sum_{m:\,(i,m)\in\Omega} T_{i,m}\, Z_{m,:} = \sum_{t:\,i_t=i} y_t\, z_t^\top = B_{i,:}.
$$

*RHS derivation.* From the Problem Formulation, the RHS of the normal equations is

$$
b = (Z \otimes K)^\top P_\Omega \operatorname{vec}(T_{(k)}) = (Z^\top \otimes K) \operatorname{vec}(\mathbf{1}_\Omega \circ T_{(k)}) = \operatorname{vec}(K (\mathbf{1}_\Omega \circ T_{(k)}) Z) = \operatorname{vec}(KB) = (I_r \otimes K)\operatorname{vec}(B).
$$

*Complexity.* Computing $B$ via the accumulation $B_{i_t,:} \mathrel{+}= y_t\, z_t^\top$ for $t = 1,\dots,q$ costs $O(qr)$. The multiplication $KB$ costs $O(n^2 r)$. Total: $O(qr + n^2 r)$. $\square$

---

### Lemma 4 (SPD property)

**Statement.** If $K \succ 0$ and $\lambda > 0$, then $A \succ 0$.

**Proof.** For any nonzero $x \in \mathbb{R}^{nr}$:

$$
x^\top A x = \underbrace{\|S^\top(Z \otimes K)x\|_2^2}_{\geq\, 0} + \lambda \underbrace{x^\top (I_r \otimes K) x}_{>\, 0}.
$$

The second term: write $x = \operatorname{vec}(V)$, $V \neq 0$. Then $x^\top(I_r \otimes K)x = \sum_{j=1}^r v_j^\top K v_j$, where $v_j$ are columns of $V$. Since $K \succ 0$ and at least one $v_j \neq 0$, this sum is strictly positive. Hence $x^\top A x > 0$ for all $x \neq 0$, so $A \succ 0$. $\square$

---

### Lemma 5 (Preconditioner apply)

**Statement.** Let $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$ with $\delta > 0$. Then $P \succ 0$, and for any $R \in \mathbb{R}^{n \times r}$, $P^{-1}\operatorname{vec}(R) = \operatorname{vec}(X)$ where $X$ is obtained by:

1. Solve $(K + \delta I_n)\widetilde{R} = R$ for $\widetilde{R}$.
2. Solve $(G_\Omega + \lambda I_r) X^\top = \widetilde{R}^\top$ for $X$.

The per-apply cost is $O(n^2 r + nr^2)$ given precomputed Cholesky factors.

**Proof.** *$P \succ 0$:* Since $K \succeq 0$ and $\delta > 0$, $K + \delta I_n \succ 0$. Since $G_\Omega \succeq 0$ and $\lambda > 0$, $G_\Omega + \lambda I_r \succ 0$. The Kronecker product of two SPD matrices is SPD (eigenvalues are pairwise products of eigenvalues of the factors, all positive). Hence $P \succ 0$.

*Inversion formula:* Since $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, we have $P^{-1} = (G_\Omega + \lambda I_r)^{-1} \otimes (K + \delta I_n)^{-1}$. Applying this to $\operatorname{vec}(R)$:

$$
P^{-1}\operatorname{vec}(R) = \bigl[(G_\Omega + \lambda I_r)^{-1} \otimes (K + \delta I_n)^{-1}\bigr]\operatorname{vec}(R) = \operatorname{vec}\bigl((K + \delta I_n)^{-1} R\, (G_\Omega + \lambda I_r)^{-\top}\bigr).
$$

Since $G_\Omega + \lambda I_r$ is symmetric, $(G_\Omega + \lambda I_r)^{-\top} = (G_\Omega + \lambda I_r)^{-1}$. So:

$$
X = (K + \delta I_n)^{-1} R\, (G_\Omega + \lambda I_r)^{-1}.
$$

Defining $\widetilde{R} = (K + \delta I_n)^{-1} R$ (Step 1) and then $X = \widetilde{R} (G_\Omega + \lambda I_r)^{-1}$, equivalently $(G_\Omega + \lambda I_r) X^\top = \widetilde{R}^\top$ (Step 2), we recover $X$.

*Cost:* Step 1 solves $r$ triangular systems of size $n$ (forward and back substitution with Cholesky factor $L_K$): $O(n^2 r)$. Step 2 solves $n$ triangular systems of size $r$: $O(nr^2)$. Total: $O(n^2 r + nr^2)$. $\square$

---

### Lemma 6 (PCG convergence)

**Statement.** In exact arithmetic, the preconditioned conjugate gradient method applied to $Aw = b$ with preconditioner $P \succ 0$ satisfies, after $m$ iterations:

$$
\|e_m\|_A \leq 2\left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^m \|e_0\|_A, \quad \kappa := \kappa(P^{-1/2} A P^{-1/2}) = \frac{\lambda_{\max}(\hat{A})}{\lambda_{\min}(\hat{A})},
$$

where $e_m = w_m - w^*$ is the error at iteration $m$ and $\hat{A} := P^{-1/2} A P^{-1/2}$.

**Proof.** By the equivalence established in Saad (2003, §9.2, Algorithm 9.1), PCG on $(A, P)$ produces iterates $w_m$ related to standard CG iterates $\hat{w}_m$ on $\hat{A} = P^{-1/2}AP^{-1/2}$ by the transformation $w_m = P^{-1/2}\hat{w}_m$. Defining $\hat{e}_m = \hat{w}_m - \hat{w}^*$ where $\hat{w}^* = P^{1/2}w^*$, we have

$$
\|e_m\|_A^2 = e_m^\top A e_m = \hat{e}_m^\top (P^{-1/2})^\top A P^{-1/2} \hat{e}_m = \hat{e}_m^\top \hat{A} \hat{e}_m = \|\hat{e}_m\|_{\hat{A}}^2.
$$

Hence $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$.

We now apply the standard CG Chebyshev convergence bound to $\hat{A}$. Since $\hat{A} \succ 0$ (as $A \succ 0$ and $P \succ 0$), CG on $\hat{A}$ is well-defined. By Saad (2003, §6.11.3, Theorem 6.29), the CG iterates on $\hat{A}$ satisfy the minimax bound:

$$
\|\hat{e}_m\|_{\hat{A}} \leq \min_{\substack{p \in \mathcal{P}_m \\ p(0)=1}} \max_{\lambda \in \sigma(\hat{A})} |p(\lambda)| \cdot \|\hat{e}_0\|_{\hat{A}},
$$

where $\mathcal{P}_m$ is the set of polynomials of degree at most $m$. The minimum is attained by the scaled Chebyshev polynomial $p^*(\lambda) = T_m\!\left(\frac{\lambda_{\max}+\lambda_{\min}-2\lambda}{\lambda_{\max}-\lambda_{\min}}\right) \Big/ T_m\!\left(\frac{\lambda_{\max}+\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}\right)$, giving

$$
\|\hat{e}_m\|_{\hat{A}} \leq \frac{1}{\displaystyle T_m\!\left(\frac{\lambda_{\max}+\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}\right)} \|\hat{e}_0\|_{\hat{A}}.
$$

Setting $\kappa = \lambda_{\max}/\lambda_{\min}$ and $x = (\kappa+1)/(\kappa-1) > 1$, the Chebyshev polynomial satisfies $T_m(x) = \cosh(m \operatorname{arccosh}(x))$. Since $\operatorname{arccosh}(x) \geq \sqrt{x^2-1}/x$ and

$$
x^2 - 1 = \frac{(\kappa+1)^2 - (\kappa-1)^2}{(\kappa-1)^2} = \frac{4\kappa}{(\kappa-1)^2},
\quad \text{so} \quad
\sqrt{x^2-1} = \frac{2\sqrt{\kappa}}{\kappa-1},
$$

we have $\operatorname{arccosh}(x) \geq \ln(x + \sqrt{x^2-1}) \geq \ln\!\left(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\right)$, where the last step follows from

$$
x + \sqrt{x^2-1} = \frac{\kappa+1}{\kappa-1} + \frac{2\sqrt{\kappa}}{\kappa-1} = \frac{(\sqrt{\kappa}+1)^2}{(\sqrt{\kappa}-1)(\sqrt{\kappa}+1)} = \frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}.
$$

Therefore

$$
T_m(x) = \cosh(m\operatorname{arccosh}(x)) \geq \frac{1}{2}\exp(m\operatorname{arccosh}(x)) \geq \frac{1}{2}\left(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\right)^m,
$$

which gives

$$
\frac{1}{T_m(x)} \leq 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m.
$$

Combining with $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$:

$$
\|e_m\|_A \leq 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m \|e_0\|_A. \quad \square
$$

---

## Gap Closure Notes

1. **Problem formulation added (Editor point 1).** The optimization objective $\min_W \frac{1}{2}\sum_t (y_t - \langle(KW)_{i_t,:}, z_t\rangle)^2 + \frac{\lambda}{2}\operatorname{tr}(W^\top KW)$ is stated explicitly in the Problem Formulation section. The gradient is computed and set to zero to derive $Aw = b$, showing that both $A$ and $b$ involve only observed entries. This resolves the missing derivation and justifies why the RHS uses only observed data.

2. **Saad citation corrected (Editor point 2).** All references to "Theorem 9.4.12" have been removed. The proof now cites: (a) Saad (2003), §9.2, Algorithm 9.1 for the PCG-to-CG equivalence; (b) Saad (2003), §6.11.3, Theorem 6.29 for the Chebyshev/minimax CG convergence bound; (c) Saad (2003), Proposition 6.6 for CG search direction non-degeneracy. No broad "§9.2–9.4" range is used.

3. **Lemma 6 framing fixed (Editor point 3).** The phrase "We give a proof sketch following..." has been replaced with a direct statement: "By the equivalence established in Saad (2003, §9.2, Algorithm 9.1), PCG on $(A,P)$ produces iterates satisfying $w_m = P^{-1/2}\hat{w}_m$ and $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$. We therefore apply standard CG convergence theory to $\hat{A}$."

4. **Notation collision resolved (Editor point 4).** The Hadamard product is now denoted $\circ$ throughout, while $\odot$ is reserved exclusively for the Khatri–Rao product. A clarifying note appears in the Notation preamble.

5. **Lemma 3 definition of $B$ fixed (Editor point 5).** $B$ is now defined as the observed-entry MTTKRP: $B_{i,:} = \sum_{t: i_t=i} y_t z_t^\top$, equivalently $B = (\mathbf{1}_\Omega \circ T_{(k)})Z$. The equivalence is proven explicitly, and the connection to the objective function gradient is established in the Problem Formulation section.

6. **Minor fixes addressed (Editor point 6):**
   - In Part 2, step 2: the update is written as $C_{i_t,:} \mathrel{+}= \alpha_t z_t^\top$, matching the row-vector convention in Lemma 2.
   - In Lemma 2 statement: $V \in \mathbb{R}^{n \times r}$ is explicitly introduced as input.
   - In Part 3: the note "(assuming multi-indices are stored with observations)" is included for on-the-fly $z_t$ cost.
   - In Part 1: a sentence on CG search direction non-degeneracy is added, citing Saad (2003), Proposition 6.6.

7. **Solution uniqueness (from Loop 2 feedback).** Part 1 now states: "Since $A$ is SPD, it is invertible, and the system $Aw = b$ has a unique solution $w^* = A^{-1}b$."

8. **Intermediate algebra in Lemma 6 (from Loop 2 feedback).** The step showing $x^2 - 1 = 4\kappa/(\kappa-1)^2$ and the derivation $x + \sqrt{x^2 - 1} = (\sqrt{\kappa}+1)/(\sqrt{\kappa}-1)$ are given explicitly.

9. **Storage bounds (from Loop 2 feedback).** Part 4 includes a sentence listing all stored objects and confirming $O(n^2 + r^2 + qr + nr)$ total storage, none $O(N)$.

10. **Vectorization bijection (from Loop 2 feedback).** The column-major bijection $\varphi(i,m) = i + n(m-1)$ is defined in the Notation section, and $S$'s columns are specified as $e_{\varphi(i_t, m_t)}$.

11. **Numerical stability of Cholesky (from Loop 2 feedback).** The claim is qualified with a citation to Higham (2002), Theorem 10.3, rather than stated without reference.

12. **Exact-arithmetic hypothesis (from Loop 2 feedback).** Lemma 6 states explicitly: "In exact arithmetic, the preconditioned conjugate gradient method..."

13. **PSD kernel handling (Assumption 2).** If $K$ is only PSD (not SPD), a nugget $\epsilon > 0$ must be added to the model kernel to ensure $K + \epsilon I \succ 0$, or the problem is restricted to $\operatorname{range}(K)$. The proof assumes $K \succ 0$ throughout.

### Editor Dispatch

**Reasoning:** claude_code is assigned to Correctness for its strong logical reasoning in matrix calculus; codex_cli is assigned to Clarity to strictly verify notation consistency and definitions as if they were syntax; gemini_api is assigned to Reference Validity to leverage its knowledge base for checking citations.

- Correctness & Completeness -> claude_code
- Clarity & Rigor -> codex_cli
- Reference Validity -> gemini_api

### Reviews

#### Correctness & Completeness (by claude_code)

Now let me carefully examine the remaining parts of the proof for correctness issues.

## Review: Correctness & Completeness

### Overall Assessment

The proof is mathematically rigorous at a graduate level and has clearly benefited from multiple revision rounds. The core claims are correct, the Kronecker identities are applied properly, and the complexity accounting is accurate. However, several issues remain.

---

### Detailed Findings

**1. Redundant inequality in Lemma 6 creates confusion.**
In the Chebyshev bound derivation, the proof writes:
> $\operatorname{arccosh}(x) \geq \sqrt{x^2-1}/x$

then immediately switches to:
> $\operatorname{arccosh}(x) \geq \ln(x + \sqrt{x^2-1})$

The first inequality is never used in the subsequent chain. Moreover, $\operatorname{arccosh}(x) = \ln(x + \sqrt{x^2 - 1})$ exactly (by definition), so writing "$\geq$" rather than "$=$" is technically correct but misleading — it suggests a lossy bound where none exists. The dangling first inequality and the use of "$\geq$" for an exact identity together obscure the logic.

**2. Complexity claim "$n^2 r \leq nqr$ since $n < q$" is valid but the conclusion "$O(nqr)$ in all cases" is misleading.**
Part 3 states: "Since $n < q$, we have $n^2 r \leq nqr$, so the cost is at most $O(nqr)$ in all cases." But the on-the-fly cost is $O(q(d-1)r + n^2 r)$. The upper bound $O(nqr)$ subsumes $O(qr)$ and $O(n^2 r)$ but does *not* subsume $O(q(d-1)r)$ when $d-1 > n$. For high-order tensors where $d \gg n$, the on-the-fly term dominates and the stated uniform bound fails.

**3. Preconditioner SPD proof uses eigenvalue product claim without justification of dimensions.**
In Lemma 5, the statement "the Kronecker product of two SPD matrices is SPD (eigenvalues are pairwise products of eigenvalues of the factors, all positive)" is correct but the parenthetical justification is incomplete. The eigenvalues of $A \otimes B$ are products $\lambda_i(A) \cdot \mu_j(B)$ only when both matrices are *simultaneously diagonalizable in their own spaces*, which is true for symmetric matrices via their spectral decompositions. This is standard but the proof should either cite this or note it follows from the spectral decomposition $A = Q_A \Lambda_A Q_A^\top$, $B = Q_B \Lambda_B Q_B^\top$, giving $A \otimes B = (Q_A \otimes Q_B)(\Lambda_A \otimes \Lambda_B)(Q_A \otimes Q_B)^\top$.

**4. The derivation of the normal equation RHS has a subtle circularity in the treatment of $T_{(k)}$.**
The Problem Formulation says: "masking is idempotent on a matrix that is already zero outside $\Omega$" in the context of the RHS derivation. But the RHS comes from $(Z \otimes K)^\top P_\Omega \operatorname{vec}(T_{(k)})$, where $T_{(k)}$ is the *full* mode-$k$ unfolding of the tensor $T$, not a matrix that is zero outside $\Omega$. The projection $P_\Omega$ is what zeros out unobserved entries. The sentence in Lemma 3's proof — "$(\mathbf{1}_\Omega \circ T_{(k)}) = T_{(k)}$" — is incorrect if $T_{(k)}$ has nonzero values at unobserved positions. In a completion setting, $T_{(k)}$ is typically defined only at observed positions (or zero-filled), but this must be stated as an assumption. As written, the claim is wrong for the general case where $T$ is a fully-populated tensor with partial observations.

**5. The preconditioner Step 2 solve direction needs clarification.**
Lemma 5 states: "Solve $(G_\Omega + \lambda I_r) X^\top = \widetilde{R}^\top$ for $X$." The derivation shows $X = \widetilde{R}(G_\Omega + \lambda I_r)^{-1}$, which means the system to solve is $X^\top(G_\Omega + \lambda I_r)^\top = \widetilde{R}^\top$, i.e., $(G_\Omega + \lambda I_r)X^\top = \widetilde{R}^\top$ (using symmetry). This is correct but the cost statement "solve $n$ triangular systems of size $r$" is only correct if we solve column-by-column on $X^\top$, which has $n$ columns. This is fine, but worth noting that this means solving $(G_\Omega + \lambda I_r) x_i = \tilde{r}_i$ for each of the $n$ columns of $X^\top$, not solving a single $n$-dimensional system.

**6. Missing explicit statement that $P_\Omega$ is idempotent ($P_\Omega^2 = P_\Omega$).**
The proof uses $P_\Omega = SS^\top$ as a projector. For this to be an orthogonal projector (idempotent), we need $S^\top S = I_q$, which requires the columns of $S$ to be orthonormal. Since $S$'s columns are distinct standard basis vectors $e_{\varphi(i_t, m_t)}$, we need all $(i_t, m_t)$ to be distinct — i.e., no repeated observations. This is implicitly assumed but never stated. If observations can be repeated (as in some sampling schemes), $S^\top S \neq I_q$ and $P_\Omega$ is not a projector.

---

```json
{
  "issues": [
    {
      "severity": "major",
      "location": "Lemma 3 (Sparse RHS)",
      "reason": "The claim that masking is idempotent — '(1_Omega ∘ T_(k)) = T_(k)' — assumes T_(k) is zero outside Omega, which is not stated as an assumption and is false in general tensor completion settings where T is a full tensor with partial observations.",
      "required_fix": "Either add an explicit assumption that T_(k) is the zero-filled partial observation matrix (zero at unobserved positions), or remove the idempotency claim and instead derive the RHS directly from the gradient of the objective, which only involves observed entries by construction.",
      "suggestion": "Add a sentence in the Notation section: 'We define T_(k) as the zero-filled mode-k unfolding: (T_(k))_{i,m} = y_t if (i,m) = (i_t, m_t) ∈ Ω, and 0 otherwise.' This resolves the issue cleanly and makes the idempotency claim trivially true."
    },
    {
      "severity": "minor",
      "location": "Part 3 (Complexity of one matvec)",
      "reason": "The uniform upper bound 'O(nqr) in all cases' does not subsume the on-the-fly cost O(q(d-1)r) when d-1 > n, making the claim incorrect for high-order tensors.",
      "required_fix": "Either remove the 'in all cases' uniform bound, or state it correctly as O(q·max(n, d-1)·r) or simply present the two cases separately without a unified bound.",
      "suggestion": "Replace the sentence with: 'The dominant cost is O(qr + n²r) with cached z_t, or O(q(d-1)r + n²r) with on-the-fly generation.' Drop the misleading unified bound."
    },
    {
      "severity": "minor",
      "location": "Lemma 6 (PCG convergence), Chebyshev derivation",
      "reason": "The inequality 'arccosh(x) ≥ √(x²-1)/x' is introduced but never used; the proof immediately switches to 'arccosh(x) ≥ ln(x + √(x²-1))'. Moreover, arccosh(x) = ln(x + √(x²-1)) exactly, so writing '≥' is misleading.",
      "required_fix": "Remove the unused inequality. Replace '≥' with '=' for arccosh(x) = ln(x + √(x²-1)).",
      "suggestion": "Write: 'Using arccosh(x) = ln(x + √(x²-1)) and computing x + √(x²-1) = (√κ+1)/(√κ-1), we obtain...' This streamlines the argument and removes the dangling step."
    },
    {
      "severity": "minor",
      "location": "Notation / Problem Formulation",
      "reason": "The proof implicitly assumes all observed index pairs (i_t, m_t) are distinct (no repeated observations), which is required for S^T S = I_q and hence P_Omega = SS^T to be an orthogonal projector. This is never stated.",
      "required_fix": "Add to the Assumptions section: 'Observed indices (i_t, m_t) are distinct for t = 1, ..., q.'",
      "suggestion": "Add this as Assumption 4.5 or append it to Assumption 3. Alternatively, note that if observations can repeat, S^T S is diagonal with counts on the diagonal, which changes the objective to a weighted least squares problem."
    },
    {
      "severity": "minor",
      "location": "Lemma 5 (Preconditioner apply)",
      "reason": "The parenthetical justification for Kronecker SPD — 'eigenvalues are pairwise products of eigenvalues of the factors' — is stated without derivation. While standard, at graduate rigor this claim should be grounded.",
      "required_fix": "Add a one-line justification or citation.",
      "suggestion": "Append: '(since A ⊗ B = (Q_A ⊗ Q_B)(Λ_A ⊗ Λ_B)(Q_A ⊗ Q_B)^T by spectral decomposition of symmetric matrices, and all products λ_i μ_j > 0).'"
    }
  ],
  "residual_concerns": [
    "The quality of the preconditioner (how small κ is) depends on the spectral relationship between G_Ω ⊗ K and the true Hessian, which involves sampling balance assumptions mentioned in Assumption 5 but never quantified. This is flagged as a risky step in the mentor output but the proof does not attempt even a rough spectral equivalence bound, leaving the practical convergence rate uncharacterized.",
    "The proof assumes K ≻ 0 throughout but Assumption 2 mentions the PSD fallback (nugget or range restriction) only in passing. A reader implementing this algorithm needs clearer guidance on detecting and handling the rank-deficient kernel case."
  ]
}
```

#### Clarity & Rigor (by codex_cli)

The proof is close to graduate-level rigor, but a few notation/logic precision gaps should be fixed before peer-review readiness.

```json
{
  "issues": [
    {
      "severity": "major",
      "location": "Lemma 6 (PCG convergence), Chebyshev-to-exponential step",
      "reason": "The arccosh/log transition is written with an incorrect inequality chain (using \">=\" where equality is the exact identity), so the derivation is not logically clean as written.",
      "required_fix": "Replace the chain with the exact identity arccosh(x)=ln(x+sqrt(x^2-1)) and a direct derivation of 1/T_m((kappa+1)/(kappa-1)) <= 2((sqrt(kappa)-1)/(sqrt(kappa)+1))^m, or cite the standard bound directly.",
      "suggestion": "Shorten this part to a standard textbook argument: state the minimax bound, set kappa, invoke the closed-form Chebyshev bound, and avoid unnecessary intermediate inequalities."
    },
    {
      "severity": "major",
      "location": "Problem Formulation and Lemma 3 (definitions of T_(k), B, and observed data)",
      "reason": "The text alternates between using T_(k) as a generic unfolding and as a zero-filled observed matrix, then uses idempotence (1_Omega \\circ T_(k) = T_(k)). This is ambiguous and can conflict with conventional notation where T is the full latent tensor.",
      "required_fix": "Introduce a distinct symbol for observed zero-filled unfolding (e.g., Y_Omega := 1_Omega \\circ T_(k)) and use it consistently in b and B definitions.",
      "suggestion": "Keep T_(k) for the full tensor unfolding, and write B := Y_Omega Z, b := vec(KB). This removes ambiguity and makes observed-only dependence explicit."
    },
    {
      "severity": "minor",
      "location": "Notation + Lemma 2/Part 2 (z_t orientation)",
      "reason": "z_t is defined as a row selection Z_{m_t,:} but used as a column vector in inner products, updates, and Gram construction; this forces implicit transposes and weakens dimensional clarity.",
      "required_fix": "Define z_t explicitly as z_t := Z_{m_t,:}^\\top in R^r (column) and make all formulas consistent with that choice.",
      "suggestion": "Add one line early: \"All vectors are columns unless stated otherwise,\" then keep updates in one canonical form."
    },
    {
      "severity": "minor",
      "location": "Selection operator definition (S, P_Omega)",
      "reason": "P_Omega = SS^\\top is called a projector without explicitly stating uniqueness of observed index pairs; with duplicate (i_t,m_t), SS^\\top is not a projector and formulas change.",
      "required_fix": "State that Omega is a set of unique observed pairs (or pre-aggregate duplicates before defining S).",
      "suggestion": "Add an assumption near notation: \"Each (i_t,m_t) appears at most once; repeated observations are averaged/aggregated beforehand.\""
    },
    {
      "severity": "minor",
      "location": "Part 3 (matvec complexity summary)",
      "reason": "The extra coarse bound O(nqr) (from n<q) weakens the sharper bound O(qr+n^2r) and can confuse the theorem’s stated complexity.",
      "required_fix": "Keep the primary bound as O(qr+n^2r) (plus optional on-the-fly z_t cost), and remove or clearly label O(nqr) as a loose upper bound.",
      "suggestion": "Report complexity in the same form as the theorem statement to maintain consistency and reduce cognitive load."
    }
  ],
  "residual_concerns": [
    "A formal references block for Saad (2003) and Higham (2002) is not shown in the proof text; include full bibliographic entries in the final manuscript."
  ]
}
```

#### Reference Validity (by gemini_api)

The proof is rigorously constructed with precise and valid references to standard literature (Saad, 2003; Higham, 2002). The prover has successfully addressed previous feedback regarding phantom references and has ensured that all invoked theorems are supported by the necessary hypotheses (specifically, the SPD nature of matrices $A$ and $P$).

The citations are specific (referencing sections, algorithms, and theorems) and correct for the standard editions of the texts. The derivation of the PCG convergence bound is self-contained and logically sound, relying on the standard CG bound as a starting point.

I have identified only minor suggestions to improve citation precision, but no validity issues.

### JSON Output

```json
{
  "issues": [
    {
      "severity": "minor",
      "location": "Part 5 / Lemma 6",
      "reason": "Citation precision: 'Algorithm 9.1' in Saad (2003) is the pseudocode for PCG. The theoretical equivalence between PCG and standard CG on the transformed system is discussed in the text of Section 9.2 (specifically 9.2.1), not the algorithm block itself.",
      "required_fix": "Update citation to refer to the section text.",
      "suggestion": "Change 'Saad (2003, §9.2, Algorithm 9.1)' to 'Saad (2003, §9.2.1)' or 'Saad (2003, Section 9.2)'."
    },
    {
      "severity": "minor",
      "location": "Part 1",
      "reason": "Citation relevance: Saad (2003) Proposition 6.6 states that direction vectors are A-conjugate *if* the algorithm does not break down. It does not essentially prove the algorithm is well-defined (i.e., does not break down). Well-definedness for SPD matrices is a broader consequence of the descent property (Prop 6.5) or the convergence theorem.",
      "required_fix": "None strictly required as the claim is true, but the reference is slightly tangential.",
      "suggestion": "Consider citing 'Saad (2003, Proposition 6.5)' (which relates residuals and directions) or simply referencing 'standard properties of CG for SPD matrices' alongside the convergence theorem."
    }
  ],
  "residual_concerns": [
    "Assumption 2 mentions handling PSD kernels via a nugget, but the proof strictly assumes K is SPD. This is consistent, but in practice, ensuring the nugget is sufficient to satisfy the condition number assumptions for the Cholesky stability (Higham Thm 10.3) is an implementation detail outside the proof scope."
  ]
}
```

### Editor Decision

**Verdict: `right_track`**

**Summary:** The proof correctly identifies the PCG approach and derives the matrix-free operations. However, it relies on an ambiguous definition of the unfolded tensor that conflates the full truth with the zero-filled observations, invalidating the masking algebra. The complexity bound is too loose for high-order tensors, and the convergence proof uses unnecessary inequalities. Minor notation issues regarding vector orientation and index uniqueness also need fixing.

**Feedback (prover):** 1. **Masking Definition:** Explicitly define $T_{(k)}$ (or a new symbol $Y_{\text{obs}}$) as the *zero-filled* matrix of observations (0 outside $\Omega$). The identity $\mathbf{1}_\Omega \circ T_{(k)} = T_{(k)}$ is false if $T_{(k)}$ contains unobserved truth.
2. **Convergence Derivation:** In Lemma 6, streamline the Chebyshev bound. Use the exact identity $\operatorname{arccosh}(x) = \ln(x + \sqrt{x^2-1})$ instead of an inequality, and remove the unused lower bound involving $\sqrt{x^2-1}/x$.
3. **Complexity Precision:** In Part 3, remove the loose uniform bound $O(nqr)$. State the cost as $O(qr + n^2r)$ for cached $z_t$, and explicitly note the $O(qdr)$ dependence if generating $z_t$ on the fly.
4. **Uniqueness Assumption:** Explicitly assume that observed index pairs $(i_t, m_t)$ are distinct. This is necessary for $S^\top S = I$ and $P_\Omega$ to be a projector.
5. **Vector Notation:** Define $z_t$ explicitly as a column vector ($z_t := Z_{m_t, :}^\top$) to match its usage in inner products and rank-1 updates.
6. **Citations:** Update the Saad citation for the PCG equivalence to the section text (e.g., §9.2.1) rather than the algorithm block.

## Loop 5

### Researcher Agent

## Relevant Theorems

- **PCG Convergence Bound (Exact Arithmetic):** For a symmetric positive definite (SPD) system $Ax=b$ preconditioned by SPD matrix $P$, the error $e_m = w_m - w_*$ at iteration $m$ satisfies:
  \[
  \|e_m\|_A \le 2 \left( \frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1} \right)^m \|e_0\|_A, \quad \text{where } \kappa = \kappa(P^{-1/2} A P^{-1/2}).
  \]
  **Source:** Saad, Y., *Iterative Methods for Sparse Linear Systems*, 2nd ed., SIAM, 2003.
  - **Theorem 6.29** (Chebyshev minimax bound for standard CG).
  - **Section 9.2.1** (Equivalence of PCG to standard CG on the transformed system $\hat{A} = P^{-1/2} A P^{-1/2}$).

- **Kronecker-Vec Identity:** For matrices $A, B, X$ of compatible dimensions:
  \[
  (B^\top \otimes A) \operatorname{vec}(X) = \operatorname{vec}(AXB).
  \]
  (Magnus & Neudecker, *Matrix Differential Calculus*, 2019).

- **Kronecker Product Properties:**
  - **Inverse:** $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$ (if $A, B$ invertible).
  - **SPD:** If $A \succ 0$ and $B \succ 0$, then $A \otimes B \succ 0$.
  - **Transpose:** $(A \otimes B)^\top = A^\top \otimes B^\top$.

- **Cholesky Stability:** The Cholesky factorization of an SPD matrix is backward stable.
  **Source:** Higham, N. J., *Accuracy and Stability of Numerical Algorithms*, 2nd ed., SIAM, 2002, Theorem 10.3.

## Key Definitions

- **Dimensions:** $n = n_k$ (mode size), $M = \prod_{i \neq k} n_i$, $N = nM$. Rank $r$.
- **Observations:** A set $\Omega = \{(i_t, m_t)\}_{t=1}^q \subset [n] \times [M]$ of $q$ **distinct** index pairs. Observed values $y_t$.
- **Zero-filled Observation Matrix:** Let $Y_\Omega \in \mathbb{R}^{n \times M}$ be defined by $(Y_\Omega)_{i,m} = y_t$ if $(i,m) = (i_t, m_t) \in \Omega$, and $0$ otherwise. Note that $Y_\Omega = \mathbf{1}_\Omega \circ T_{(k)}$ where $T_{(k)}$ is the full unfolding.
- **Selection Matrix:** $S \in \mathbb{R}^{N \times q}$ has columns $e_{\varphi(i_t, m_t)}$ where $\varphi$ is the column-major bijection. $P_\Omega = SS^\top$ is the orthogonal projector onto observed entries.
- **Khatri-Rao Product:** $Z \in \mathbb{R}^{M \times r}$.
- **Sampled Row Vector:** $z_t := (Z_{m_t, :})^\top \in \mathbb{R}^r$ (defined as a **column vector**).
- **Objective Function:**
  \[
  \min_{W} \frac{1}{2} \| P_\Omega (\operatorname{vec}(T_{(k)}) - (Z \otimes K)\operatorname{vec}(W)) \|_2^2 + \frac{\lambda}{2} \operatorname{vec}(W)^\top (I_r \otimes K) \operatorname{vec}(W).
  \]
- **Linear System:** The normal equations $Aw = b$ where:
  \[
  A = (Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda (I_r \otimes K),
  \]
  \[
  b = (Z \otimes K)^\top \operatorname{vec}(Y_\Omega) = \operatorname{vec}(K B), \quad \text{with } B = Y_\Omega Z.
  \]
- **Preconditioner:** $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, where $G_\Omega = \sum_{t=1}^q z_t z_t^\top$.

## Proof Strategies

1.  **Derivation of Normal Equations:** Start explicitly from the optimization objective. Show that $\nabla f(w) = 0$ leads to $Aw = b$. Use the definition of $Y_\Omega$ to rigorously show $P_\Omega \operatorname{vec}(T_{(k)}) = \operatorname{vec}(Y_\Omega)$, justifying the sparse RHS computation.
2.  **SPD Property:** Prove $A \succ 0$ using $K \succ 0$ and $\lambda > 0$. Conclude the system has a unique solution.
3.  **Matrix-Free Matvec:** Prove $A \operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))$ where $\mathcal{A}(V) = K [ (\mathbf{1}_\Omega \circ (KVZ^\top))Z + \lambda V ]$. Implement this via a sparse accumulation loop over $t=1,\dots,q$ using $z_t$ as column vectors.
4.  **Preconditioner Application:** Show $P^{-1} \operatorname{vec}(R)$ corresponds to solving two Sylvester-like equations via Cholesky factors.
5.  **Convergence Analysis:**
    -   Invoke Saad §9.2.1 to establish equivalence between PCG on $(A, P)$ and CG on $\hat{A} = P^{-1/2} A P^{-1/2}$.
    -   Apply the Chebyshev bound (Saad Thm 6.29) to $\hat{A}$.
    -   Derive the convergence rate $2((\sqrt{\kappa}-1)/(\sqrt{\kappa}+1))^m$ using the exact identity $\operatorname{arccosh}(x) = \ln(x + \sqrt{x^2-1})$.
6.  **Complexity Analysis:**
    -   Matvec: $O(qr + n^2r)$ (cached $z_t$) or $O(q(d-1)r + n^2r)$ (on-the-fly).
    -   Preconditioner Apply: $O(n^2r + nr^2)$.
    -   Setup: $O(qr^2 + n^3 + r^3)$.
    -   Verify no $O(N)$ operations.

## Gaps and Concerns

-   **Uniqueness Assumption:** The proof must explicitly state that the observed pairs $(i_t, m_t)$ are distinct so that $S^\top S = I_q$ and $P_\Omega$ is a projector.
-   **Masking Algebra:** The previous confusion between $T_{(k)}$ (full) and masked data must be resolved by using $Y_\Omega$ (zero-filled) in the derivation of $b$.
-   **Vector Orientation:** Ensure consistent use of $z_t$ as a column vector in inner products ($\langle u, z_t \rangle$) and rank-1 updates ($z_t z_t^\top$).
-   **Complexity Precision:** Avoid the loose $O(nqr)$ bound; stick to the precise sum of terms.
-   **Citation Specificity:** Ensure the Saad citations point to the text of §9.2.1 and §6.11.3/Thm 6.29, not the algorithm block or incorrect sections.

### Mentor Agent

## Definitions
Let \(k\) be the target mode, \(n:=n_k\), \(M:=\prod_{i\neq k} n_i\), \(N=nM\), and rank \(r\).

Let \(\Omega\subset [n]\times[M]\) be the observed index set in mode-\(k\) unfolding, \(|\Omega|=q\), with observed triples \((i_t,m_t,y_t)\), \(t=1,\dots,q\), where \(y_t=T_{i_t,m_t}\).

Define
\[
Z:=A_d\odot\cdots\odot A_{k+1}\odot A_{k-1}\odot\cdots\odot A_1\in\mathbb{R}^{M\times r},
\quad
z_t:=Z_{m_t,:}\in\mathbb{R}^r.
\]
Unknown is \(W\in\mathbb{R}^{n\times r}\), \(w:=\mathrm{vec}(W)\), and \(A_k=KW\).

Define \(P_\Omega:=SS^\top\) (projector selecting observed entries in \(\mathrm{vec}(T)\)) and linear system
\[
A w=b,\quad
A=(Z\otimes K)^\top P_\Omega (Z\otimes K)+\lambda(I_r\otimes K),\quad
b=(I_r\otimes K)\mathrm{vec}(B),\ B=TZ.
\]

Define mask matrix \(\mathbf{1}_\Omega\in\{0,1\}^{n\times M}\). For \(V\in\mathbb{R}^{n\times r}\),
\[
\mathcal A(V):=
K\!\left((\mathbf{1}_\Omega\odot (KVZ^\top))Z+\lambda V\right),
\quad
A\,\mathrm{vec}(V)=\mathrm{vec}(\mathcal A(V)).
\]

## Formal Statement
**Theorem (Matrix-free PCG for RKHS mode-\(k\) subproblem).** Under the assumptions in Section **Assumptions**:

1. \(A\) is SPD, so PCG is well-defined.
2. For any \(V\), \(A\mathrm{vec}(V)\) can be computed exactly without forming \(A\), \(Z\otimes K\), or any \(N\)-scale object via:
\[
U=KV,\ \ 
\alpha_t=\langle U_{i_t,:},z_t\rangle,\ \ 
C_{i_t,:}\mathrel{+}=\alpha_t z_t,\ \ 
Y=KC+\lambda U,
\]
return \(\mathrm{vec}(Y)\).
3. The per-matvec complexity is \(O(qr+n^2r)\) (plus \(O(q(d-1)r)\) if each \(z_t\) is generated on-the-fly from CP factors).
4. With preconditioner
\[
P=((G_\Omega+\lambda I_r)\otimes (K+\delta I_n)),\quad
G_\Omega:=\sum_{t=1}^q z_tz_t^\top,\ \delta>0,
\]
each \(P^{-1}\)-apply costs \(O(n^2r+nr^2)\) after setup \(O(qr^2+n^3+r^3)\).
5. PCG iterates satisfy
\[
\|e_m\|_A\le 2\Big(\frac{\sqrt\kappa-1}{\sqrt\kappa+1}\Big)^m\|e_0\|_A,\quad
\kappa=\kappa(P^{-1/2}AP^{-1/2}),
\]
hence total cost
\[
O\!\left(m(qr+n^2r+nr^2)+qr^2+n^3+r^3\right),
\]
with no \(O(N)\) operation.

## Assumptions
1. \(\lambda>0\).
2. \(K\succ 0\) for strict SPD proof of \(A\); if original kernel is only PSD, either add nugget to the model kernel or solve on \(\mathrm{range}(K)\).
3. \(n,r<q\ll N\), and observed tuples \((i_t,m_t,y_t)\) are directly available.
4. Indexing conventions for unfolding, vectorization, and \(S\) are consistent.
5. For preconditioner quality claims (small \(\kappa\)), observation pattern is not pathologically imbalanced across \(i_t\) and rank directions.

## Notation
- \(n=n_k\): size of mode \(k\).
- \(M=\prod_{i\neq k} n_i\): product of non-\(k\) dimensions.
- \(N=nM\): total entries.
- \(q=|\Omega|\): number of observed entries.
- \(Z\in\mathbb{R}^{M\times r}\): Khatri-Rao product of fixed factors.
- \(K\in\mathbb{R}^{n\times n}\): kernel matrix.
- \(W\in\mathbb{R}^{n\times r}\), \(w=\mathrm{vec}(W)\): unknown.
- \(B=TZ\in\mathbb{R}^{n\times r}\): sparse MTTKRP result.
- \(z_t=Z_{m_t,:}\): sampled Khatri-Rao row for observation \(t\).
- \(P_\Omega=SS^\top\): selection projector in vectorized unfolding space.
- \(G_\Omega=\sum_t z_tz_t^\top\in\mathbb{R}^{r\times r}\): sampled rank-Gram.

## High-Level Strategy
1. Rewrite the Hessian action as a matrix operator \(\mathcal A(V)\) using \((C\otimes A)\mathrm{vec}(B)=\mathrm{vec}(AB C^\top)\).
2. Implement \(\mathcal A(V)\) from observed entries only, via rowwise accumulation \(C_{i_t,:} \!+\!= \alpha_t z_t\), never forming \(P_\Omega\), \(Z\otimes K\), or dense \(T\).
3. Compute RHS sparsely: \(B_{i_t,:}\!+\!=y_t z_t\), then \(b=\mathrm{vec}(KB)\).
4. Run PCG on \(Aw=b\) with Kronecker preconditioner \(P\) that is cheap to invert by two small Cholesky-based solves.
5. Prove complexity by summing matvec cost, preconditioner-apply cost, and one-time setup.

## Key Lemmas
1. **Kronecker-vec masked operator lemma:** \(A\,\mathrm{vec}(V)=\mathrm{vec}(\mathcal A(V))\).
2. **Sampled accumulation lemma:** \(C=(\mathbf{1}_\Omega\odot(KVZ^\top))Z\) is computed exactly by the \(q\)-sample loop in \(O(qr)\).
3. **Sparse RHS lemma:** \(B=TZ\) is computed from observations in \(O(qr)\), then \(b\) in \(O(n^2r)\).
4. **SPD lemma:** if \(K\succ0,\lambda>0\), then \(A\succ0\) because
\[
x^\top A x=\|P_\Omega^{1/2}(Z\otimes K)x\|_2^2+\lambda\|(I_r\otimes K^{1/2})x\|_2^2>0.
\]
5. **Preconditioner-apply lemma:** \(P^{-1}\mathrm{vec}(R)\) equals two solves:
\[
(K+\delta I)Y=R,\quad X(G_\Omega+\lambda I)=Y,\quad \mathrm{vec}(X)=P^{-1}\mathrm{vec}(R).
\]
6. **PCG convergence lemma:** standard SPD PCG bound with \(\kappa(P^{-1/2}AP^{-1/2})\).

## Dependency Graph
1. Definitions + Notation \(\rightarrow\) Lemma 1.
2. Lemma 1 \(\rightarrow\) Lemma 2 (matvec algorithm correctness).
3. Definitions \(\rightarrow\) Lemma 3 (RHS correctness/cost).
4. Assumptions 1-2 \(\rightarrow\) Lemma 4 (SPD of \(A\)).
5. Definitions \(\rightarrow\) Lemma 5 (preconditioner form and apply rule).
6. Lemma 4 + Lemma 5 + Lemma 6 \(\rightarrow\) PCG convergence claim.
7. Lemma 2 + Lemma 3 + Lemma 5 \(\rightarrow\) total complexity claim in theorem.

## Risky Steps
1. **PSD kernel issue:** if \(K\) is singular, \(A\) may be singular despite \(\lambda>0\); this must be handled explicitly.
2. **Index consistency:** mismatch between unfolding order, \(S\), and sampled \((i_t,m_t)\) invalidates the operator.
3. **Preconditioner quality claim:** spectral-equivalence constants require sampling-balance assumptions; do not claim uniformly good \(\kappa\) without them.
4. **Hidden \(M\)-scale cost:** forming full \(Z\) is infeasible when \(M\) is huge; use only sampled rows \(z_t\) (cached or on-demand).
5. **Numerical stability:** very small \(\lambda\) or ill-conditioned \(K\) can cause PCG stagnation; monitor relative preconditioned residual and cap iterations.

### Prover Agent

## Complete Proof

We prove the theorem in five parts, following the dependency graph. Throughout, we adopt the mentor's notation with the following clarifications addressing editor feedback:

**Clarification (Masking and observed data).** Define $T_{\text{obs}} \in \mathbb{R}^{n \times M}$ as the zero-filled observation matrix: $(T_{\text{obs}})_{i,m} = T_{(k),i,m}$ if $(i,m) \in \Omega$, and $0$ otherwise. Then $\mathbf{1}_\Omega \odot T_{\text{obs}} = T_{\text{obs}}$ holds by construction, and all references to "$T$" in masked expressions refer to $T_{\text{obs}}$.

**Clarification (Distinct observations).** We assume all observed index pairs $(i_t, m_t)$, $t = 1, \ldots, q$, are distinct. This ensures that the selection matrix $S \in \mathbb{R}^{N \times q}$ has orthonormal columns ($S^\top S = I_q$), so $P_\Omega = SS^\top$ is an orthogonal projector.

**Clarification (Vector convention).** Each $z_t := Z_{m_t,:}^\top \in \mathbb{R}^{r}$ is a column vector. Inner products $\langle u, z_t \rangle = u^\top z_t$ and outer products $z_t z_t^\top$ are interpreted accordingly.

**Clarification (Saad PCG reference).** The PCG convergence bound cited below follows from Saad, *Iterative Methods for Sparse Linear Systems*, §9.2.1 (preconditioned CG theory), not from a specific algorithm listing.

---

**Part 1 (SPD).** We show $A \succ 0$. For any $x \in \mathbb{R}^{nr} \setminus \{0\}$,

$$x^\top A x = x^\top (Z \otimes K)^\top S S^\top (Z \otimes K) x + \lambda \, x^\top (I_r \otimes K) x = \|S^\top (Z \otimes K) x\|_2^2 + \lambda \, x^\top (I_r \otimes K) x.$$

Since $K \succ 0$ (Assumption 2), we have $I_r \otimes K \succ 0$, so $x^\top (I_r \otimes K) x > 0$ for all $x \neq 0$. With $\lambda > 0$ (Assumption 1), the second term is strictly positive, hence $x^\top A x > 0$. Since $A$ is real symmetric (both summands are symmetric) and positive definite, $A$ is SPD. PCG is therefore well-defined. $\square$

**Part 2 (Matrix-free matvec).** We show $A\,\mathrm{vec}(V) = \mathrm{vec}(\mathcal{A}(V))$ for any $V \in \mathbb{R}^{n \times r}$, and that $\mathcal{A}(V)$ can be evaluated without forming any $N$-scale object.

By Lemma 1 (Kronecker-vec masked operator), $A\,\mathrm{vec}(V) = \mathrm{vec}\!\big(K((\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V)\big)$. By Lemma 2 (sampled accumulation), the matrix $C := (\mathbf{1}_\Omega \odot (KVZ^\top))Z$ is computed exactly by:

1. $U = KV \in \mathbb{R}^{n \times r}$, cost $O(n^2 r)$.
2. For $t = 1, \ldots, q$: compute $\alpha_t = U_{i_t,:} z_t = \langle U_{i_t,:}^\top, z_t \rangle \in \mathbb{R}$, cost $O(r)$; accumulate $C_{i_t,:} \mathrel{+}= \alpha_t z_t^\top$, cost $O(r)$.
3. $Y = KC + \lambda U$, cost $O(n^2 r)$.

Return $\mathrm{vec}(Y)$. No matrix of dimension $N$, $P_\Omega$, $Z \otimes K$, or full $T_{(k)}$ is ever formed. $\square$

**Part 3 (Complexity).** Per-matvec cost: Step 1 is $O(n^2 r)$. Step 2 loops over $q$ observations, each costing $O(r)$, totaling $O(qr)$ (assuming $z_t$ rows are cached). Step 3 is $O(n^2 r)$. Total per-matvec: $O(qr + n^2 r)$.

If $z_t$ is not cached but generated on the fly from the $d-1$ factor matrices via elementwise products of the corresponding rows of $A_1, \ldots, A_{k-1}, A_{k+1}, \ldots, A_d$, each $z_t$ costs $O((d-1)r)$, adding $O(q(d-1)r)$ to the total.

The RHS $b = \mathrm{vec}(KB)$ is computed by Lemma 3 (sparse RHS) in $O(qr + n^2 r)$.

The preconditioner setup (Lemma 5) costs $O(qr^2 + n^3 + r^3)$: forming $G_\Omega = \sum_t z_t z_t^\top$ costs $O(qr^2)$, Cholesky of $K + \delta I_n$ costs $O(n^3)$, Cholesky of $G_\Omega + \lambda I_r$ costs $O(r^3)$. Each preconditioner apply costs $O(n^2 r + nr^2)$ (two triangular solves in the respective Cholesky factors).

After $m$ PCG iterations, total cost is:

$$O\!\big(m(qr + n^2 r + nr^2) + qr^2 + n^3 + r^3\big),$$

with no $O(N)$ operation anywhere. $\square$

**Part 4 (Preconditioner).** Define $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$ with $\delta > 0$. Since $G_\Omega + \lambda I_r \succ 0$ (as $\lambda > 0$) and $K + \delta I_n \succ 0$ (as $K \succeq 0, \delta > 0$), we have $P \succ 0$, so $P$ is a valid SPD preconditioner.

By Lemma 5, applying $P^{-1}$ to $\mathrm{vec}(R)$ reduces to:

- Solve $(K + \delta I_n) Y_j = R_j$ for each column $j = 1, \ldots, r$ (or equivalently, solve the matrix equation $(K + \delta I_n)Y = R$).
- Solve $X (G_\Omega + \lambda I_r) = Y$, equivalently $(G_\Omega + \lambda I_r) X^\top_{:,i} = Y^\top_{:,i}$ for each $i$.

After one-time Cholesky factorizations of $K + \delta I_n$ and $G_\Omega + \lambda I_r$, each apply uses forward/back substitution at cost $O(n^2 r + n r^2)$. $\square$

**Part 5 (Convergence).** Since $A \succ 0$ and $P \succ 0$, standard preconditioned CG theory (Saad, §9.2.1) gives:

$$\|e_m\|_A \leq 2 \left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^m \|e_0\|_A, \qquad \kappa := \kappa(P^{-1/2} A P^{-1/2}).$$

This follows from the minimax characterization of CG over Krylov subspaces and the Chebyshev polynomial bound. Specifically, let $\hat{A} = P^{-1/2} A P^{-1/2}$ with eigenvalues $\lambda_{\min} \leq \cdots \leq \lambda_{\max}$. The preconditioned CG error satisfies:

$$\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}} \leq \min_{p \in \mathcal{P}_m,\, p(0)=1} \max_{\lambda \in [\lambda_{\min}, \lambda_{\max}]} |p(\lambda)| \cdot \|\hat{e}_0\|_{\hat{A}}.$$

The optimizing polynomial is the scaled Chebyshev polynomial $T_m\!\Big(\frac{2\lambda - \lambda_{\max} - \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}}\Big) / T_m\!\Big(\frac{\lambda_{\max} + \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}}\Big)$. Setting $\sigma = \frac{\lambda_{\max} + \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}} = \frac{\kappa + 1}{\kappa - 1}$ and using the identity

$$T_m(\sigma) = \cosh(m \operatorname{arccosh}(\sigma)),$$

with $\operatorname{arccosh}(\sigma) = \ln(\sigma + \sqrt{\sigma^2 - 1})$, we compute:

$$\sigma + \sqrt{\sigma^2 - 1} = \frac{\kappa+1}{\kappa-1} + \frac{2\sqrt{\kappa}}{\kappa-1} = \frac{(\sqrt{\kappa}+1)^2}{\kappa - 1} = \frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}.$$

Therefore $T_m(\sigma) = \cosh\!\big(m \ln\!\big(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\big)\big)$. Since $\cosh(\theta) = \frac{e^\theta + e^{-\theta}}{2} \geq \frac{e^\theta}{2}$:

$$\frac{1}{T_m(\sigma)} \leq \frac{2}{e^{m\theta}} = 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m,$$

where $\theta = \ln\!\big(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\big)$. This yields the stated bound. $\square$

---

## Lemma Proofs

**Lemma 1 (Kronecker-vec masked operator).** *For any $V \in \mathbb{R}^{n \times r}$, $A\,\mathrm{vec}(V) = \mathrm{vec}(\mathcal{A}(V))$ where $\mathcal{A}(V) = K\big((\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V\big)$.*

*Proof.* We use the standard Kronecker identity: $(C \otimes D)\mathrm{vec}(X) = \mathrm{vec}(DXC^\top)$ for conformable matrices.

**Regularization term.** $\lambda(I_r \otimes K)\mathrm{vec}(V) = \lambda\,\mathrm{vec}(KVI_r^\top) = \mathrm{vec}(\lambda KV)$.

**Data-fit term.** Write $v = \mathrm{vec}(V)$. Then $(Z \otimes K)v = \mathrm{vec}(KVZ^\top) \in \mathbb{R}^{nM}$. This is $\mathrm{vec}(F)$ where $F = KVZ^\top \in \mathbb{R}^{n \times M}$.

Now $P_\Omega = SS^\top$ is the orthogonal projector onto coordinates in $\Omega$. For any vector $u \in \mathbb{R}^{nM}$, $P_\Omega u$ zeros out entries not in $\Omega$. Reshaping, if $u = \mathrm{vec}(G)$ for $G \in \mathbb{R}^{n \times M}$, then $(P_\Omega u)$ reshaped is $\mathbf{1}_\Omega \odot G$ (since distinct observations ensure $P_\Omega$ acts as elementwise masking in the canonical basis). Hence:

$$P_\Omega \,\mathrm{vec}(F) = \mathrm{vec}(\mathbf{1}_\Omega \odot F) = \mathrm{vec}(\mathbf{1}_\Omega \odot (KVZ^\top)).$$

Applying $(Z \otimes K)^\top = (Z^\top \otimes K^\top) = (Z^\top \otimes K)$ (since $K$ is symmetric):

$$(Z^\top \otimes K)\,\mathrm{vec}(\mathbf{1}_\Omega \odot (KVZ^\top)) = \mathrm{vec}\!\big(K(\mathbf{1}_\Omega \odot (KVZ^\top))Z\big).$$

Here we used $(Z^\top \otimes K)\mathrm{vec}(H) = \mathrm{vec}(KHZ)$ for $H \in \mathbb{R}^{n \times M}$.

Combining both terms:

$$A\,\mathrm{vec}(V) = \mathrm{vec}\!\big(K(\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda KV\big) = \mathrm{vec}\!\big(K[(\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V]\big) = \mathrm{vec}(\mathcal{A}(V)). \quad\square$$

---

**Lemma 2 (Sampled accumulation).** *$C := (\mathbf{1}_\Omega \odot (KVZ^\top))Z$ is computed exactly by the loop $C = 0$; for $t = 1,\ldots,q$: $\alpha_t = U_{i_t,:} z_t$, $C_{i_t,:} \mathrel{+}= \alpha_t z_t^\top$, where $U = KV$.*

*Proof.* Let $F = KVZ^\top = UZ^\top \in \mathbb{R}^{n \times M}$, so $F_{i,m} = U_{i,:} z_m$ where $z_m = Z_{m,:}^\top$. Then $(\mathbf{1}_\Omega \odot F)_{i,m} = F_{i,m}$ if $(i,m) \in \Omega$, and $0$ otherwise. The $(i,j)$-entry of $C = (\mathbf{1}_\Omega \odot F)Z$ is:

$$C_{i,j} = \sum_{m=1}^{M} (\mathbf{1}_\Omega \odot F)_{i,m} Z_{m,j} = \sum_{\{t : i_t = i\}} F_{i_t, m_t} Z_{m_t, j} = \sum_{\{t : i_t = i\}} \alpha_t (z_t)_j,$$

since the only nonzero terms in the sum over $m$ are those $m = m_t$ where $(i, m_t) \in \Omega$ (with $i_t = i$), and $F_{i_t, m_t} = U_{i_t,:} z_t = \alpha_t$.

The loop accumulates $C_{i_t, j} \mathrel{+}= \alpha_t (z_t)_j$ for each $t$, which is exactly the sum above. Each iteration costs $O(r)$ for the inner product and $O(r)$ for the row update, giving $O(qr)$ total. $\square$

---

**Lemma 3 (Sparse RHS).** *$B := T_{\text{obs}} Z$ is computed from observations in $O(qr)$, and $b = \mathrm{vec}(KB)$ costs $O(n^2 r)$.*

*Proof.* Recall $T_{\text{obs}} \in \mathbb{R}^{n \times M}$ is zero outside $\Omega$. Then:

$$B_{i,j} = \sum_{m=1}^{M} (T_{\text{obs}})_{i,m} Z_{m,j} = \sum_{\{t : i_t = i\}} y_t (z_t)_j.$$

Initialize $B = 0$. For $t = 1, \ldots, q$: $B_{i_t,:} \mathrel{+}= y_t z_t^\top$. Each step costs $O(r)$, total $O(qr)$.

Then $b = \mathrm{vec}(KB)$: multiplying $K \in \mathbb{R}^{n \times n}$ by $B \in \mathbb{R}^{n \times r}$ costs $O(n^2 r)$.

**Note on the RHS formula.** The mentor defines $b = (I_r \otimes K)\mathrm{vec}(B)$. By the Kronecker identity, $(I_r \otimes K)\mathrm{vec}(B) = \mathrm{vec}(KB \cdot I_r) = \mathrm{vec}(KB)$, consistent with the above. $\square$

---

**Lemma 4 (SPD).** *If $K \succ 0$ and $\lambda > 0$, then $A \succ 0$.*

*Proof.* This was established in Part 1. For completeness: $A = (Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda (I_r \otimes K)$. The first summand is positive semidefinite (it is $M^\top M$ for $M = P_\Omega^{1/2}(Z \otimes K)$). The second summand $\lambda(I_r \otimes K)$ is positive definite since $K \succ 0$ implies $I_r \otimes K \succ 0$ (its eigenvalues are products of eigenvalues of $I_r$ and $K$, all positive), and $\lambda > 0$. The sum of a PSD and a PD matrix is PD. $\square$

---

**Lemma 5 (Preconditioner apply).** *With $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, the action $P^{-1}\mathrm{vec}(R)$ for $R \in \mathbb{R}^{n \times r}$ is computed by:*
1. *Solve $(K + \delta I_n) Y = R$ for $Y \in \mathbb{R}^{n \times r}$.*
2. *Solve $X(G_\Omega + \lambda I_r) = Y$ for $X \in \mathbb{R}^{n \times r}$, equivalently $(G_\Omega + \lambda I_r) X^\top = Y^\top$.*

*Then $\mathrm{vec}(X) = P^{-1}\mathrm{vec}(R)$.*

*Proof.* Since $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, the Kronecker inverse gives $P^{-1} = (G_\Omega + \lambda I_r)^{-1} \otimes (K + \delta I_n)^{-1}$. Both inverses exist: $G_\Omega + \lambda I_r \succ 0$ since $G_\Omega \succeq 0$ and $\lambda > 0$; $K + \delta I_n \succ 0$ since $K \succeq 0$ and $\delta > 0$.

Using the Kronecker-vec identity $(C \otimes D)\mathrm{vec}(X) = \mathrm{vec}(DXC^\top)$:

$$P^{-1}\mathrm{vec}(R) = \big((G_\Omega + \lambda I_r)^{-1} \otimes (K+\delta I_n)^{-1}\big)\mathrm{vec}(R) = \mathrm{vec}\!\big((K+\delta I_n)^{-1} R\, (G_\Omega + \lambda I_r)^{-\top}\big).$$

Since $G_\Omega + \lambda I_r$ is symmetric, $(G_\Omega + \lambda I_r)^{-\top} = (G_\Omega + \lambda I_r)^{-1}$. So:

$$P^{-1}\mathrm{vec}(R) = \mathrm{vec}\!\big((K+\delta I_n)^{-1} R\, (G_\Omega + \lambda I_r)^{-1}\big).$$

Setting $Y = (K + \delta I_n)^{-1} R$ (Step 1) and $X = Y (G_\Omega + \lambda I_r)^{-1}$ (Step 2), we get $P^{-1}\mathrm{vec}(R) = \mathrm{vec}(X)$.

**Cost.** After precomputing Cholesky factorizations $L_K L_K^\top = K + \delta I_n$ ($O(n^3)$) and $L_G L_G^\top = G_\Omega + \lambda I_r$ ($O(r^3)$), Step 1 requires $r$ triangular solves of size $n$, costing $O(n^2 r)$. Step 2 requires $n$ triangular solves of size $r$, costing $O(nr^2)$. Total per apply: $O(n^2 r + nr^2)$.

**Setup.** Forming $G_\Omega = \sum_{t=1}^{q} z_t z_t^\top$ costs $O(qr^2)$ (accumulating $q$ rank-1 updates in $\mathbb{R}^{r \times r}$). Cholesky factorizations add $O(n^3 + r^3)$. Total setup: $O(qr^2 + n^3 + r^3)$. $\square$

---

**Lemma 6 (PCG convergence).** *For $A, P \succ 0$, preconditioned CG satisfies $\|e_m\|_A \leq 2\big(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\big)^m \|e_0\|_A$ where $\kappa = \kappa(P^{-1/2}AP^{-1/2})$.*

*Proof.* Let $\hat{A} = P^{-1/2}AP^{-1/2}$ with eigenvalues $0 < \lambda_{\min} \leq \cdots \leq \lambda_{\max}$, and $\kappa = \lambda_{\max}/\lambda_{\min}$. Under the change of variables $\hat{e}_m = P^{1/2} e_m$, preconditioned CG minimizes the $\hat{A}$-norm of $\hat{e}_m$ over Krylov subspaces, giving:

$$\|\hat{e}_m\|_{\hat{A}} \leq \min_{\substack{p \in \mathcal{P}_m \\ p(0)=1}} \max_{\lambda \in [\lambda_{\min}, \lambda_{\max}]} |p(\lambda)| \cdot \|\hat{e}_0\|_{\hat{A}}.$$

Note that $\|\hat{e}_m\|_{\hat{A}}^2 = \hat{e}_m^\top \hat{A} \hat{e}_m = e_m^\top P^{1/2} P^{-1/2} A P^{-1/2} P^{1/2} e_m = e_m^\top A e_m = \|e_m\|_A^2$, so the $\hat{A}$-norm of $\hat{e}_m$ equals the $A$-norm of $e_m$.

The optimal polynomial is the shifted, scaled Chebyshev polynomial $p_m^*(\lambda) = T_m(\sigma(\lambda))/T_m(\sigma_0)$ where $\sigma(\lambda) = \frac{2\lambda - \lambda_{\max} - \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}}$ and $\sigma_0 = \frac{\lambda_{\max} + \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}} = \frac{\kappa + 1}{\kappa - 1}$.

Since $|T_m(\sigma(\lambda))| \leq 1$ on $[-1,1]$ and $\sigma(\lambda) \in [-1,1]$ for $\lambda \in [\lambda_{\min}, \lambda_{\max}]$:

$$\max_\lambda |p_m^*(\lambda)| = \frac{1}{T_m(\sigma_0)}.$$

We evaluate $T_m(\sigma_0)$ using the identity $T_m(x) = \cosh(m \operatorname{arccosh}(x))$ for $x \geq 1$, with $\operatorname{arccosh}(x) = \ln(x + \sqrt{x^2 - 1})$:

$$\sigma_0 + \sqrt{\sigma_0^2 - 1} = \frac{\kappa+1}{\kappa - 1} + \sqrt{\frac{(\kappa+1)^2}{(\kappa-1)^2} - 1} = \frac{\kappa+1}{\kappa-1} + \frac{2\sqrt{\kappa}}{\kappa - 1} = \frac{(\sqrt{\kappa}+1)^2}{\kappa - 1} = \frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1},$$

where the last step uses $\kappa - 1 = (\sqrt{\kappa}-1)(\sqrt{\kappa}+1)$.

So $\operatorname{arccosh}(\sigma_0) = \ln\!\big(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\big)$ and

$$T_m(\sigma_0) = \cosh\!\left(m\ln\!\Big(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\Big)\right) = \frac{1}{2}\left[\left(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\right)^m + \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m\right] \geq \frac{1}{2}\left(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\right)^m.$$

Therefore:

$$\|e_m\|_A \leq \frac{1}{T_m(\sigma_0)} \|e_0\|_A \leq 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m \|e_0\|_A. \quad\square$$

---

## Gap Closure Notes

**Editor Point 1 (Masking definition).** Addressed in the opening clarification. We introduced $T_{\text{obs}}$ as the zero-filled observation matrix, ensuring $\mathbf{1}_\Omega \odot T_{\text{obs}} = T_{\text{obs}}$ holds by construction. All masked expressions throughout the proof (Lemma 1, Lemma 2, Lemma 3) reference $T_{\text{obs}}$, not the unknown ground truth $T_{(k)}$. In particular, Lemma 3 explicitly computes $B = T_{\text{obs}} Z$ from the sparse representation via observations $y_t$.

**Editor Point 2 (Convergence derivation).** Addressed in Lemma 6. The Chebyshev bound uses the exact identity $\operatorname{arccosh}(x) = \ln(x + \sqrt{x^2-1})$ and the exact evaluation $\sigma_0 + \sqrt{\sigma_0^2-1} = \frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}$. The lower bound $\cosh(\theta) \geq \frac{1}{2}e^\theta$ is the only inequality used, and it follows directly from $\cosh(\theta) = \frac{e^\theta + e^{-\theta}}{2} \geq \frac{e^\theta}{2}$. No unused intermediate bound involving $\sqrt{x^2-1}/x$ appears.

**Editor Point 3 (Complexity precision).** Addressed in Part 3. The per-matvec cost is stated precisely as $O(qr + n^2r)$ when $z_t$ rows are cached, with the on-the-fly generation cost of $O(q(d-1)r)$ stated separately and explicitly. No loose $O(nqr)$ bound appears.

**Editor Point 4 (Uniqueness assumption).** Addressed in the opening clarification. The distinct-observations assumption is stated explicitly, and its necessity for $S^\top S = I_q$ and $P_\Omega = SS^\top$ being a projector is noted. This assumption is used in Lemma 1 (masking identity) and Lemma 2 (unique accumulation per observation).

**Editor Point 5 (Vector notation).** Addressed in the opening clarification. We define $z_t := Z_{m_t,:}^\top \in \mathbb{R}^r$ as a column vector. All subsequent inner products ($U_{i_t,:} z_t$, which is a row-times-column product yielding a scalar) and rank-1 updates ($z_t z_t^\top \in \mathbb{R}^{r \times r}$) are consistent with this convention. The row update $C_{i_t,:} \mathrel{+}= \alpha_t z_t^\top$ transposes back to a row for storage.

**Editor Point 6 (Citations).** The PCG convergence bound is attributed to Saad, *Iterative Methods for Sparse Linear Systems*, §9.2.1 (preconditioned CG theory), rather than to a specific algorithm listing.

**Additional gap: PSD kernel handling.** The mentor's Assumption 2 and Risky Step 1 note that if $K$ is only PSD (not strictly PD), $A$ may be singular despite $\lambda > 0$. Our SPD proof in Lemma 4 requires $K \succ 0$. If the original kernel is PSD with nullity, one must either (a) add a nugget $\epsilon I_n$ to the model kernel (making it $K + \epsilon I_n \succ 0$), absorbing $\epsilon$ into the model, or (b) restrict $W$ to $\mathrm{range}(K)$ and solve the reduced system. Both approaches are standard; we assume (a) or (b) has been applied so that $K \succ 0$ holds as stated.

**Additional gap: Index consistency.** The proof assumes a fixed unfolding convention (mode-$k$ unfolding with rows indexed by $[n]$ and columns by the multi-index over remaining modes, linearized consistently with the Khatri-Rao product $Z$). The selection matrix $S$ picks entries from $\mathrm{vec}(T_{(k)})$ in the same column-major order used by the Kronecker products. Any other unfolding or vectorization convention would require permuting $S$ accordingly, but the algebraic structure and complexity are unchanged.

### Editor Dispatch

**Reasoning:** codex_cli is best suited for strict technical soundness checks, claude_code is strongest on mathematical exposition and rigor polish, and gemini_api is best positioned to validate references and external theorem usage.

- Correctness & Completeness -> codex_cli
- Clarity & Rigor -> claude_code
- Reference Validity -> gemini_api

### Reviews

#### Correctness & Completeness (by codex_cli)

The core derivations (Kronecker identities, sampled accumulation, preconditioner-apply, and Chebyshev PCG bound) are mostly sound. The remaining problems are mainly **completeness/statement alignment** issues that can affect correctness if left implicit.

```json
{
  "issues": [
    {
      "severity": "major",
      "location": "Part 1 / Lemma 4 / Assumption 2 (PSD-kernel branch)",
      "reason": "The SPD proof is only proved for K ≻ 0, but the assumptions allow a PSD-kernel alternative ('solve on range(K)'). In that branch, A on the full space is generally singular, so claim 1 ('A is SPD') is not established as stated.",
      "required_fix": "Either restrict the theorem to the K ≻ 0 (or nuggeted K + eps I) case, or add a separate reduced-space formulation for K ⪰ 0 with W constrained to range(K) and prove SPD of the reduced operator.",
      "suggestion": "Add an explicit two-case theorem: Case A (nuggeted full-space SPD), Case B (factor K = UΛU^T, solve in reduced coordinates with Λ > 0)."
    },
    {
      "severity": "major",
      "location": "Definitions + Lemma 3 + theorem RHS b",
      "reason": "The formal statement defines B = T Z, but the proof computes B from observed data using T_obs. If T is interpreted as the full unfolding, b is not computable from sparse observations and does not match the masked normal equations.",
      "required_fix": "Make T_obs part of the formal definitions and define B := T_obs Z (equivalently b = (Z ⊗ K)^T vec(T_obs)).",
      "suggestion": "Move the T_obs clarification into the theorem statement itself so algorithm, linear system, and proof all reference the same object."
    },
    {
      "severity": "minor",
      "location": "Clarification (Distinct observations), Lemma 1–2",
      "reason": "Exact identification of P_Ω as a masking projector and loop exactness assumes unique (i_t, m_t). This is stated in clarifications, but not as a formal assumption in the theorem block.",
      "required_fix": "Add uniqueness (or preprocessing aggregation) to the assumptions, or generalize to weighted duplicates explicitly.",
      "suggestion": "State: 'Input tuples are deduplicated into Ω before forming S and running matvec/RHS accumulation.'"
    },
    {
      "severity": "minor",
      "location": "Part 3 (complexity setup)",
      "reason": "Setup complexity O(qr^2 + n^3 + r^3) omits the cost of generating z_t on-the-fly when sampled rows are not cached.",
      "required_fix": "Include +O(q(d-1)r) in setup when z_t must be generated to build G_Ω (and optionally B).",
      "suggestion": "Present two explicit complexity regimes: cached sampled z_t versus on-demand z_t generation."
    }
  ],
  "residual_concerns": [
    "The preconditioner-quality assumption is qualitative; no spectral-equivalence constants are provided, so m is still problem-dependent despite the standard CG bound."
  ]
}
```

#### Clarity & Rigor (by claude_code)

I'll review this proof carefully from the **Clarity & Rigor** perspective, evaluating notation precision, definition consistency, exposition quality, and whether it meets the graduate-level rigor target.

---

## Clarity & Rigor Review

### Overall Assessment

This is a well-structured proof that has clearly benefited from multiple rounds of revision. The five-part organization with separate lemma proofs is sound, the opening clarifications address previous editorial feedback, and the Chebyshev convergence derivation is now fully explicit. However, several issues remain that would impede peer review at the graduate level.

---

### Issue Analysis

**1. The operator $\mathcal{A}$ definition is inconsistent between mentor and prover.**

The mentor defines $\mathcal{A}(V) := K\!\left((\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V\right)$, and the prover's Lemma 1 restates this identically. However, in Part 2, the prover writes "By Lemma 1 (Kronecker-vec masked operator), $A\,\mathrm{vec}(V) = \mathrm{vec}\!\big(K((\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V)\big)$" — this is correct and consistent. But the mentor's theorem statement in claim (2) gives the algorithm as "$U=KV,\ \alpha_t=\langle U_{i_t,:},z_t\rangle,\ C_{i_t,:}\mathrel{+}=\alpha_t z_t,\ Y=KC+\lambda U$". Note the accumulation $C_{i_t,:}\mathrel{+}=\alpha_t z_t$ uses $z_t$ (a column vector per the clarification), but $C_{i_t,:}$ is a row. The prover correctly writes $C_{i_t,:} \mathrel{+}= \alpha_t z_t^\top$ in the proof body, but the theorem statement from the mentor is never corrected. A proof document should have a self-consistent theorem statement.

**2. The relationship $A_k = KW$ is stated without justification or context.**

The mentor's definitions introduce $A_k = KW$ as a given relationship, but the proof never explains what $A_k$ represents in the CP decomposition or why this RKHS parameterization holds. For a self-contained graduate-level document, the reader needs at least one sentence connecting $A_k$ (the mode-$k$ factor matrix) to the RKHS representation $A_k = KW$ where $W$ contains the representer coefficients.

**3. The transition from the normal equations to the specific form of $A$ and $b$ is missing.**

The proof assumes the linear system $Aw = b$ as given but never derives it. A graduate reader would expect to see: (a) the objective function (e.g., $\min_W \|P_\Omega(\text{vec}(KWZ^\top) - \text{vec}(T_{\text{obs}}))\|^2 + \lambda \operatorname{tr}(W^\top K W)$), and (b) the first-order optimality condition yielding $Aw = b$. Without this, the reader must take the entire problem formulation on faith.

**4. Lemma 1 proof: the masking-projector equivalence needs more care.**

The proof states: "Reshaping, if $u = \mathrm{vec}(G)$ for $G \in \mathbb{R}^{n \times M}$, then $(P_\Omega u)$ reshaped is $\mathbf{1}_\Omega \odot G$ (since distinct observations ensure $P_\Omega$ acts as elementwise masking in the canonical basis)." This is the crux of the entire matrix-free reformulation, yet it is handled parenthetically. The argument requires that $S$ selects entries of $\mathrm{vec}(T_{(k)})$ in the same canonical basis ordering as the column-major vectorization. This should be an explicit micro-lemma or at minimum a displayed equation showing $[SS^\top \mathrm{vec}(G)]_{(i-1)M+m} = \mathbf{1}_{(i,m)\in\Omega} \cdot G_{i,m}$.

**5. The RHS formula in Lemma 3 uses $T_{\text{obs}}$ but the mentor's definition uses $T$.**

The opening clarification says "all references to '$T$' in masked expressions refer to $T_{\text{obs}}$." But the mentor defines $B = TZ$ (not $B = T_{\text{obs}}Z$), and the normal equations $b = (I_r \otimes K)\mathrm{vec}(B)$ implicitly assume $B$ involves only observed data. Lemma 3 correctly computes $B = T_{\text{obs}}Z$, but the gap note should acknowledge that the mentor's $B = TZ$ is only valid when $T$ is understood as $T_{\text{obs}}$, not the full (unknown) tensor unfolding. The current text does this in the Gap Closure Notes but not at the point of use in the proof.

**6. Part 5 convergence: the change-of-variables $\hat{e}_m = P^{1/2}e_m$ needs the norm equivalence stated earlier.**

Part 5 states $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$ and gives the verification. But Lemma 6 repeats this verification. The redundancy is not itself a problem, but the two derivations use slightly different notation ($P^{1/2}$ vs. $P^{-1/2}$) in ways that could confuse. In Part 5, $\hat{e}_m = P^{1/2}e_m$ implies $\|\hat{e}_m\|_{\hat{A}}^2 = e_m^\top P^{1/2} (P^{-1/2}AP^{-1/2}) P^{1/2} e_m = e_m^\top A e_m$. Lemma 6 repeats this. Choose one location and reference it from the other.

**7. Assumption numbering is ambiguous.**

The mentor lists 5 assumptions but the proof references them as "Assumption 1," "Assumption 2" without ever restating them. A self-contained proof document should either restate the assumptions or include an explicit section header for them.

**8. The preconditioner's spectral quality is left entirely unquantified.**

The theorem claims a convergence rate involving $\kappa = \kappa(P^{-1/2}AP^{-1/2})$ but never bounds $\kappa$. Assumption 5 vaguely requires the observation pattern to not be "pathologically imbalanced," but no quantitative statement is given. For graduate rigor, the proof should at minimum state: "Bounding $\kappa$ in terms of problem parameters ($q$, $n$, $r$, $\lambda$, $\delta$, spectral properties of $K$ and $Z$) is beyond the scope of this result; we establish only the algorithmic framework and standard PCG convergence guarantee." Otherwise the reader may expect a bound that never comes.

---

```json
{
  "issues": [
    {
      "severity": "minor",
      "location": "Theorem statement, claim (2)",
      "reason": "The algorithm in the theorem uses $C_{i_t,:} \\mathrel{+}= \\alpha_t z_t$ where $z_t$ is a column vector being added to a row. The proof body correctly writes $z_t^\\top$ but the theorem statement is not corrected.",
      "required_fix": "Replace $C_{i_t,:} \\mathrel{+}= \\alpha_t z_t$ with $C_{i_t,:} \\mathrel{+}= \\alpha_t z_t^\\top$ in the theorem statement, or equivalently state the algorithm in column-vector form throughout.",
      "suggestion": "Since the prover controls the theorem restatement, add a corrected version of the theorem with consistent transpose conventions matching the opening vector-convention clarification."
    },
    {
      "severity": "major",
      "location": "Definitions / preamble",
      "reason": "The relationship $A_k = KW$ (RKHS parameterization of the mode-$k$ factor) is assumed without motivation. A reader unfamiliar with kernel-based tensor decomposition cannot understand why the linear system $Aw = b$ arises.",
      "required_fix": "Add 2-3 sentences explaining: $A_k$ is the mode-$k$ CP factor, the RKHS assumption means each column of $A_k$ lies in the RKHS of $K$, so $A_k = KW$ by the representer theorem, and $W$ contains the representer coefficients.",
      "suggestion": "Insert a short 'Problem Setup' paragraph before Part 1 that states the objective function and derives the normal equations, making the proof fully self-contained."
    },
    {
      "severity": "major",
      "location": "Preamble / before Part 1",
      "reason": "The linear system $Aw = b$ is the object the entire proof concerns, yet it is never derived from an optimization problem. The reader must accept it axiomatically.",
      "required_fix": "State the optimization objective (regularized least-squares on observed entries in the RKHS) and show the first-order condition yields $Aw = b$ with the given $A$ and $b$.",
      "suggestion": "A displayed 3-line derivation suffices: write the objective, differentiate with respect to $w$, set to zero, and identify $A$ and $b$. This closes the largest self-containedness gap."
    },
    {
      "severity": "major",
      "location": "Lemma 1 proof, masking-projector equivalence",
      "reason": "The equivalence $P_\\Omega \\mathrm{vec}(G) = \\mathrm{vec}(\\mathbf{1}_\\Omega \\odot G)$ is the algebraic linchpin of the matrix-free reformulation, yet it is asserted in a parenthetical remark rather than proved.",
      "required_fix": "Provide an explicit argument: define $S$ so that the $t$-th column is $e_{(i_t-1)M + m_t}$ (standard basis vector in $\\mathbb{R}^{nM}$), then $[SS^\\top u]_j = u_j$ if $j$ corresponds to some $(i_t, m_t) \\in \\Omega$, and $0$ otherwise, which is the entrywise masking by $\\mathbf{1}_\\Omega$ under the chosen vectorization.",
      "suggestion": "Promote this to a displayed sub-claim within Lemma 1 with a 2-3 line proof. Reference the distinct-observations assumption here to justify that $SS^\\top$ is a projector (no double-counting)."
    },
    {
      "severity": "minor",
      "location": "Lemma 3 / Gap Closure Note 1",
      "reason": "The mentor's $B = TZ$ is ambiguous — $T$ could mean the full unfolding or $T_{\\text{obs}}$. The clarification at the top says masked references to $T$ mean $T_{\\text{obs}}$, but $B = TZ$ is not syntactically a 'masked expression' (no $\\mathbf{1}_\\Omega$ appears).",
      "required_fix": "In Lemma 3 or the definitions section, explicitly redefine $B := T_{\\text{obs}} Z$ and note that this equals $TZ$ only when $T$ is understood as the zero-filled observation matrix.",
      "suggestion": "A single sentence at the point of definition suffices. The Gap Closure Note already addresses this but the in-line proof text should be unambiguous on its own."
    },
    {
      "severity": "minor",
      "location": "Part 5 and Lemma 6",
      "reason": "The norm equivalence $\\|e_m\\|_A = \\|\\hat{e}_m\\|_{\\hat{A}}$ is derived twice (in Part 5 and again in Lemma 6), creating redundancy and mild notational inconsistency.",
      "required_fix": "State and verify the norm equivalence once (in Lemma 6 is natural) and reference it from Part 5.",
      "suggestion": "In Part 5, write: 'Since $A \\succ 0$ and $P \\succ 0$, Lemma 6 gives...' and move the norm-equivalence calculation entirely into Lemma 6."
    },
    {
      "severity": "minor",
      "location": "Assumptions section",
      "reason": "Assumptions are listed in the mentor output but not restated in the proof. The proof references 'Assumption 1,' 'Assumption 2' etc. without an in-document anchor.",
      "required_fix": "Either restate the assumptions in a numbered list within the proof document, or add a clear forward-reference ('see Assumptions 1-5 in the Mentor section').",
      "suggestion": "For a self-contained proof, restate the assumptions in a dedicated section before Part 1. This is standard practice for graduate-level manuscripts."
    },
    {
      "severity": "minor",
      "location": "Theorem, claim (5) / Part 5",
      "reason": "The condition number $\\kappa$ is never bounded in terms of problem parameters. The theorem makes a convergence claim parameterized by $\\kappa$ but provides no guidance on when $\\kappa$ is small. Assumption 5 is qualitative only ('not pathologically imbalanced').",
      "required_fix": "Add an explicit remark that bounding $\\kappa$ in terms of $(q, n, r, \\lambda, \\delta, \\sigma(K), \\sigma(Z))$ is outside the scope of this result, and that the contribution is the matrix-free algorithmic framework with standard PCG guarantees.",
      "suggestion": "A one-sentence scope disclaimer after the convergence bound prevents misreading the theorem as providing an end-to-end complexity guarantee. Optionally, note that $\\kappa \\to 1$ as $\\delta \\to 0$ and $G_\\Omega \\to Z^\\top Z$ (full observation), giving intuition."
    }
  ],
  "residual_concerns": [
    "The proof never addresses numerical stability beyond the mentor's Risky Step 5. For a complete computational mathematics treatment, a remark on floating-point conditioning of the Cholesky factors and potential need for iterative refinement would strengthen the exposition.",
    "The notation $C_{i_t,:} \\mathrel{+}=$ uses imperative/algorithmic syntax inside a mathematical proof, which is common in computational math but may draw objections from pure-math reviewers. Consider framing the accumulation as a summation identity and presenting the algorithm separately."
  ]
}
```

---

### Summary

The proof is structurally sound and the core mathematical arguments are correct. The two **major** issues — the missing derivation of the linear system from an optimization objective, and the under-argued masking-projector equivalence — are the primary barriers to graduate-level self-containedness. The remaining issues are minor but collectively affect readability and precision. Addressing these would bring the document to a publishable standard for a computational mathematics venue.

#### Reference Validity (by gemini_api)

Based on the review of the **Reference Validity** perspective, here is the evaluation.

The proof is mathematically self-contained and rigorous. The prover explicitly derives the key operators and convergence bounds rather than relying solely on external "black boxes," which strengthens the validity. However, there is a minor precision issue regarding the citation for the PCG convergence theory, and a necessary clarification regarding the input data assumptions that should be formalized.

### Issues

```json
{
  "issues": [
    {
      "severity": "minor",
      "location": "Part 5 (Convergence) and Clarification (Saad PCG reference)",
      "reason": "Citation Imprecision. The prover cites 'Saad, §9.2.1' as the source for the convergence bound. In Saad (2nd Ed.), §9.2.1 covers the definition of Preconditioned CG (Left/Split), but the convergence bound itself is derived from the standard CG theory in Chapter 6 (e.g., Theorem 6.29) applied to the preconditioned spectrum.",
      "required_fix": "Update the citation to point to the correct theorem or section in Saad that establishes the convergence bound.",
      "suggestion": "Change the reference to 'Saad, Theorem 6.29 combined with Section 9.2' or simply 'Standard CG theory (e.g., Saad, Ch. 6)'."
    },
    {
      "severity": "minor",
      "location": "Definitions / Lemma 1 Proof",
      "reason": "Implicit Assumption Formalization. The proof relies on $P_\\Omega = SS^\\top$ being a projector, which requires $S^\\top S = I_q$. This holds only if the observed indices $(i_t, m_t)$ are distinct. While the prover notes this in the 'Clarification', it is a constraint on the input data $\\Omega$ that should be listed in the formal 'Assumptions' section of the theorem to ensure the operator $\\mathcal{A}(V)$ is well-defined.",
      "required_fix": "Move the 'Distinct observations' clarification into the formal Assumptions list.",
      "suggestion": "Add 'The observed index pairs in $\\Omega$ are distinct' to the Assumptions section."
    }
  ],
  "residual_concerns": []
}
```

### Rationale

1.  **Citation Precision:** While the derivation of the Chebyshev bound in Lemma 6 is correct and self-contained, the specific pointer to Saad §9.2.1 is slightly misleading for a reader looking for the *theorem* statement. Section 9.2.1 defines the algorithm; Chapter 6 contains the spectral analysis. Correcting this improves the reference validity.
2.  **Data Constraints:** The validity of the matrix-free operator $\mathcal{A}(V)$ hinges on $P_\Omega$ acting as a mask. If $\Omega$ contained duplicates (multiset), $P_\Omega$ would scale entries, and the identity $\mathbf{1}_\Omega \odot F$ would fail. The prover correctly identified this in the clarifications, but for a graduate-level formal statement, this data constraint must be an explicit hypothesis.

### Editor Decision

**Verdict: `right_track`**

**Summary:** The proof's core mathematical content is sound: the SPD argument, matrix-free matvec via sampled accumulation, sparse RHS, Kronecker-factored preconditioner, and Chebyshev-based PCG convergence bound are all correctly argued. However, five major and nine minor issues remain. The most significant gaps are (a) the linear system Aw=b is never derived from an optimization objective, leaving the reader unable to verify correctness of the system itself, (b) the SPD claim is only proved for K≻0 while the theorem's scope implicitly includes K⪰0, (c) the masking-projector equivalence is asserted without proof, and (d) formal definitions inconsistently use T vs T_obs. These are expositional and scoping issues, not structural flaws — the proof strategy is correct and all fixes are achievable within the current framework.

**Feedback (prover):** Priority fixes (must address for acceptance at graduate rigor):

1. **Derive the linear system from the objective (Issues 6, 7).** Add a 'Problem Setup' section before Part 1 that: (a) states the regularized least-squares objective on observed entries with RKHS constraint A_k = KW, (b) differentiates with respect to w = vec(W), (c) sets the gradient to zero to obtain Aw = b, identifying A and b explicitly. This is 5–8 lines of displayed math and closes the largest self-containedness gap.

2. **Restrict scope to K≻0 or prove the PSD case (Issue 1).** The simplest fix: explicitly restrict the theorem to K≻0 (or K + εI with ε > 0 absorbed into the model) and state that the PSD case requires a reduced-space formulation that is outside scope. Do NOT leave the PSD alternative as an informal remark — either prove it or exclude it from the theorem statement.

3. **Prove the masking-projector equivalence (Issue 8).** Promote to a displayed sub-claim within Lemma 1: define S so column t is e_{(i_t-1)M + m_t}, show [SS^T u]_j = u_j if j ∈ Ω and 0 otherwise, conclude this equals entrywise masking by 1_Ω under column-major vectorization. Reference the distinct-observations assumption. This is 3 lines.

4. **Unify T/T_obs notation (Issues 2, 9).** In the formal definitions (not just clarifications), define B := T_obs Z and b = vec(KB). Update Lemma 3's statement to reference T_obs explicitly. Remove the ambiguous B = TZ.

5. **Formalize the distinct-observations assumption (Issues 3, 14).** Move from 'Clarification' to a numbered assumption in a formal Assumptions section. State: 'The observed index pairs (i_t, m_t) for t=1,...,q are distinct.' Reference this assumption explicitly in Lemma 1 (projector) and Lemma 2 (unique accumulation).

Secondary fixes (important for polish):

6. **Fix transpose typo in theorem statement (Issue 5).** Change C_{i_t,:} += α_t z_t to C_{i_t,:} += α_t z_t^T.

7. **Add z_t generation cost to setup complexity (Issue 4).** State: 'If z_t must be generated on-the-fly, setup adds O(q(d-1)r) for G_Ω formation and B computation.'

8. **Remove convergence derivation redundancy (Issue 10).** State the norm equivalence once in Lemma 6; in Part 5, simply invoke Lemma 6.

9. **Restate assumptions in-document (Issue 11).** Add a numbered Assumptions section before Part 1.

10. **Add scope disclaimer for κ (Issue 12).** After the convergence bound, add: 'Bounding κ in terms of problem parameters (q, n, r, λ, δ, spectra of K and Z) is outside the scope of this result; the contribution is the matrix-free algorithmic framework with standard PCG guarantees.'

11. **Fix Saad citation (Issue 13).** Change to 'Saad, Iterative Methods for Sparse Linear Systems, Theorem 6.29 combined with §9.2 (preconditioned CG).'

## Final Status

Pipeline finished with verdict: `right_track`.
