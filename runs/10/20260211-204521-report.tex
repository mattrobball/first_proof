\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{fancyhdr}

\definecolor{red}{RGB}{204,0,0}
\definecolor{orange}{RGB}{204,102,0}
\definecolor{blue}{RGB}{0,102,204}
\definecolor{green}{RGB}{0,153,51}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Proof Pipeline Report}
\fancyhead[R]{Problem \texttt{10}}
\fancyfoot[C]{\thepage}

\title{Proof Pipeline Report\\[0.5em]\large Problem \texttt{10}}
\author{Multi-Agent Proof Pipeline}
\date{2026-02-11}

\begin{document}
\maketitle
\begin{abstract}
Automated proof analysis report generated by the multi-agent proof pipeline. The pipeline executed \textbf{5} loop(s) with \textbf{3} reviewer perspectives. Final verdict: \textcolor{orange}{\textbf{right_track}}.
\end{abstract}

\section*{Summary}
\begin{tabular}{ll}
\toprule
Problem ID & \texttt{10} \\
Started & \texttt{2026-02-11T20:45:21.899837+00:00} \\
Finished & \texttt{2026-02-11T21:24:19.400720+00:00} \\
Max loops & 5 \\
Executed loops & 5 \\
Rigor target & graduate \\
Reviewer perspectives & 3 \\
Final verdict & \textcolor{orange}{\textbf{right_track}} \\
\bottomrule
\end{tabular}

\bigskip
\noindent\textbf{Reviewer perspectives:}
\begin{enumerate}
\item \textbf{Correctness \& Completeness}: Focus on the validity of logical inferences, correct application of proof techniques, soundness of deductions, mathematical gaps, missing lemmas, unstated assumptions, and incomplete case analysis. Ensure every claim is justified and no steps are hand-waved.
\item \textbf{Clarity \& Rigor}: Focus on precision of notation, consistency of definitions, quality of exposition, and adherence to the stated rigor target. Ensure the proof would be acceptable for peer review at the specified rigor level.
\item \textbf{Reference Validity}: Verify all citations, referenced theorems, and external results. Ensure every invoked theorem or lemma is correctly stated, its hypotheses are satisfied in context, and no phantom references are used. Flag any appeal to results not established or cited.
\end{enumerate}

\section{Loop 1}
\noindent Editor verdict for this loop: \textcolor{orange}{\textbf{right_track}}

\subsection{Researcher}
\subsection{Relevant Theorems}
\begin{itemize}
\item **Representer theorem (RKHS regularization):** For empirical losses depending only on finitely many evaluations and strictly increasing norm penalty, minimizers lie in the span of kernel sections. This justifies \(A\_k = K W\) for the mode-\(k\) RKHS factor (Kimeldorf-Wahba, 1971; Schölkopf-Herbrich-Smola, 2001; RKHS foundation: Aronszajn, 1950).
\item **Kronecker/vec identity:** \((C\otimes A)\,\mathrm\{vec\}(B)=\mathrm\{vec\}(A B C\textasciicircum{}\top)\). This is the key to turning \((Z\otimes K)\) products into matrix operations on \(W\), without forming \(N\times nr\) matrices (Magnus-Neudecker; Golub-Van Loan).
\item **Khatri-Rao Gram identity:** \((\odot\_i A\_i)\textasciicircum{}\top(\odot\_i A\_i)=*\_\{i\}(A\_i\textasciicircum{}\top A\_i)\) (Hadamard product). Useful for small-\(r\) preconditioner construction (Kolda-Bader, 2009).
\item **PCG convergence theorem:** If \(A\) and preconditioner \(P\) are SPD, PCG error satisfies
\end{itemize}
  \[
  \|e\_m\|\_A \le 2\left(\frac\{\sqrt\{\kappa\}-1\}\{\sqrt\{\kappa\}+1\}\right)\textasciicircum{}m \|e\_0\|\_A,\quad
  \kappa=\kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\}).
  \]
  (Hestenes-Stiefel, 1952; Saad, \textit{Iterative Methods for Sparse Linear Systems}).
\begin{itemize}
\item **Spectral equivalence principle:** If \(c\_1 P\preceq A\preceq c\_2 P\), then \(\kappa(P\textasciicircum{}\{-1\}A)\le c\_2/c\_1\). This is the standard justification framework for preconditioner quality.

\end{itemize}
\subsection{Key Definitions}
\begin{itemize}
\item **Observed set and mask:** Let \(\Omega\subset[n]\times[M]\), \(|\Omega|=q\). \(S\in\mathbb\{R\}\textasciicircum{}\{N\times q\}\) is the selection matrix, \(P\_\Omega:=SS\textasciicircum{}\top\) is a diagonal projector onto observed entries in \(\mathrm\{vec\}(T)\).
\item **Unknown:** \(w=\mathrm\{vec\}(W)\in\mathbb\{R\}\textasciicircum{}\{nr\}\), with \(A\_k=KW\).
\item **Linear system operator:**
\end{itemize}
  \[
  A w := \left[(Z\otimes K)\textasciicircum{}\top P\_\Omega (Z\otimes K)+\lambda(I\_r\otimes K)\right]w.
  \]
\begin{itemize}
\item **Equivalent matrix form for \(V\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\):**
\end{itemize}
  \[
  \mathcal\{A\}(V)=K\Big((\Omega\odot(KVZ\textasciicircum{}\top))Z+\lambda V\Big),\quad
  A\,\mathrm\{vec\}(V)=\mathrm\{vec\}(\mathcal\{A\}(V)).
  \]
\begin{itemize}
\item **Sampled row features:** For observed pair \((i\_t,m\_t)\), define \(z\_t:=Z\_\{m\_t,:\}\in\mathbb\{R\}\textasciicircum{}r\). Then data Hessian term is \(\sum\_\{t=1\}\textasciicircum{}q (z\_t z\_t\textasciicircum{}\top)\otimes(k\_\{i\_t\}k\_\{i\_t\}\textasciicircum{}\top)\), \(k\_\{i\_t\}\textasciicircum{}\top\) = row \(i\_t\) of \(K\).

\end{itemize}
\subsection{Proof Strategies}
\begin{enumerate}
\item **Use matrix-free PCG on \(A w=b\)** with \(b=(I\_r\otimes K)\mathrm\{vec\}(B)\), \(B=TZ\). Never form \(A\), \(Z\otimes K\), or any \(N\)-scale object.
\item **Matvec implementation (core step):**
\end{enumerate}
\begin{itemize}
\item Compute \(U=KV\) (\(O(n\textasciicircum{}2r)\)).
\item Initialize \(C=0\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\).
\item For each observed entry \(t=1,\dots,q\):
\item \(\alpha\_t=\langle U\_\{i\_t,:\}, z\_t\rangle\) (\(O(r)\)).
\item \(C\_\{i\_t,:\}\mathrel\{+\}= \alpha\_t z\_t\) (\(O(r)\)).
\item Output \(Y=K C+\lambda U\) (\(O(n\textasciicircum{}2r)\)).
\item Then \(\mathrm\{vec\}(Y)=A\,\mathrm\{vec\}(V)\).
\item Total per matvec: \(O(qr+n\textasciicircum{}2r)\) (plus \(O(q(d-1)r)\) if each \(z\_t\) is generated on-the-fly from CP factors).
\end{itemize}
\begin{enumerate}
\item **RHS computation without \(N\):**
\end{enumerate}
\begin{itemize}
\item Sparse MTTKRP: \(B\_\{i\_t,:\}\mathrel\{+\}=T\_\{i\_t,m\_t\} z\_t\) over \(q\) observations (\(O(qr)\)).
\item Then \(b=\mathrm\{vec\}(KB)\) (\(O(n\textasciicircum{}2r)\)).
\end{itemize}
\begin{enumerate}
\item **Preconditioner choice (practical strong option):**
\end{enumerate}
\begin{itemize}
\item \(P=(G\_\Omega+\lambda I\_r)\otimes (K+\delta I\_n)\), \(\delta>0\) nugget for robustness.
\item \(G\_\Omega=\sum\_\{t=1\}\textasciicircum{}q z\_t z\_t\textasciicircum{}\top\) (\(O(qr\textasciicircum{}2)\)).
\item Apply \(P\textasciicircum{}\{-1\}\) to residual \(R\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\) via two small solves:
\item Solve \((K+\delta I)Y=R\),
\item Solve \(X(G\_\Omega+\lambda I)=Y\),
\item return \(\mathrm\{vec\}(X)\).
\item Precompute Cholesky factors: \(O(n\textasciicircum{}3+r\textasciicircum{}3)\). Per iteration preconditioner cost: \(O(n\textasciicircum{}2r+nr\textasciicircum{}2)\).
\end{itemize}
\begin{enumerate}
\item **Complexity bound:**
\end{enumerate}
\begin{itemize}
\item Per PCG iteration: \(O(qr+n\textasciicircum{}2r+nr\textasciicircum{}2)\), dominated by \(O(qr)\) when \(q\gg n,r\).
\item Total: \(O\!\left((qr+n\textasciicircum{}2r+nr\textasciicircum{}2)\,m + qr\textasciicircum{}2 + n\textasciicircum{}3 + r\textasciicircum{}3\right)\), with \(m=O(\sqrt\{\kappa\}\log(1/\varepsilon))\).
\item No \(O(N)\) operations.

\end{itemize}
\subsection{Gaps and Concerns}
\begin{itemize}
\item **SPD condition is not automatic:** If \(K\) is only PSD, \(A\) can be singular (nullspace propagates through both terms). For strict PCG guarantees, assume \(K\succ0\) or use \(K+\delta I\), \(\delta>0\), or solve in \(\mathrm\{range\}(K)\).
\item **Need \(\lambda>0\):** Without regularization, ill-posedness/slow convergence can be severe with missing data.
\item **Preconditioner quality depends on sampling structure:** Spectral-equivalence claims need assumptions (roughly balanced/mixing observations across mode-\(k\) indices and rank directions).
\item **Do not materialize full \(Z\) when \(M\) is huge:** Use only sampled rows \(z\_t\) (computed or cached from observed multi-indices).
\item **Index-order consistency is critical:** The mapping between tensor indices, mode-\(k\) unfolding, and \(\mathrm\{vec\}\) order must match \(S\), otherwise the operator is wrong.
\item **Numerical monitoring:** Use relative preconditioned residual and max-iteration safeguards; with extreme missingness, stagnation is possible even with PCG.
\end{itemize}

\subsection{Mentor}
\subsection{Definitions}
Let \(k\) be the target mode, \(n:=n\_k\), \(M:=\prod\_\{i\neq k\} n\_i\), \(N=nM\), and rank \(r\).

Let \(\Omega\subset [n]\times[M]\) be the observed index set in mode-\(k\) unfolding, \(|\Omega|=q\), with observed triples \((i\_t,m\_t,y\_t)\), \(t=1,\dots,q\), where \(y\_t=T\_\{i\_t,m\_t\}\).

Define
\[
Z:=A\_d\odot\cdots\odot A\_\{k+1\}\odot A\_\{k-1\}\odot\cdots\odot A\_1\in\mathbb\{R\}\textasciicircum{}\{M\times r\},
\quad
z\_t:=Z\_\{m\_t,:\}\in\mathbb\{R\}\textasciicircum{}r.
\]
Unknown is \(W\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\), \(w:=\mathrm\{vec\}(W)\), and \(A\_k=KW\).

Define \(P\_\Omega:=SS\textasciicircum{}\top\) (projector selecting observed entries in \(\mathrm\{vec\}(T)\)) and linear system
\[
A w=b,\quad
A=(Z\otimes K)\textasciicircum{}\top P\_\Omega (Z\otimes K)+\lambda(I\_r\otimes K),\quad
b=(I\_r\otimes K)\mathrm\{vec\}(B),\ B=TZ.
\]

Define mask matrix \(\mathbf\{1\}\_\Omega\in\\{0,1\\}\textasciicircum{}\{n\times M\}\). For \(V\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\),
\[
\mathcal A(V):=
K\!\left((\mathbf\{1\}\_\Omega\odot (KVZ\textasciicircum{}\top))Z+\lambda V\right),
\quad
A\,\mathrm\{vec\}(V)=\mathrm\{vec\}(\mathcal A(V)).
\]

\subsection{Formal Statement}
\textbf{Theorem (Matrix-free PCG for RKHS mode-\(k\) subproblem).} Under the assumptions in Section \textbf{Assumptions}:

\begin{enumerate}
\item \(A\) is SPD, so PCG is well-defined.
\item For any \(V\), \(A\mathrm\{vec\}(V)\) can be computed exactly without forming \(A\), \(Z\otimes K\), or any \(N\)-scale object via:
\end{enumerate}
\[
U=KV,\ \ 
\alpha\_t=\langle U\_\{i\_t,:\},z\_t\rangle,\ \ 
C\_\{i\_t,:\}\mathrel\{+\}=\alpha\_t z\_t,\ \ 
Y=KC+\lambda U,
\]
return \(\mathrm\{vec\}(Y)\).
\begin{enumerate}
\item The per-matvec complexity is \(O(qr+n\textasciicircum{}2r)\) (plus \(O(q(d-1)r)\) if each \(z\_t\) is generated on-the-fly from CP factors).
\item With preconditioner
\end{enumerate}
\[
P=((G\_\Omega+\lambda I\_r)\otimes (K+\delta I\_n)),\quad
G\_\Omega:=\sum\_\{t=1\}\textasciicircum{}q z\_tz\_t\textasciicircum{}\top,\ \delta>0,
\]
each \(P\textasciicircum{}\{-1\}\)-apply costs \(O(n\textasciicircum{}2r+nr\textasciicircum{}2)\) after setup \(O(qr\textasciicircum{}2+n\textasciicircum{}3+r\textasciicircum{}3)\).
\begin{enumerate}
\item PCG iterates satisfy
\end{enumerate}
\[
\|e\_m\|\_A\le 2\Big(\frac\{\sqrt\kappa-1\}\{\sqrt\kappa+1\}\Big)\textasciicircum{}m\|e\_0\|\_A,\quad
\kappa=\kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\}),
\]
hence total cost
\[
O\!\left(m(qr+n\textasciicircum{}2r+nr\textasciicircum{}2)+qr\textasciicircum{}2+n\textasciicircum{}3+r\textasciicircum{}3\right),
\]
with no \(O(N)\) operation.

\subsection{Assumptions}
\begin{enumerate}
\item \(\lambda>0\).
\item \(K\succ 0\) for strict SPD proof of \(A\); if original kernel is only PSD, either add nugget to the model kernel or solve on \(\mathrm\{range\}(K)\).
\item \(n,r<q\ll N\), and observed tuples \((i\_t,m\_t,y\_t)\) are directly available.
\item Indexing conventions for unfolding, vectorization, and \(S\) are consistent.
\item For preconditioner quality claims (small \(\kappa\)), observation pattern is not pathologically imbalanced across \(i\_t\) and rank directions.

\end{enumerate}
\subsection{Notation}
\begin{itemize}
\item \(n=n\_k\): size of mode \(k\).
\item \(M=\prod\_\{i\neq k\} n\_i\): product of non-\(k\) dimensions.
\item \(N=nM\): total entries.
\item \(q=|\Omega|\): number of observed entries.
\item \(Z\in\mathbb\{R\}\textasciicircum{}\{M\times r\}\): Khatri-Rao product of fixed factors.
\item \(K\in\mathbb\{R\}\textasciicircum{}\{n\times n\}\): kernel matrix.
\item \(W\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\), \(w=\mathrm\{vec\}(W)\): unknown.
\item \(B=TZ\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\): sparse MTTKRP result.
\item \(z\_t=Z\_\{m\_t,:\}\): sampled Khatri-Rao row for observation \(t\).
\item \(P\_\Omega=SS\textasciicircum{}\top\): selection projector in vectorized unfolding space.
\item \(G\_\Omega=\sum\_t z\_tz\_t\textasciicircum{}\top\in\mathbb\{R\}\textasciicircum{}\{r\times r\}\): sampled rank-Gram.

\end{itemize}
\subsection{High-Level Strategy}
\begin{enumerate}
\item Rewrite the Hessian action as a matrix operator \(\mathcal A(V)\) using \((C\otimes A)\mathrm\{vec\}(B)=\mathrm\{vec\}(AB C\textasciicircum{}\top)\).
\item Implement \(\mathcal A(V)\) from observed entries only, via rowwise accumulation \(C\_\{i\_t,:\} \!+\!= \alpha\_t z\_t\), never forming \(P\_\Omega\), \(Z\otimes K\), or dense \(T\).
\item Compute RHS sparsely: \(B\_\{i\_t,:\}\!+\!=y\_t z\_t\), then \(b=\mathrm\{vec\}(KB)\).
\item Run PCG on \(Aw=b\) with Kronecker preconditioner \(P\) that is cheap to invert by two small Cholesky-based solves.
\item Prove complexity by summing matvec cost, preconditioner-apply cost, and one-time setup.

\end{enumerate}
\subsection{Key Lemmas}
\begin{enumerate}
\item **Kronecker-vec masked operator lemma:** \(A\,\mathrm\{vec\}(V)=\mathrm\{vec\}(\mathcal A(V))\).
\item **Sampled accumulation lemma:** \(C=(\mathbf\{1\}\_\Omega\odot(KVZ\textasciicircum{}\top))Z\) is computed exactly by the \(q\)-sample loop in \(O(qr)\).
\item **Sparse RHS lemma:** \(B=TZ\) is computed from observations in \(O(qr)\), then \(b\) in \(O(n\textasciicircum{}2r)\).
\item **SPD lemma:** if \(K\succ0,\lambda>0\), then \(A\succ0\) because
\end{enumerate}
\[
x\textasciicircum{}\top A x=\|P\_\Omega\textasciicircum{}\{1/2\}(Z\otimes K)x\|\_2\textasciicircum{}2+\lambda\|(I\_r\otimes K\textasciicircum{}\{1/2\})x\|\_2\textasciicircum{}2>0.
\]
\begin{enumerate}
\item **Preconditioner-apply lemma:** \(P\textasciicircum{}\{-1\}\mathrm\{vec\}(R)\) equals two solves:
\end{enumerate}
\[
(K+\delta I)Y=R,\quad X(G\_\Omega+\lambda I)=Y,\quad \mathrm\{vec\}(X)=P\textasciicircum{}\{-1\}\mathrm\{vec\}(R).
\]
\begin{enumerate}
\item **PCG convergence lemma:** standard SPD PCG bound with \(\kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\})\).

\end{enumerate}
\subsection{Dependency Graph}
\begin{enumerate}
\item Definitions + Notation \(\rightarrow\) Lemma 1.
\item Lemma 1 \(\rightarrow\) Lemma 2 (matvec algorithm correctness).
\item Definitions \(\rightarrow\) Lemma 3 (RHS correctness/cost).
\item Assumptions 1-2 \(\rightarrow\) Lemma 4 (SPD of \(A\)).
\item Definitions \(\rightarrow\) Lemma 5 (preconditioner form and apply rule).
\item Lemma 4 + Lemma 5 + Lemma 6 \(\rightarrow\) PCG convergence claim.
\item Lemma 2 + Lemma 3 + Lemma 5 \(\rightarrow\) total complexity claim in theorem.

\end{enumerate}
\subsection{Risky Steps}
\begin{enumerate}
\item **PSD kernel issue:** if \(K\) is singular, \(A\) may be singular despite \(\lambda>0\); this must be handled explicitly.
\item **Index consistency:** mismatch between unfolding order, \(S\), and sampled \((i\_t,m\_t)\) invalidates the operator.
\item **Preconditioner quality claim:** spectral-equivalence constants require sampling-balance assumptions; do not claim uniformly good \(\kappa\) without them.
\item **Hidden \(M\)-scale cost:** forming full \(Z\) is infeasible when \(M\) is huge; use only sampled rows \(z\_t\) (cached or on-demand).
\item **Numerical stability:** very small \(\lambda\) or ill-conditioned \(K\) can cause PCG stagnation; monitor relative preconditioned residual and cap iterations.
\end{enumerate}

\subsection{Proof}
\subsection{Complete Proof}

We prove the theorem in five parts, corresponding to claims (1)–(5) of the formal statement. Throughout, we use the notation and assumptions established by the mentor.

\textbf{Part 1: \(A\) is symmetric positive definite.}

Recall
\[
A = (Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K) + \lambda (I\_r \otimes K).
\]

Since \(P\_\Omega = S S\textasciicircum{}\top\) is a projection matrix (hence PSD), the first term \((Z \otimes K)\textasciicircum{}\top S S\textasciicircum{}\top (Z \otimes K)\) is PSD for any \(Z, K\). The second term \(\lambda (I\_r \otimes K)\) is SPD when \(\lambda > 0\) and \(K \succ 0\), since eigenvalues of \(I\_r \otimes K\) are exactly the eigenvalues of \(K\) each with multiplicity \(r\), all positive. Thus \(A\) is the sum of a PSD matrix and an SPD matrix, hence SPD. In particular, for any \(x \neq 0\):
\[
x\textasciicircum{}\top A x = \|S\textasciicircum{}\top (Z \otimes K) x\|\_2\textasciicircum{}2 + \lambda \, x\textasciicircum{}\top (I\_r \otimes K) x > 0,
\]
where the strict inequality follows because \(x\textasciicircum{}\top (I\_r \otimes K) x > 0\) for all \(x \neq 0\) when \(K \succ 0\). Since \(A\) is SPD, the PCG iteration is well-defined and convergent.

\textbf{Part 2: Matrix-free matvec via the operator \(\mathcal\{A\}(V)\).}

We must show that \(A \operatorname\{vec\}(V) = \operatorname\{vec\}(\mathcal\{A\}(V))\) where
\[
\mathcal\{A\}(V) = K\!\left((\mathbf\{1\}\_\Omega \odot (K V Z\textasciicircum{}\top)) Z + \lambda V\right),
\]
and that this is implemented without forming any \(N\)-scale object by the four-step procedure:
\[
U = KV, \quad \alpha\_t = \langle U\_\{i\_t,:\}, z\_t \rangle, \quad C\_\{i\_t,:\} \mathrel\{+\}= \alpha\_t \, z\_t, \quad Y = KC + \lambda U.
\]

The proof proceeds by expanding \(A \operatorname\{vec\}(V)\) and applying the Kronecker-vec identity. See Lemma 1 below for the algebraic verification, and Lemma 2 for the equivalence of the sampled accumulation loop to the masked matrix product.

\textbf{Part 3: Per-matvec complexity is \(O(qr + n\textasciicircum{}2 r)\).}

Each application of \(\mathcal\{A\}\) involves:
\begin{itemize}
\item Computing \(U = KV\): matrix-matrix multiply \(K \in \mathbb\{R\}\textasciicircum{}\{n \times n\}\) times \(V \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\), cost \(O(n\textasciicircum{}2 r)\).
\item The accumulation loop over \(t = 1, \ldots, q\): computing \(\alpha\_t = \langle U\_\{i\_t,:\}, z\_t \rangle\) costs \(O(r)\) per observation, and updating \(C\_\{i\_t,:\} \mathrel\{+\}= \alpha\_t z\_t\) costs \(O(r)\) per observation, for a total of \(O(qr)\). (Initialize \(C = 0 \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\) in \(O(nr)\).)
\item Computing \(Y = KC + \lambda U\): one more \(n \times n\) by \(n \times r\) multiply plus an elementwise scaled addition, cost \(O(n\textasciicircum{}2 r)\).

\end{itemize}
Total: \(O(n\textasciicircum{}2 r + qr + n\textasciicircum{}2 r) = O(qr + n\textasciicircum{}2 r)\), since \(nr \leq qr\) by assumption \(n < q\).

If the rows \(z\_t\) are not precomputed but generated on-the-fly from the \(d-1\) CP factor matrices via elementwise products \(z\_t = A\_1(j\_1\textasciicircum{}\{(t)\},:) \odot A\_2(j\_2\textasciicircum{}\{(t)\},:) \odot \cdots\) (omitting factor \(k\)), each \(z\_t\) costs \(O((d-1)r)\), adding \(O(q(d-1)r)\) to the total.

\textbf{Part 4: Preconditioner application cost.}

The Kronecker preconditioner is
\[
P = (G\_\Omega + \lambda I\_r) \otimes (K + \delta I\_n),
\]
where \(G\_\Omega = \sum\_\{t=1\}\textasciicircum{}q z\_t z\_t\textasciicircum{}\top \in \mathbb\{R\}\textasciicircum{}\{r \times r\}\) and \(\delta > 0\).

\textbf{Setup (one-time):}
\begin{itemize}
\item Form \(G\_\Omega\): accumulate \(q\) rank-1 updates \(z\_t z\_t\textasciicircum{}\top\), each costing \(O(r\textasciicircum{}2)\), total \(O(qr\textasciicircum{}2)\).
\item Cholesky factor \(G\_\Omega + \lambda I\_r \in \mathbb\{R\}\textasciicircum{}\{r \times r\}\): cost \(O(r\textasciicircum{}3)\).
\item Cholesky factor \(K + \delta I\_n \in \mathbb\{R\}\textasciicircum{}\{n \times n\}\): cost \(O(n\textasciicircum{}3)\).

\end{itemize}
Total setup: \(O(qr\textasciicircum{}2 + n\textasciicircum{}3 + r\textasciicircum{}3)\).

\textbf{Per-apply:} Given residual \(R \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\), compute \(X\) such that \(\operatorname\{vec\}(X) = P\textasciicircum{}\{-1\} \operatorname\{vec\}(R)\). By the Kronecker inverse identity \(P\textasciicircum{}\{-1\} = (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\} \otimes (K + \delta I\_n)\textasciicircum{}\{-1\}\), we have (see Lemma 5):
\[
(K + \delta I\_n) \, Y\_\{\cdot, j\} = R\_\{\cdot, j\} \quad \text\{for \} j = 1, \ldots, r,
\]
then
\[
(G\_\Omega + \lambda I\_r) \, X\_\{i, \cdot\}\textasciicircum{}\top = Y\_\{i, \cdot\}\textasciicircum{}\top \quad \text\{for \} i = 1, \ldots, n.
\]

The first step solves \(r\) systems of size \(n\) using the precomputed Cholesky factor: \(O(n\textasciicircum{}2 r)\) (forward-backward substitution costs \(O(n\textasciicircum{}2)\) per right-hand side). The second step solves \(n\) systems of size \(r\): \(O(n r\textasciicircum{}2)\). Total per-apply: \(O(n\textasciicircum{}2 r + n r\textasciicircum{}2)\).

\textbf{Part 5: PCG convergence and total complexity.}

Since \(A \succ 0\) (Part 1) and \(P \succ 0\) (both Kronecker factors are SPD: \(K + \delta I \succ 0\) trivially, and \(G\_\Omega + \lambda I \succ 0\) since \(\lambda > 0\)), the preconditioned system \(P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\}\) is SPD with condition number \(\kappa = \kappa(P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\})\).

By the standard PCG convergence theorem (see Lemma 6), the \(A\)-norm error satisfies:
\[
\|e\_m\|\_A \leq 2 \left(\frac\{\sqrt\{\kappa\} - 1\}\{\sqrt\{\kappa\} + 1\}\right)\textasciicircum{}m \|e\_0\|\_A.
\]

To achieve \(\|e\_m\|\_A / \|e\_0\|\_A \leq \epsilon\), it suffices to take \(m = O(\sqrt\{\kappa\} \log(1/\epsilon))\) iterations.

Each PCG iteration requires one matvec (\(O(qr + n\textasciicircum{}2 r)\) by Part 3) and one preconditioner apply (\(O(n\textasciicircum{}2 r + nr\textasciicircum{}2)\) by Part 4). The total cost for \(m\) iterations is therefore:
\[
O\!\left(m(qr + n\textasciicircum{}2 r + n r\textasciicircum{}2) + qr\textasciicircum{}2 + n\textasciicircum{}3 + r\textasciicircum{}3\right),
\]
where the second group is the one-time preconditioner setup. Since \(n, r < q \ll N\), no operation scales as \(O(N)\). \(\blacksquare\)

---

\subsection{Lemma Proofs}

\subsubsection{Lemma 1: Kronecker-vec masked operator identity}

\textbf{Statement.} For any \(V \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\),
\[
A \operatorname\{vec\}(V) = \operatorname\{vec\}\!\left(K\big((\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top))Z + \lambda V\big)\right).
\]

\textbf{Proof.} We expand each term in \(A \operatorname\{vec\}(V) = \big[(Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K) + \lambda (I\_r \otimes K)\big] \operatorname\{vec\}(V)\).

\textit{Regularization term.} By the standard Kronecker-vec identity \((C \otimes A) \operatorname\{vec\}(B) = \operatorname\{vec\}(ABC\textasciicircum{}\top)\):
\[
\lambda (I\_r \otimes K) \operatorname\{vec\}(V) = \lambda \operatorname\{vec\}(K V I\_r\textasciicircum{}\top) = \lambda \operatorname\{vec\}(KV).
\]

\textit{Data-fit term.} First, \((Z \otimes K) \operatorname\{vec\}(V) = \operatorname\{vec\}(KVZ\textasciicircum{}\top)\). This is a vector in \(\mathbb\{R\}\textasciicircum{}\{nM\}\), which we can identify with the matrix \(KVZ\textasciicircum{}\top \in \mathbb\{R\}\textasciicircum{}\{n \times M\}\).

The projector \(P\_\Omega\) acts on vectorized \(n \times M\) matrices by zeroing out entries not in \(\Omega\). In matrix form, \(P\_\Omega \operatorname\{vec\}(X) = \operatorname\{vec\}(\mathbf\{1\}\_\Omega \odot X)\) where \(\mathbf\{1\}\_\Omega\) is the binary mask and \(\odot\) is the Hadamard (entrywise) product. To verify: if \((i, m) \in \Omega\), the corresponding entry of \(\operatorname\{vec\}(X)\) is kept; otherwise it is zeroed. This is exactly what the Hadamard product with \(\mathbf\{1\}\_\Omega\) does.

So:
\[
P\_\Omega (Z \otimes K) \operatorname\{vec\}(V) = P\_\Omega \operatorname\{vec\}(KVZ\textasciicircum{}\top) = \operatorname\{vec\}(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top)).
\]

Now apply \((Z \otimes K)\textasciicircum{}\top\). Using the identity \((Z \otimes K)\textasciicircum{}\top \operatorname\{vec\}(X) = (Z\textasciicircum{}\top \otimes K\textasciicircum{}\top) \operatorname\{vec\}(X) = \operatorname\{vec\}(K\textasciicircum{}\top X Z) = \operatorname\{vec\}(KXZ)\) (since \(K\) is symmetric, \(K\textasciicircum{}\top = K\)):
\[
(Z \otimes K)\textasciicircum{}\top \operatorname\{vec\}(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top)) = \operatorname\{vec\}\!\left(K \big(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top)\big) Z\right).
\]

Combining:
\[
A \operatorname\{vec\}(V) = \operatorname\{vec\}\!\left(K(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top))Z\right) + \lambda \operatorname\{vec\}(KV) = \operatorname\{vec\}\!\left(K\big[(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top))Z + \lambda V\big]\right).
\]

This equals \(\operatorname\{vec\}(\mathcal\{A\}(V))\). \(\square\)

\subsubsection{Lemma 2: Sampled accumulation correctness}

\textbf{Statement.} The matrix \(C := (\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top)) Z \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\) is computed exactly by:
\begin{enumerate}
\item Set \(U = KV\), \(C = 0\).
\item For \(t = 1, \ldots, q\): compute \(\alpha\_t = \langle U\_\{i\_t,:\}, z\_t \rangle\) and update \(C\_\{i\_t,:\} \mathrel\{+\}= \alpha\_t z\_t\).

\end{enumerate}
The cost is \(O(qr)\) (excluding the \(O(n\textasciicircum{}2 r)\) cost of forming \(U\)).

\textbf{Proof.} We must show that the loop produces the same matrix \(C\) as the formula \((\mathbf\{1\}\_\Omega \odot (UZ\textasciicircum{}\top))Z\), where \(U = KV\).

Consider the \((i, m)\)-entry of \(UZ\textasciicircum{}\top\):
\[
(UZ\textasciicircum{}\top)\_\{i,m\} = \sum\_\{s=1\}\textasciicircum{}r U\_\{i,s\} Z\_\{m,s\} = \langle U\_\{i,:\}, Z\_\{m,:\} \rangle.
\]

After masking, \((\mathbf\{1\}\_\Omega \odot (UZ\textasciicircum{}\top))\_\{i,m\} = \langle U\_\{i,:\}, Z\_\{m,:\} \rangle\) if \((i, m) \in \Omega\) and \(0\) otherwise.

The \(i\)-th row of \(C = (\mathbf\{1\}\_\Omega \odot (UZ\textasciicircum{}\top))Z\) is:
\[
C\_\{i,:\} = \sum\_\{m=1\}\textasciicircum{}M (\mathbf\{1\}\_\Omega)\_\{i,m\} \langle U\_\{i,:\}, Z\_\{m,:\} \rangle \, Z\_\{m,:\} = \sum\_\{\\{t : i\_t = i\\}\} \langle U\_\{i,:\}, z\_t \rangle \, z\_t = \sum\_\{\\{t : i\_t = i\\}\} \alpha\_t \, z\_t,
\]
where the second equality uses the fact that the nonzero contributions come precisely from the observations \((i\_t, m\_t) \in \Omega\) with \(i\_t = i\), and \(Z\_\{m\_t,:\} = z\_t\).

This is exactly what the accumulation loop computes: for each \(t\), it adds \(\alpha\_t z\_t\) to row \(i\_t\) of \(C\). The order of summation is immaterial since addition is commutative and associative (in exact arithmetic).

Each of the \(q\) iterations performs an inner product (\(O(r)\)) and a scaled vector addition (\(O(r)\)), giving total cost \(O(qr)\). \(\square\)

\subsubsection{Lemma 3: Sparse RHS computation}

\textbf{Statement.} The right-hand side \(b = \operatorname\{vec\}(KB)\) where \(B = T\_\{(k)\} Z \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\) (with \(T\_\{(k)\}\) the mode-\(k\) unfolding) can be computed from the observed entries alone in \(O(qr)\), and then \(b = \operatorname\{vec\}(KB)\) costs \(O(n\textasciicircum{}2 r)\).

\textbf{Proof.} The \((i, s)\)-entry of \(B = T\_\{(k)\} Z\) is:
\[
B\_\{i,s\} = \sum\_\{m=1\}\textasciicircum{}M (T\_\{(k)\})\_\{i,m\} Z\_\{m,s\}.
\]

Since only entries \((i\_t, m\_t) \in \Omega\) of \(T\_\{(k)\}\) are observed (with values \(y\_t\)), and unobserved entries do not contribute to the reconstruction problem (they are not part of the data-fit term), the sparse MTTKRP computes:
\[
B\_\{i,s\} = \sum\_\{\\{t : i\_t = i\\}\} y\_t \, (z\_t)\_s.
\]

This is implemented by initializing \(B = 0\) and, for each \(t = 1, \ldots, q\), updating \(B\_\{i\_t,:\} \mathrel\{+\}= y\_t z\_t\). Each update costs \(O(r)\), so the total is \(O(qr)\).

Then \(b = \operatorname\{vec\}(KB)\) requires the matrix product \(KB\) (\(n \times n\) times \(n \times r\)), costing \(O(n\textasciicircum{}2 r)\). \(\square\)

\textbf{Remark on RHS consistency.} We verify that this \(b\) matches the definition \(b = (I\_r \otimes K) \operatorname\{vec\}(B)\). Indeed, \((I\_r \otimes K) \operatorname\{vec\}(B) = \operatorname\{vec\}(K B I\_r\textasciicircum{}\top) = \operatorname\{vec\}(KB)\), consistent with the mentor's definition.

\subsubsection{Lemma 4: SPD property of \(A\)}

\textbf{Statement.} Under Assumptions 1–2 (\(\lambda > 0\), \(K \succ 0\)), \(A \succ 0\).

\textbf{Proof.} For any nonzero \(x \in \mathbb\{R\}\textasciicircum{}\{nr\}\):
\[
x\textasciicircum{}\top A x = x\textasciicircum{}\top (Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K) x + \lambda \, x\textasciicircum{}\top (I\_r \otimes K) x.
\]

The first term is \(\|S\textasciicircum{}\top (Z \otimes K) x\|\_2\textasciicircum{}2 \geq 0\).

For the second term, let \(V \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\) be the matricization of \(x\), so \(x = \operatorname\{vec\}(V)\). Then:
\[
x\textasciicircum{}\top (I\_r \otimes K) x = \operatorname\{vec\}(V)\textasciicircum{}\top (I\_r \otimes K) \operatorname\{vec\}(V) = \operatorname\{vec\}(V)\textasciicircum{}\top \operatorname\{vec\}(KV) = \operatorname\{tr\}(V\textasciicircum{}\top K V).
\]

Since \(K \succ 0\), we can write \(K = K\textasciicircum{}\{1/2\} K\textasciicircum{}\{1/2\}\) (symmetric positive definite square root), so:
\[
\operatorname\{tr\}(V\textasciicircum{}\top K V) = \operatorname\{tr\}(V\textasciicircum{}\top K\textasciicircum{}\{1/2\} K\textasciicircum{}\{1/2\} V) = \|K\textasciicircum{}\{1/2\} V\|\_F\textasciicircum{}2.
\]

If \(x \neq 0\) then \(V \neq 0\), and since \(K\textasciicircum{}\{1/2\}\) is nonsingular, \(K\textasciicircum{}\{1/2\} V \neq 0\), so \(\|K\textasciicircum{}\{1/2\} V\|\_F\textasciicircum{}2 > 0\). Therefore:
\[
x\textasciicircum{}\top A x \geq 0 + \lambda \|K\textasciicircum{}\{1/2\} V\|\_F\textasciicircum{}2 > 0.
\]

Hence \(A \succ 0\). \(\square\)

\textbf{Remark on the PSD case.} If \(K\) is only PSD (i.e., has a nontrivial nullspace), then \(I\_r \otimes K\) is only PSD, and the regularization term \(\lambda x\textasciicircum{}\top (I\_r \otimes K) x\) can be zero for \(x \in \ker(I\_r \otimes K) = \mathbb\{R\}\textasciicircum{}r \otimes \ker(K)\). In this case, \(A\) is SPD if and only if \((Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K)\) is positive definite on \(\ker(I\_r \otimes K)\), which is not guaranteed in general. The remedy is either: (a) add a nugget \(\epsilon I\_n\) to \(K\) so that \(K + \epsilon I \succ 0\), which is standard in kernel methods; or (b) restrict the problem to \(\operatorname\{range\}(K)\) by projecting out the nullspace. Both approaches are noted in Assumption 2.

\subsubsection{Lemma 5: Preconditioner apply via two triangular solves}

\textbf{Statement.} With \(P = (G\_\Omega + \lambda I\_r) \otimes (K + \delta I\_n)\), the action \(\operatorname\{vec\}(X) = P\textasciicircum{}\{-1\} \operatorname\{vec\}(R)\) for \(R \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\) is computed by:
\begin{enumerate}
\item Solve \((K + \delta I\_n) Y = R\) (i.e., solve for each column of \(Y\)).
\item Solve \(X (G\_\Omega + \lambda I\_r) = Y\) (i.e., solve for each row of \(X\)).

\end{enumerate}
The cost is \(O(n\textasciicircum{}2 r + nr\textasciicircum{}2)\) per apply after one-time Cholesky factorizations costing \(O(n\textasciicircum{}3 + r\textasciicircum{}3)\).

\textbf{Proof.} By the Kronecker product inverse identity, for invertible \(A, B\):
\[
(A \otimes B)\textasciicircum{}\{-1\} = A\textasciicircum{}\{-1\} \otimes B\textasciicircum{}\{-1\}.
\]

Applied here:
\[
P\textasciicircum{}\{-1\} = (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\} \otimes (K + \delta I\_n)\textasciicircum{}\{-1\}.
\]

Both factors are SPD (\(G\_\Omega \succeq 0\) and \(\lambda > 0\) give \(G\_\Omega + \lambda I\_r \succ 0\); \(K \succ 0\) by assumption and \(\delta > 0\) gives \(K + \delta I\_n \succ 0\)), hence invertible.

Using the Kronecker-vec identity:
\[
P\textasciicircum{}\{-1\} \operatorname\{vec\}(R) = \big((G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\} \otimes (K + \delta I\_n)\textasciicircum{}\{-1\}\big) \operatorname\{vec\}(R) = \operatorname\{vec\}\!\left((K + \delta I\_n)\textasciicircum{}\{-1\} R \, (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-\top\}\right).
\]

Since \(G\_\Omega + \lambda I\_r\) is symmetric, \((G\_\Omega + \lambda I\_r)\textasciicircum{}\{-\top\} = (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\}\). So:
\[
X = (K + \delta I\_n)\textasciicircum{}\{-1\} R \, (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\}.
\]

This factors as:
\begin{itemize}
\item Step 1: \(Y = (K + \delta I\_n)\textasciicircum{}\{-1\} R\), i.e., solve \((K + \delta I\_n) Y = R\). This requires \(r\) solves of size \(n\), each costing \(O(n\textasciicircum{}2)\) given the Cholesky factor, for a total of \(O(n\textasciicircum{}2 r)\).
\item Step 2: \(X = Y (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\}\), i.e., solve \(X (G\_\Omega + \lambda I\_r) = Y\), equivalently \((G\_\Omega + \lambda I\_r) X\textasciicircum{}\top = Y\textasciicircum{}\top\). This requires \(n\) solves of size \(r\), each costing \(O(r\textasciicircum{}2)\) given the Cholesky factor, for a total of \(O(nr\textasciicircum{}2)\).

\end{itemize}
Total per-apply: \(O(n\textasciicircum{}2 r + nr\textasciicircum{}2)\). The one-time Cholesky factorizations cost \(O(n\textasciicircum{}3)\) for \(K + \delta I\_n\) and \(O(r\textasciicircum{}3)\) for \(G\_\Omega + \lambda I\_r\). \(\square\)

\subsubsection{Lemma 6: PCG convergence bound}

\textbf{Statement.} The preconditioned CG iterates for the SPD system \(Aw = b\) with SPD preconditioner \(P\) satisfy:
\[
\|e\_m\|\_A \leq 2 \left(\frac\{\sqrt\{\kappa\} - 1\}\{\sqrt\{\kappa\} + 1\}\right)\textasciicircum{}m \|e\_0\|\_A, \quad \kappa = \kappa(P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\}).
\]

\textbf{Proof.} This is the standard result from the theory of the conjugate gradient method applied to the preconditioned system. We include the argument for completeness.

PCG applied to \(Aw = b\) with preconditioner \(P\) is mathematically equivalent to standard CG applied to the transformed system \(\hat\{A\} \hat\{w\} = \hat\{b\}\), where \(\hat\{A\} = P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\}\), \(\hat\{w\} = P\textasciicircum{}\{1/2\} w\), and \(\hat\{b\} = P\textasciicircum{}\{-1/2\} b\).

By the CG optimality property, the \(m\)-th iterate \(\hat\{w\}\_m\) minimizes the \(\hat\{A\}\)-norm of the error over the Krylov subspace:
\[
\hat\{w\}\_m = \arg\min\_\{\hat\{w\} \in \hat\{w\}\_0 + \mathcal\{K\}\_m(\hat\{A\}, \hat\{r\}\_0)\} \|\hat\{w\} - \hat\{w\}\_*\|\_\{\hat\{A\}\},
\]
where \(\hat\{r\}\_0 = \hat\{b\} - \hat\{A\}\hat\{w\}\_0\) and \(\hat\{w\}\_* = \hat\{A\}\textasciicircum{}\{-1\}\hat\{b\}\).

Equivalently, \(\hat\{e\}\_m = \hat\{w\}\_m - \hat\{w\}\_*\) satisfies:
\[
\|\hat\{e\}\_m\|\_\{\hat\{A\}\} = \min\_\{p \in \mathcal\{P\}\_m,\, p(0)=1\} \|p(\hat\{A\}) \hat\{e\}\_0\|\_\{\hat\{A\}\} \leq \min\_\{p \in \mathcal\{P\}\_m,\, p(0)=1\} \max\_\{\mu \in \sigma(\hat\{A\})\} |p(\mu)| \cdot \|\hat\{e\}\_0\|\_\{\hat\{A\}\},
\]
where \(\mathcal\{P\}\_m\) denotes polynomials of degree \(\leq m\).

The eigenvalues of \(\hat\{A\}\) lie in \([\lambda\_\{\min\}(\hat\{A\}), \lambda\_\{\max\}(\hat\{A\})]\). Choosing the Chebyshev polynomial of degree \(m\) on this interval, scaled to satisfy \(p(0) = 1\):
\[
\min\_\{p \in \mathcal\{P\}\_m,\, p(0)=1\} \max\_\{\mu \in [\lambda\_\{\min\}, \lambda\_\{\max\}]\} |p(\mu)| \leq \frac\{2\}\{1 + (T\_m(\frac\{\lambda\_\{\max\}+\lambda\_\{\min\}\}\{\lambda\_\{\max\}-\lambda\_\{\min\}\}))\textasciicircum{}\{-1\}\} \leq 2\left(\frac\{\sqrt\{\kappa\}-1\}\{\sqrt\{\kappa\}+1\}\right)\textasciicircum{}m,
\]
where \(\kappa = \lambda\_\{\max\}/\lambda\_\{\min\}\) and \(T\_m\) is the Chebyshev polynomial of the first kind. The last inequality uses the bound \(T\_m(x) \geq \frac\{1\}\{2\}\left(x + \sqrt\{x\textasciicircum{}2-1\}\right)\textasciicircum{}m\) for \(x \geq 1\), applied with \(x = (\kappa+1)/(\kappa-1)\).

Now we relate back to the original variables. The \(A\)-norm of the original error satisfies:
\[
\|e\_m\|\_A\textasciicircum{}2 = e\_m\textasciicircum{}\top A e\_m = (P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\} \hat\{e\}\_m)\textasciicircum{}\top \cdot \ldots
\]

More directly: since \(e\_m = P\textasciicircum{}\{-1/2\} \hat\{e\}\_m\), we have
\[
\|e\_m\|\_A\textasciicircum{}2 = \hat\{e\}\_m\textasciicircum{}\top P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\} \hat\{e\}\_m = \|\hat\{e\}\_m\|\_\{\hat\{A\}\}\textasciicircum{}2.
\]

Therefore:
\[
\|e\_m\|\_A = \|\hat\{e\}\_m\|\_\{\hat\{A\}\} \leq 2\left(\frac\{\sqrt\{\kappa\}-1\}\{\sqrt\{\kappa\}+1\}\right)\textasciicircum{}m \|\hat\{e\}\_0\|\_\{\hat\{A\}\} = 2\left(\frac\{\sqrt\{\kappa\}-1\}\{\sqrt\{\kappa\}+1\}\right)\textasciicircum{}m \|e\_0\|\_A. \quad \square
\]

---

\subsection{Gap Closure Notes}

\begin{enumerate}
\item **Symmetry of \(K\).** Throughout, the Kronecker-vec identity \((C \otimes A)\operatorname\{vec\}(B) = \operatorname\{vec\}(ABC\textasciicircum{}\top)\) is applied with \(A = K\). In Lemma 1, we use \((Z \otimes K)\textasciicircum{}\top = (Z\textasciicircum{}\top \otimes K\textasciicircum{}\top) = (Z\textasciicircum{}\top \otimes K)\), which requires \(K = K\textasciicircum{}\top\). This holds because \(K\) is a kernel matrix (Gram matrix of kernel evaluations), hence symmetric by construction. This symmetry is implicit in the mentor's setup but is used critically in the algebra.

\item **Projector \(P\_\Omega\) acts as Hadamard masking.** The mentor defines \(P\_\Omega = SS\textasciicircum{}\top\) as a projector in the vectorized space \(\mathbb\{R\}\textasciicircum{}\{nM\}\). We identified its action with Hadamard masking: \(P\_\Omega \operatorname\{vec\}(X) = \operatorname\{vec\}(\mathbf\{1\}\_\Omega \odot X)\). This holds because \(S \in \mathbb\{R\}\textasciicircum{}\{nM \times q\}\) has columns \(e\_\{(i\_t, m\_t)\}\) (standard basis vectors indexed by the vectorization of position \((i\_t, m\_t)\)), so \(SS\textasciicircum{}\top\) is diagonal with ones at observed positions and zeros elsewhere. This is precisely the action of Hadamard multiplication by \(\mathbf\{1\}\_\Omega\) under the natural bijection between \(\mathbb\{R\}\textasciicircum{}\{nM\}\) and \(\mathbb\{R\}\textasciicircum{}\{n \times M\}\).

\item **Commutativity of accumulation order.** In Lemma 2, we noted that the accumulation loop \(C\_\{i\_t,:\} \mathrel\{+\}= \alpha\_t z\_t\) produces the correct result regardless of the order in which observations are processed. This relies on exact arithmetic (commutativity and associativity of real addition). In finite-precision arithmetic, the result is subject to round-off but remains a faithful approximation, with errors bounded by standard floating-point accumulation analysis (\(O(q \epsilon\_\{\text\{mach\}\})\) relative error per entry under no catastrophic cancellation).

\item **Well-definedness of the Cholesky factors.** The preconditioner-apply procedure (Lemma 5) requires Cholesky factorizations of \(K + \delta I\_n\) and \(G\_\Omega + \lambda I\_r\). The former is SPD because \(K \succeq 0\) (PSD kernel matrix) and \(\delta > 0\). The latter is SPD because \(G\_\Omega = \sum\_t z\_t z\_t\textasciicircum{}\top \succeq 0\) and \(\lambda > 0\). Both Cholesky factorizations are therefore well-defined and numerically stable (with condition numbers bounded by those of the respective matrices).

\item **Consistency of the RHS definition.** The mentor defines \(b = (I\_r \otimes K)\operatorname\{vec\}(B)\) with \(B = TZ\). In the sparse setting, \(T\) is not fully available; only the entries \((T\_\{(k)\})\_\{i\_t, m\_t\} = y\_t\) are observed. Lemma 3 computes \(B\) using only these entries. This is consistent with the optimization objective, which penalizes fit only at observed entries (the data-fit term involves \(P\_\Omega\)). Formally, the normal equations for
\end{enumerate}
\[
\min\_w \frac\{1\}\{2\}\|P\_\Omega(\operatorname\{vec\}(T\_\{(k)\}) - (Z \otimes K)w)\|\textasciicircum{}2 + \frac\{\lambda\}\{2\} w\textasciicircum{}\top (I\_r \otimes K) w
\]
yield \(Aw = b\) with the sparse \(B\) exactly as computed in Lemma 3, since the unobserved entries of \(T\_\{(k)\}\) are annihilated by \(P\_\Omega\) in the gradient.

\begin{enumerate}
\item **No hidden \(O(N)\) costs.** We verify that no step requires \(O(N)\) storage or computation:
\end{enumerate}
\begin{itemize}
\item The full tensor \(T\) and its unfolding \(T\_\{(k)\} \in \mathbb\{R\}\textasciicircum{}\{n \times M\}\) are never formed; only the \(q\) observed triples are used.
\item The full Khatri-Rao product \(Z \in \mathbb\{R\}\textasciicircum{}\{M \times r\}\) is never formed; only the sampled rows \(z\_t \in \mathbb\{R\}\textasciicircum{}r\) (for \(t = 1, \ldots, q\)) are accessed.
\item The Kronecker product \(Z \otimes K \in \mathbb\{R\}\textasciicircum{}\{nM \times nr\}\) and the projector \(P\_\Omega \in \mathbb\{R\}\textasciicircum{}\{nM \times nM\}\) are never formed.
\item All matrices that are explicitly formed have dimensions involving only \(n, r, q\) (specifically: \(K \in \mathbb\{R\}\textasciicircum{}\{n \times n\}\), \(V, U, C, B, Y \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\), \(G\_\Omega \in \mathbb\{R\}\textasciicircum{}\{r \times r\}\)), none of which is \(O(N)\) under the assumption \(n, r < q \ll N\).

\end{itemize}
\begin{enumerate}
\item **Preconditioner quality and the role of Assumption 5.** The convergence rate depends on \(\kappa = \kappa(P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\})\). The preconditioner \(P = (G\_\Omega + \lambda I\_r) \otimes (K + \delta I\_n)\) approximates \(A\) well when the observation pattern is reasonably balanced. Specifically, if observations are distributed such that the per-row sampling density is approximately uniform, then \((\mathbf\{1\}\_\Omega \odot (X)) Z \approx \frac\{q\}\{nM\} X Z\) for smooth \(X\), making \(A \approx \frac\{q\}\{nM\} (Z\textasciicircum{}\top Z \otimes K\textasciicircum{}2) + \lambda (I\_r \otimes K)\), which is spectrally close to a scalar multiple of \(P\) with appropriate \(\delta\). The precise spectral equivalence constants depend on the sampling distribution and are not derived here; Assumption 5 (no pathological imbalance) ensures that \(\kappa\) remains moderate. Without this assumption, adversarial observation patterns can make \(\kappa\) arbitrarily large, degrading convergence.
\end{enumerate}

\subsection{Editor Dispatch}
\noindent\textit{Reasoning:} codex\_cli is strongest for strict step-by-step logical validation and detecting hidden gaps; claude\_code is best for improving notation discipline, readability, and graduate-level presentation rigor; gemini\_api is best matched to validating cited results, theorem statements, and hypothesis compliance.
\begin{itemize}
\item Correctness \& Completeness $\to$ codex\_cli
\item Clarity \& Rigor $\to$ claude\_code
\item Reference Validity $\to$ gemini\_api
\end{itemize}

\subsection{Reviewer Feedback}
\subsubsection{Correctness \& Completeness (by codex\_cli)}
\begin{enumerate}
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Lemma 3 / RHS definition (and theorem definitions of b, B): The proof implicitly replaces B = T\_\{(k)\}Z with a sparse observed-only accumulation, but that is only equal if T is explicitly the masked tensor T\_Ω := 1\_Ω ⊙ T (or unobserved entries are defined as zero). As written, if T denotes the full tensor, the computed B is not T\_\{(k)\}Z, so b is mismatched.
\\
\textit{Required fix:} State explicitly that the objective uses T\_Ω (observed entries only), redefine B as B := T\_Ω Z, and derive b from normal equations as b = (Z ⊗ K)\textasciicircum{}T P\_Ω vec(T) = vec(KB).
\\
\textit{Suggestion:} Add a short derivation from the masked least-squares objective to the normal equations so the sparse accumulation formula for B is unambiguous and exact.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Lemma 6 (PCG convergence proof): One displayed line is incomplete (`(P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\} \hat e\_m)\textasciicircum{}T \cdot ...`), and the Chebyshev minimax bound is written in a nonstandard form without justification. This weakens rigor at graduate level.
\\
\textit{Required fix:} Replace that segment with a clean standard statement: CG on \hat A = P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\} gives \|e\_m\|\_A = \|\hat e\_m\|\_\{\hat A\} and the textbook bound with κ(\hat A).
\\
\textit{Suggestion:} Either cite a standard theorem directly or provide the exact minimax identity \(\min\_\{p(0)=1\}\max\_\{\lambda\in[a,b]\}|p(\lambda)| = 1/|T\_m((a+b)/(b-a))|\) before the final inequality.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Gap Closure Note 7 (preconditioner quality discussion): The claim that balanced sampling makes A spectrally close to a Kronecker form is heuristic and not proved (no sampling model, no concentration/spectral bounds).
\\
\textit{Required fix:} Either remove this as a proof claim or formalize a sampling assumption (e.g., i.i.d. Bernoulli/uniform without replacement) and prove explicit spectral-equivalence bounds.
\\
\textit{Suggestion:} Keep the theorem’s unconditional PCG bound in terms of κ, and move small-κ intuition to a separate remark labeled as heuristic unless a formal lemma is added.
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item Uniqueness of observed pairs is implicitly used via Ω as a set; if duplicate observations are allowed, define how they are aggregated and how S, P\_Ω are modified.
\end{itemize}

\subsubsection{Clarity \& Rigor (by claude\_code)}
\begin{enumerate}
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Lemma 6, Chebyshev bound derivation: The intermediate expression involving 2/(1 + T\_m(...)\textasciicircum{}\{-1\}) is not a standard or correct form of the Chebyshev minimax bound. The correct minimax value is 2/T\_m((kappa+1)/(kappa-1)), and the displayed formula does not reduce to this.
\\
\textit{Required fix:} Replace the intermediate bound with the standard form: min\_\{p\} max\_\{mu\} |p(mu)| = 1/T\_m((kappa+1)/(kappa-1)) <= 2((sqrt(kappa)-1)/(sqrt(kappa)+1))\textasciicircum{}m.
\\
\textit{Suggestion:} Use the standard two-step derivation: (1) the minimax polynomial on [lambda\_min, lambda\_max] with p(0)=1 has value 1/T\_m((lambda\_max+lambda\_min)/(lambda\_max-lambda\_min)), (2) bound T\_m from below using the exponential identity. Cite Golub \& Van Loan or Saad's textbook for the clean statement.
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Lemma 3 / overall proof structure: The optimization objective that gives rise to the normal equations Aw = b is never stated in the main proof. The RHS b is defined algebraically but the reader cannot verify its correctness without knowing what is being minimized. This appears only in gap closure note 5, which is too late.
\\
\textit{Required fix:} State the optimization problem (regularized least-squares on observed entries) before Part 1, and derive the normal equations Aw = b from it. This makes the SPD property of A and the form of b self-evident.
\\
\textit{Suggestion:} Add a 'Problem Setup' paragraph before Part 1 that states: we seek w minimizing (1/2)||P\_Omega(vec(T\_\{(k)\}) - (Z otimes K)w)||\textasciicircum{}2 + (lambda/2) w\textasciicircum{}T (I\_r otimes K) w. Then the gradient set to zero gives Aw = b. This also clarifies why only observed entries appear in b.
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Definitions / Lemma 3: The mentor defines B = TZ where T is the full tensor, but Lemma 3 uses B = T\_\{(k)\} Z where T\_\{(k)\} is the mode-k unfolding. These are different mathematical objects and the conflation is never resolved.
\\
\textit{Required fix:} Consistently use T\_\{(k)\} (the mode-k matricization) throughout. The mentor's notation section should define T\_\{(k)\} and use it in the definition of B.
\\
\textit{Suggestion:} In the Definitions section, add: 'Let T\_\{(k)\} in R\textasciicircum{}\{n x M\} denote the mode-k unfolding of T.' Then replace B = TZ with B = T\_\{(k)\}Z everywhere. This small fix eliminates a genuine ambiguity.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Lemma 6, error transformation: The identity e\_m = P\textasciicircum{}\{-1/2\} hat\{e\}\_m is asserted with an incomplete derivation. The proof writes 'More directly: since e\_m = P\textasciicircum{}\{-1/2\} hat\{e\}\_m' without showing why, then uses this to equate the A-norm and hat\{A\}-norm.
\\
\textit{Required fix:} Add a one-line derivation: since hat\{w\}\_m = P\textasciicircum{}\{1/2\} w\_m and hat\{w\}\_* = P\textasciicircum{}\{1/2\} w\_*, we get hat\{e\}\_m = P\textasciicircum{}\{1/2\} e\_m, hence e\_m = P\textasciicircum{}\{-1/2\} hat\{e\}\_m.
\\
\textit{Suggestion:} Replace the 'More directly' paragraph with an explicit substitution showing the chain hat\{e\}\_m = hat\{w\}\_m - hat\{w\}\_* = P\textasciicircum{}\{1/2\}(w\_m - w\_*) = P\textasciicircum{}\{1/2\} e\_m. Then ||e\_m||\_A = ||hat\{e\}\_m||\_\{hat\{A\}\} follows by expanding the quadratic forms.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 3, complexity simplification: The claim 'nr <= qr by assumption n < q' is used to justify dropping the O(nr) initialization cost, but the final complexity O(qr + n\textasciicircum{}2r) already subsumes O(nr) regardless. The dominance relationship between n\textasciicircum{}2r and qr is never discussed.
\\
\textit{Required fix:} Remove the unnecessary justification 'since nr <= qr by assumption n < q'. Instead, note that the two dominant terms are qr (from the accumulation loop) and n\textasciicircum{}2r (from two kernel multiplies), and the total is O(qr + n\textasciicircum{}2r) without further simplification.
\\
\textit{Suggestion:} Briefly note that if q >= n\textasciicircum{}2 the accumulation loop dominates, and if q < n\textasciicircum{}2 the kernel multiplies dominate, so both terms must be retained in the bound.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 4 / Part 3, on-the-fly z\_t cost: Part 3 notes that on-the-fly computation of z\_t adds O(q(d-1)r) to the matvec cost, but Part 4 does not account for the same cost when forming G\_Omega during preconditioner setup.
\\
\textit{Required fix:} Add a note in Part 4 that if z\_t vectors are generated on-the-fly, the setup cost for G\_Omega becomes O(q(d-1)r + qr\textasciicircum{}2) rather than O(qr\textasciicircum{}2).
\\
\textit{Suggestion:} Add a parenthetical remark after 'total O(qr\textasciicircum{}2)' in the G\_Omega formation step: '(plus O(q(d-1)r) if z\_t are generated on-the-fly, as in Part 3).'
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Theorem statement, claim 2: The += notation in the theorem statement is pseudocode, not standard mathematical notation. At graduate rigor level, a theorem statement should use purely mathematical language.
\\
\textit{Required fix:} Reformulate claim 2 to describe the matvec algorithm in mathematical terms, reserving pseudocode for an 'Algorithm' environment or a remark.
\\
\textit{Suggestion:} Replace the algorithmic steps in the theorem statement with: 'A vec(V) = vec(K(CZ + lambda KV)) where C\_i,: = sum\_\{t: i\_t=i\} <(KV)\_i,:, z\_t> z\_t.' Then present the four-step procedure as Algorithm 1 or as a remark after the theorem.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Gap closure note 7: The qualitative discussion of preconditioner quality uses informal approximations ('approximately uniform', 'spectrally close to a scalar multiple') without any formal statement or reference.
\\
\textit{Required fix:} Either provide a concrete spectral equivalence bound (even under simplifying assumptions) or cite a specific result from the tensor completion / kernel learning literature.
\\
\textit{Suggestion:} Consider adding: 'For a formal treatment of spectral equivalence bounds for Kronecker preconditioners in sampled settings, see [reference].' If no suitable reference exists, state explicitly that bounding kappa in terms of sampling parameters is an open question or deferred to future work.
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item The proof assumes throughout that the observations (i\_t, m\_t) are distinct (no repeated indices in Omega). If repeated observations are allowed, the projector P\_Omega = SS\textasciicircum{}T is no longer idempotent and the masking interpretation breaks down. This edge case should be stated as an assumption or handled.
\item Numerical stability of the accumulation loop (gap closure note 3) is mentioned but the floating-point error bound O(q epsilon\_mach) is stated without justification. For very large q, compensated summation or other techniques may be needed, which could affect the practical complexity.
\item The proof never specifies the vectorization convention (column-major vs row-major). The Kronecker-vec identity (C otimes A)vec(B) = vec(ABC\textasciicircum{}T) assumes column-major (vec stacks columns). If the implementation uses a different convention, the Kronecker ordering would need to be transposed. Assumption 4 from the mentor mentions consistency but this should be made explicit.
\end{itemize}

\subsubsection{Reference Validity (by gemini\_api)}
\noindent No issues identified.

\subsection{Editor Decision}
\noindent\textbf{Verdict:} \textcolor{orange}{\textbf{right_track}}

\noindent\textbf{Summary:} The proof uses the right overall strategy and most algebraic components are directionally correct, but it does not yet meet graduate-level rigor because key setup and consistency pieces are missing or flawed: the objective and normal equations are not stated upfront, RHS definitions are inconsistent (full tensor vs masked observed data; B = TZ vs B = T\_(k)Z), and Lemma 6 contains an incomplete derivation plus a nonstandard/incorrect Chebyshev bound expression. The preconditioner-quality discussion is currently heuristic without formal assumptions or proof. Remaining comments are presentation/consistency refinements rather than fundamental redesign.

\noindent\textbf{Feedback (prover):} Revise the proof in this order: (1) add a Problem Setup paragraph before Part 1 stating the masked regularized least-squares objective and derive Aw=b directly from its gradient; (2) standardize notation globally to mode-k unfolding T\_(k), define observed-data masking explicitly (e.g., T\_Omega = 1\_Omega \odot T\_(k)), and define B consistently as T\_Omega Z so the sparse accumulation in Lemma 3 is exactly the stated object; (3) replace Lemma 6 with the textbook PCG result on \hat A = P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\}, include the explicit error transform \hat e\_m = P\textasciicircum{}\{1/2\}e\_m, and use the standard minimax/Chebyshev identity 1/T\_m((\lambda\_max+\lambda\_min)/(\lambda\_max-\lambda\_min)) <= 2((sqrt\kappa-1)/(sqrt\kappa+1))\textasciicircum{}m; remove the incorrect intermediate bound and incomplete display; (4) move preconditioner-quality claims to a clearly labeled heuristic remark unless you provide a formal sampling model and spectral-equivalence theorem; (5) make minor clarity fixes: remove += pseudocode from theorem claims, clean Part 3 complexity wording without unnecessary nr<=qr justification, and in Part 4 include the extra O(q(d-1)r) setup term when z\_t are generated on-the-fly.

\section{Loop 2}
\noindent Editor verdict for this loop: \textcolor{orange}{\textbf{right_track}}

\subsection{Researcher}
\subsection{Relevant Theorems}
\begin{itemize}
\item **PCG Convergence Bound:** For a linear system $Ax=b$ with $A$ symmetric positive definite (SPD) and an SPD preconditioner $P$, the $A$-norm of the error at iteration $m$, $\|e_m\|_A$, satisfies:
\end{itemize}
  \[
  \frac\{\|e\_m\|\_A\}\{\|e\_0\|\_A\} \le 2 \left( \frac\{\sqrt\{\kappa\}-1\}\{\sqrt\{\kappa\}+1\} \right)\textasciicircum{}m, \quad \text\{where \} \kappa = \kappa(P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\}) = \frac\{\lambda\_\{\max\}(P\textasciicircum{}\{-1\}A)\}\{\lambda\_\{\min\}(P\textasciicircum{}\{-1\}A)\}.
  \]
  (Saad, \textit{Iterative Methods for Sparse Linear Systems}, 2nd ed., Thm 9.4; Golub \& Van Loan, \textit{Matrix Computations}, 4th ed., §11.3).
\begin{itemize}
\item **Kronecker-Vec Identity:** $\operatorname{vec}(AXB) = (B^\top \otimes A)\operatorname{vec}(X)$.
\item **Kronecker Product Inverse:** $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$ (assuming invertibility).
\item **Trace-Norm Relation:** $\|X\|_F^2 = \operatorname{tr}(X^\top X) = \operatorname{vec}(X)^\top \operatorname{vec}(X)$.
\item **Gradient of Quadratic Form:** $\nabla_x (\frac{1}{2} \|Ax - b\|_2^2 + \frac{\lambda}{2} x^\top Q x) = A^\top(Ax - b) + \lambda Q x$.

\end{itemize}
\subsection{Key Definitions}
\begin{itemize}
\item **Mode-$k$ Unfolding:** Let $T_{(k)} \in \mathbb{R}^{n \times M}$ denote the mode-$k$ unfolding of the tensor $\mathcal{T}$.
\item **Observed Set and Mask:** Let $\Omega \subset [n] \times [M]$ be the set of indices $(i, m)$ where the tensor entry is observed. Let $q = |\Omega|$.
\item **Selection Operator:** $P_\Omega: \mathbb{R}^{nM} \to \mathbb{R}^{nM}$ is the diagonal projection matrix such that for any matrix $X \in \mathbb{R}^{n \times M}$, $P_\Omega \operatorname{vec}(X) = \operatorname{vec}(\mathbf{1}_\Omega \odot X)$, where $\mathbf{1}_\Omega$ is the binary mask matrix.
\item **Optimization Objective:** We seek $W \in \mathbb{R}^{n \times r}$ minimizing:
\end{itemize}
  \[
  J(W) = \frac\{1\}\{2\} \| P\_\Omega (\operatorname\{vec\}(T\_\{(k)\}) - (Z \otimes K)\operatorname\{vec\}(W)) \|\_2\textasciicircum{}2 + \frac\{\lambda\}\{2\} \operatorname\{vec\}(W)\textasciicircum{}\top (I\_r \otimes K) \operatorname\{vec\}(W).
  \]
\begin{itemize}
\item **Linear System:** The normal equations $\nabla J(W) = 0$ yield $Aw = b$ with $w = \operatorname{vec}(W)$, where:
\end{itemize}
  \[
  A = (Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K) + \lambda (I\_r \otimes K),
  \]
  \[
  b = (Z \otimes K)\textasciicircum{}\top P\_\Omega \operatorname\{vec\}(T\_\{(k)\}).
  \]
\begin{itemize}
\item **Sparse RHS Accumulator:** Let $B \in \mathbb{R}^{n \times r}$ be defined as $B = (\mathbf{1}_\Omega \odot T_{(k)}) Z$. Note that this differs from $T_{(k)}Z$ if unobserved entries are not zeroed.
\item **Sampled Gram Matrix:** $G_\Omega = \sum_{t=1}^q z_t z_t^\top \in \mathbb{R}^{r \times r}$, where $z_t$ is the $m_t$-th row of $Z$.
\item **Preconditioner:** $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, with $\delta > 0$.

\end{itemize}
\subsection{Proof Strategies}
\begin{enumerate}
\item **Derivation from Objective:** Explicitly derive the linear system $Aw=b$ from the gradient of the regularized least-squares objective $J(W)$. This establishes the correctness of the system and the specific form of $b$ involving only observed entries.
\item **Matrix-Free Operator:** Prove that the matrix-vector product $A \operatorname{vec}(V)$ can be computed as $\operatorname{vec}(\mathcal{A}(V))$ where $\mathcal{A}(V) = K [ (\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V ]$. Describe the computation using mathematical summation notation over $\Omega$ to demonstrate $O(qr + n^2r)$ complexity.
\item **Sparse RHS Computation:** Show that $b = \operatorname{vec}(KB)$ where $B$ is the sparse MTTKRP result. Prove that computing $B$ requires only $O(qr)$ operations by iterating over observed entries, and computing $b$ requires $O(n^2r)$.
\item **Preconditioner Application:** Show that $P^{-1} \operatorname{vec}(R)$ is equivalent to solving two small systems (Sylvester-like structure via Kronecker factors). Specifically, $X = (K+\delta I)^{-1} R (G_\Omega + \lambda I)^{-1}$. Analyze the complexity: $O(n^3 + r^3)$ setup, $O(n^2r + nr^2)$ per iteration.
\item **PCG Convergence:** Invoke the standard PCG convergence theorem for the transformed system $\hat{A} = P^{-1/2} A P^{-1/2}$. Explicitly relate the error in the original space to the transformed space ($\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$).
\item **Complexity Analysis:** Sum the costs: Setup + $m \times$ (Matvec + Preconditioner Apply). Ensure no $O(N)$ terms appear.

\end{enumerate}
\subsection{Gaps and Concerns}
\begin{itemize}
\item **Notation Consistency:** The prompt uses $T$ for the unfolding, but standard literature often uses $T_{(k)}$. I will use $T_{(k)}$ to be precise, as requested by the editor, while noting it corresponds to the prompt's $T$.
\item **RHS Definition:** The prompt defined $B = TZ$ (implying full multiplication). I must clarify that in the context of missing data, the "data" vector $b$ comes from the projected observed data, effectively using $T_\Omega = \mathbf{1}_\Omega \odot T_{(k)}$.
\item **Chebyshev Bound:** I must ensure the bound is stated in its standard simple form without the incorrect intermediate algebraic manipulation found in the previous draft.
\item **Preconditioner Quality:** The spectral equivalence of $P$ and $A$ is heuristic (dependent on sampling uniformity). I will present the preconditioner as a "standard choice" motivated by the structure of $A$ when sampling is uniform, but will not claim a proven condition number bound, relying instead on the generic PCG bound in terms of $\kappa$.
\item **On-the-fly Factors:** If $Z$ rows are generated on the fly, the setup of $G_\Omega$ costs $O(q(d-1)r + qr^2)$. This should be included for completeness.
\end{itemize}

\subsection{Mentor}
\subsection{Definitions}
Let \(k\) be the target mode, \(n:=n\_k\), \(M:=\prod\_\{i\neq k\} n\_i\), \(N=nM\), and rank \(r\).

Let \(\Omega\subset [n]\times[M]\) be the observed index set in mode-\(k\) unfolding, \(|\Omega|=q\), with observed triples \((i\_t,m\_t,y\_t)\), \(t=1,\dots,q\), where \(y\_t=T\_\{i\_t,m\_t\}\).

Define
\[
Z:=A\_d\odot\cdots\odot A\_\{k+1\}\odot A\_\{k-1\}\odot\cdots\odot A\_1\in\mathbb\{R\}\textasciicircum{}\{M\times r\},
\quad
z\_t:=Z\_\{m\_t,:\}\in\mathbb\{R\}\textasciicircum{}r.
\]
Unknown is \(W\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\), \(w:=\mathrm\{vec\}(W)\), and \(A\_k=KW\).

Define \(P\_\Omega:=SS\textasciicircum{}\top\) (projector selecting observed entries in \(\mathrm\{vec\}(T)\)) and linear system
\[
A w=b,\quad
A=(Z\otimes K)\textasciicircum{}\top P\_\Omega (Z\otimes K)+\lambda(I\_r\otimes K),\quad
b=(I\_r\otimes K)\mathrm\{vec\}(B),\ B=TZ.
\]

Define mask matrix \(\mathbf\{1\}\_\Omega\in\\{0,1\\}\textasciicircum{}\{n\times M\}\). For \(V\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\),
\[
\mathcal A(V):=
K\!\left((\mathbf\{1\}\_\Omega\odot (KVZ\textasciicircum{}\top))Z+\lambda V\right),
\quad
A\,\mathrm\{vec\}(V)=\mathrm\{vec\}(\mathcal A(V)).
\]

\subsection{Formal Statement}
\textbf{Theorem (Matrix-free PCG for RKHS mode-\(k\) subproblem).} Under the assumptions in Section \textbf{Assumptions}:

\begin{enumerate}
\item \(A\) is SPD, so PCG is well-defined.
\item For any \(V\), \(A\mathrm\{vec\}(V)\) can be computed exactly without forming \(A\), \(Z\otimes K\), or any \(N\)-scale object via:
\end{enumerate}
\[
U=KV,\ \ 
\alpha\_t=\langle U\_\{i\_t,:\},z\_t\rangle,\ \ 
C\_\{i\_t,:\}\mathrel\{+\}=\alpha\_t z\_t,\ \ 
Y=KC+\lambda U,
\]
return \(\mathrm\{vec\}(Y)\).
\begin{enumerate}
\item The per-matvec complexity is \(O(qr+n\textasciicircum{}2r)\) (plus \(O(q(d-1)r)\) if each \(z\_t\) is generated on-the-fly from CP factors).
\item With preconditioner
\end{enumerate}
\[
P=((G\_\Omega+\lambda I\_r)\otimes (K+\delta I\_n)),\quad
G\_\Omega:=\sum\_\{t=1\}\textasciicircum{}q z\_tz\_t\textasciicircum{}\top,\ \delta>0,
\]
each \(P\textasciicircum{}\{-1\}\)-apply costs \(O(n\textasciicircum{}2r+nr\textasciicircum{}2)\) after setup \(O(qr\textasciicircum{}2+n\textasciicircum{}3+r\textasciicircum{}3)\).
\begin{enumerate}
\item PCG iterates satisfy
\end{enumerate}
\[
\|e\_m\|\_A\le 2\Big(\frac\{\sqrt\kappa-1\}\{\sqrt\kappa+1\}\Big)\textasciicircum{}m\|e\_0\|\_A,\quad
\kappa=\kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\}),
\]
hence total cost
\[
O\!\left(m(qr+n\textasciicircum{}2r+nr\textasciicircum{}2)+qr\textasciicircum{}2+n\textasciicircum{}3+r\textasciicircum{}3\right),
\]
with no \(O(N)\) operation.

\subsection{Assumptions}
\begin{enumerate}
\item \(\lambda>0\).
\item \(K\succ 0\) for strict SPD proof of \(A\); if original kernel is only PSD, either add nugget to the model kernel or solve on \(\mathrm\{range\}(K)\).
\item \(n,r<q\ll N\), and observed tuples \((i\_t,m\_t,y\_t)\) are directly available.
\item Indexing conventions for unfolding, vectorization, and \(S\) are consistent.
\item For preconditioner quality claims (small \(\kappa\)), observation pattern is not pathologically imbalanced across \(i\_t\) and rank directions.

\end{enumerate}
\subsection{Notation}
\begin{itemize}
\item \(n=n\_k\): size of mode \(k\).
\item \(M=\prod\_\{i\neq k\} n\_i\): product of non-\(k\) dimensions.
\item \(N=nM\): total entries.
\item \(q=|\Omega|\): number of observed entries.
\item \(Z\in\mathbb\{R\}\textasciicircum{}\{M\times r\}\): Khatri-Rao product of fixed factors.
\item \(K\in\mathbb\{R\}\textasciicircum{}\{n\times n\}\): kernel matrix.
\item \(W\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\), \(w=\mathrm\{vec\}(W)\): unknown.
\item \(B=TZ\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\): sparse MTTKRP result.
\item \(z\_t=Z\_\{m\_t,:\}\): sampled Khatri-Rao row for observation \(t\).
\item \(P\_\Omega=SS\textasciicircum{}\top\): selection projector in vectorized unfolding space.
\item \(G\_\Omega=\sum\_t z\_tz\_t\textasciicircum{}\top\in\mathbb\{R\}\textasciicircum{}\{r\times r\}\): sampled rank-Gram.

\end{itemize}
\subsection{High-Level Strategy}
\begin{enumerate}
\item Rewrite the Hessian action as a matrix operator \(\mathcal A(V)\) using \((C\otimes A)\mathrm\{vec\}(B)=\mathrm\{vec\}(AB C\textasciicircum{}\top)\).
\item Implement \(\mathcal A(V)\) from observed entries only, via rowwise accumulation \(C\_\{i\_t,:\} \!+\!= \alpha\_t z\_t\), never forming \(P\_\Omega\), \(Z\otimes K\), or dense \(T\).
\item Compute RHS sparsely: \(B\_\{i\_t,:\}\!+\!=y\_t z\_t\), then \(b=\mathrm\{vec\}(KB)\).
\item Run PCG on \(Aw=b\) with Kronecker preconditioner \(P\) that is cheap to invert by two small Cholesky-based solves.
\item Prove complexity by summing matvec cost, preconditioner-apply cost, and one-time setup.

\end{enumerate}
\subsection{Key Lemmas}
\begin{enumerate}
\item **Kronecker-vec masked operator lemma:** \(A\,\mathrm\{vec\}(V)=\mathrm\{vec\}(\mathcal A(V))\).
\item **Sampled accumulation lemma:** \(C=(\mathbf\{1\}\_\Omega\odot(KVZ\textasciicircum{}\top))Z\) is computed exactly by the \(q\)-sample loop in \(O(qr)\).
\item **Sparse RHS lemma:** \(B=TZ\) is computed from observations in \(O(qr)\), then \(b\) in \(O(n\textasciicircum{}2r)\).
\item **SPD lemma:** if \(K\succ0,\lambda>0\), then \(A\succ0\) because
\end{enumerate}
\[
x\textasciicircum{}\top A x=\|P\_\Omega\textasciicircum{}\{1/2\}(Z\otimes K)x\|\_2\textasciicircum{}2+\lambda\|(I\_r\otimes K\textasciicircum{}\{1/2\})x\|\_2\textasciicircum{}2>0.
\]
\begin{enumerate}
\item **Preconditioner-apply lemma:** \(P\textasciicircum{}\{-1\}\mathrm\{vec\}(R)\) equals two solves:
\end{enumerate}
\[
(K+\delta I)Y=R,\quad X(G\_\Omega+\lambda I)=Y,\quad \mathrm\{vec\}(X)=P\textasciicircum{}\{-1\}\mathrm\{vec\}(R).
\]
\begin{enumerate}
\item **PCG convergence lemma:** standard SPD PCG bound with \(\kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\})\).

\end{enumerate}
\subsection{Dependency Graph}
\begin{enumerate}
\item Definitions + Notation \(\rightarrow\) Lemma 1.
\item Lemma 1 \(\rightarrow\) Lemma 2 (matvec algorithm correctness).
\item Definitions \(\rightarrow\) Lemma 3 (RHS correctness/cost).
\item Assumptions 1-2 \(\rightarrow\) Lemma 4 (SPD of \(A\)).
\item Definitions \(\rightarrow\) Lemma 5 (preconditioner form and apply rule).
\item Lemma 4 + Lemma 5 + Lemma 6 \(\rightarrow\) PCG convergence claim.
\item Lemma 2 + Lemma 3 + Lemma 5 \(\rightarrow\) total complexity claim in theorem.

\end{enumerate}
\subsection{Risky Steps}
\begin{enumerate}
\item **PSD kernel issue:** if \(K\) is singular, \(A\) may be singular despite \(\lambda>0\); this must be handled explicitly.
\item **Index consistency:** mismatch between unfolding order, \(S\), and sampled \((i\_t,m\_t)\) invalidates the operator.
\item **Preconditioner quality claim:** spectral-equivalence constants require sampling-balance assumptions; do not claim uniformly good \(\kappa\) without them.
\item **Hidden \(M\)-scale cost:** forming full \(Z\) is infeasible when \(M\) is huge; use only sampled rows \(z\_t\) (cached or on-demand).
\item **Numerical stability:** very small \(\lambda\) or ill-conditioned \(K\) can cause PCG stagnation; monitor relative preconditioned residual and cap iterations.
\end{enumerate}

\subsection{Proof}
Now I have a thorough understanding of the prior proof, the reviewer feedback, and the editor's specific revision requests. Let me write the complete revised proof addressing all five editor feedback points.

\subsection{Complete Proof}

We prove the theorem in five parts, corresponding to claims (1)–(5) of the formal statement. We adopt the following notational conventions throughout.

\textbf{Notation.} Let \(\mathcal\{T\} \in \mathbb\{R\}\textasciicircum{}\{n\_1 \times \cdots \times n\_d\}\) be the observed tensor, \(k\) the target mode, \(n := n\_k\), \(M := \prod\_\{i \neq k\} n\_i\), and \(N = nM\). Let \(T\_\{(k)\} \in \mathbb\{R\}\textasciicircum{}\{n \times M\}\) denote the mode-\(k\) matricization (unfolding) of \(\mathcal\{T\}\). The set of observed index pairs in this unfolding is \(\Omega \subset [n] \times [M]\), with \(|\Omega| = q\). We enumerate the observations as \((i\_t, m\_t, y\_t)\) for \(t = 1, \ldots, q\), where \(y\_t = (T\_\{(k)\})\_\{i\_t, m\_t\}\) is the observed value. Define the binary mask matrix \(\mathbf\{1\}\_\Omega \in \\{0,1\\}\textasciicircum{}\{n \times M\}\) with \((\mathbf\{1\}\_\Omega)\_\{i,m\} = 1\) iff \((i,m) \in \Omega\), and the masked unfolding \(T\_\Omega := \mathbf\{1\}\_\Omega \odot T\_\{(k)\} \in \mathbb\{R\}\textasciicircum{}\{n \times M\}\), which equals \(T\_\{(k)\}\) at observed entries and zero elsewhere. The selection matrix \(S \in \mathbb\{R\}\textasciicircum{}\{N \times q\}\) has columns \(e\_\{(i\_t, m\_t)\}\) (standard basis vectors in \(\mathbb\{R\}\textasciicircum{}N\) indexed by the column-major vectorization of position \((i\_t, m\_t)\)), so \(P\_\Omega := SS\textasciicircum{}\top\) is the diagonal projector satisfying \(P\_\Omega \operatorname\{vec\}(X) = \operatorname\{vec\}(\mathbf\{1\}\_\Omega \odot X)\) for any \(X \in \mathbb\{R\}\textasciicircum{}\{n \times M\}\). We assume the observations \((i\_t, m\_t)\) are distinct (i.e., \(\Omega\) is a set, not a multiset), so \(P\_\Omega\) is an orthogonal projector (\(P\_\Omega\textasciicircum{}2 = P\_\Omega\)).

Let \(Z := A\_d \odot \cdots \odot A\_\{k+1\} \odot A\_\{k-1\} \odot \cdots \odot A\_1 \in \mathbb\{R\}\textasciicircum{}\{M \times r\}\) be the Khatri–Rao product of fixed factor matrices, and \(z\_t := Z\_\{m\_t,:\} \in \mathbb\{R\}\textasciicircum{}r\) the row of \(Z\) corresponding to the \(t\)-th observation. Let \(K \in \mathbb\{R\}\textasciicircum{}\{n \times n\}\) be the symmetric kernel matrix with \(K \succ 0\) (Assumption 2). The unknown is \(W \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\) with \(w := \operatorname\{vec\}(W)\), and the RKHS factor is \(A\_k = KW\). We write \(G\_\Omega := \sum\_\{t=1\}\textasciicircum{}q z\_t z\_t\textasciicircum{}\top \in \mathbb\{R\}\textasciicircum{}\{r \times r\}\) for the sampled Gram matrix of Khatri–Rao rows. All vectorizations use column-major ordering, so the Kronecker-vec identity \((C \otimes A)\operatorname\{vec\}(B) = \operatorname\{vec\}(ABC\textasciicircum{}\top)\) holds throughout.

\subsubsection{Problem Setup: Objective and Normal Equations}

The mode-\(k\) subproblem minimizes the regularized least-squares loss over observed entries only:
\[
\mathcal\{L\}(w) := \frac\{1\}\{2\}\bigl\|P\_\Omega\bigl(\operatorname\{vec\}(T\_\{(k)\}) - (Z \otimes K)w\bigr)\bigr\|\_2\textasciicircum{}2 + \frac\{\lambda\}\{2\}\, w\textasciicircum{}\top (I\_r \otimes K)\, w.
\]

We derive the normal equations by setting \(\nabla\_w \mathcal\{L\} = 0\). Since \(P\_\Omega\) is a symmetric projector (\(P\_\Omega = P\_\Omega\textasciicircum{}\top = P\_\Omega\textasciicircum{}2\)):
\[
\nabla\_w \mathcal\{L\} = -(Z \otimes K)\textasciicircum{}\top P\_\Omega\bigl(\operatorname\{vec\}(T\_\{(k)\}) - (Z \otimes K)w\bigr) + \lambda (I\_r \otimes K)\, w.
\]

Setting this to zero and rearranging:
\[
\bigl[(Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K) + \lambda (I\_r \otimes K)\bigr]\, w = (Z \otimes K)\textasciicircum{}\top P\_\Omega \operatorname\{vec\}(T\_\{(k)\}).
\]

The left-hand side defines \(A w\) with \(A := (Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K) + \lambda (I\_r \otimes K)\).

For the right-hand side, we compute:
\[
b := (Z \otimes K)\textasciicircum{}\top P\_\Omega \operatorname\{vec\}(T\_\{(k)\}) = (Z\textasciicircum{}\top \otimes K\textasciicircum{}\top) \operatorname\{vec\}(\mathbf\{1\}\_\Omega \odot T\_\{(k)\}) = (Z\textasciicircum{}\top \otimes K) \operatorname\{vec\}(T\_\Omega),
\]
where we used \(K\textasciicircum{}\top = K\) (symmetry) and the projector-masking equivalence. By the Kronecker-vec identity:
\[
b = \operatorname\{vec\}(K\, T\_\Omega\, Z).
\]

Define \(B := T\_\Omega Z \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\). Then \(b = \operatorname\{vec\}(KB)\). Note that since \(T\_\Omega\) is zero at unobserved entries, \(B\) depends only on the \(q\) observed values \(y\_t\), and we have:
\[
B\_\{i,s\} = \sum\_\{\\{t : i\_t = i\\}\} y\_t\, (z\_t)\_s.
\]

This is the sparse MTTKRP. The system to be solved is thus \(Aw = b\) with \(b = \operatorname\{vec\}(KB)\), exactly as stated in the question.

\textbf{Remark.} The question defines \(B = TZ\) where \(T\) denotes the mode-\(k\) unfolding "with all missing entries set to zero." This convention means \(T \equiv T\_\Omega = \mathbf\{1\}\_\Omega \odot T\_\{(k)\}\), so the question's definition is consistent with ours. We use the explicit notation \(T\_\Omega\) throughout to avoid ambiguity.

\subsubsection{Part 1: \(A\) is symmetric positive definite}

\textbf{Symmetry.} Both terms composing \(A\) are symmetric. For the data-fit term: \(\bigl((Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K)\bigr)\textasciicircum{}\top = (Z \otimes K)\textasciicircum{}\top P\_\Omega\textasciicircum{}\top (Z \otimes K) = (Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K)\), since \(P\_\Omega = P\_\Omega\textasciicircum{}\top\). For the regularization term: \((\lambda(I\_r \otimes K))\textasciicircum{}\top = \lambda(I\_r \otimes K)\), since \(K = K\textasciicircum{}\top\).

\textbf{Positive definiteness.} For any nonzero \(x \in \mathbb\{R\}\textasciicircum{}\{nr\}\), let \(V \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\) be its matricization (\(x = \operatorname\{vec\}(V)\)):
\[
x\textasciicircum{}\top A x = \underbrace\{\|S\textasciicircum{}\top(Z \otimes K)x\|\_2\textasciicircum{}2\}\_\{\geq\, 0\} + \lambda\, x\textasciicircum{}\top (I\_r \otimes K)\, x.
\]

For the second term:
\[
x\textasciicircum{}\top (I\_r \otimes K) x = \operatorname\{vec\}(V)\textasciicircum{}\top \operatorname\{vec\}(KV) = \operatorname\{tr\}(V\textasciicircum{}\top K V) = \|K\textasciicircum{}\{1/2\} V\|\_F\textasciicircum{}2,
\]
where \(K\textasciicircum{}\{1/2\}\) is the unique symmetric positive definite square root of \(K\) (which exists since \(K \succ 0\)). Since \(V \neq 0\) and \(K\textasciicircum{}\{1/2\}\) is nonsingular, \(K\textasciicircum{}\{1/2\}V \neq 0\), hence \(\|K\textasciicircum{}\{1/2\}V\|\_F\textasciicircum{}2 > 0\). Therefore \(x\textasciicircum{}\top A x \geq \lambda \|K\textasciicircum{}\{1/2\}V\|\_F\textasciicircum{}2 > 0\), and \(A\) is SPD. The PCG iteration is well-defined and convergent. \(\square\)

\textbf{Remark (PSD kernel case).} If \(K\) is only PSD with nontrivial nullspace, then \(I\_r \otimes K\) is PSD, and \(\lambda x\textasciicircum{}\top(I\_r \otimes K)x = 0\) for \(x \in \mathbb\{R\}\textasciicircum{}r \otimes \ker(K)\). In this case \(A\) need not be SPD. The standard remedies are: (a) replace \(K\) with \(K + \epsilon I\_n\) for small \(\epsilon > 0\); or (b) restrict the problem to \(\operatorname\{range\}(K)\) in each column. Both ensure strict positive definiteness of the reduced system.

\subsubsection{Part 2: Matrix-free matvec via the operator \(\mathcal\{A\}(V)\)}

We claim that for any \(V \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\),
\[
A\operatorname\{vec\}(V) = \operatorname\{vec\}(\mathcal\{A\}(V)), \quad \mathcal\{A\}(V) := K\bigl((\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top))Z + \lambda V\bigr),
\]
and that this is computed without forming any \(N\)-scale object by the procedure:

> \textbf{Algorithm (Matvec).} Given \(V \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\):
> 1. Compute \(U = KV \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\).
> 2. Initialize \(C = 0 \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\). For each \(t = 1, \ldots, q\): compute \(\alpha\_t = \langle U\_\{i\_t,:\}, z\_t \rangle\) and set \(C\_\{i\_t,:\} \leftarrow C\_\{i\_t,:\} + \alpha\_t z\_t\).
> 3. Compute \(Y = KC + \lambda U\).
> 4. Return \(\operatorname\{vec\}(Y)\).

The algebraic identity \(A\operatorname\{vec\}(V) = \operatorname\{vec\}(\mathcal\{A\}(V))\) is proved in Lemma 1. The equivalence of the accumulation loop to the masked matrix product \(C = (\mathbf\{1\}\_\Omega \odot (UZ\textasciicircum{}\top))Z\) is proved in Lemma 2. Together, these establish that the algorithm computes \(A\operatorname\{vec\}(V)\) exactly. The only objects formed are matrices of dimension \(n \times r\), \(n \times n\), vectors of length \(r\), and scalars — none of dimension \(N\) or \(M\).

\subsubsection{Part 3: Per-matvec complexity is \(O(qr + n\textasciicircum{}2r)\)}

Each application of the matvec algorithm involves:

\begin{itemize}
\item **Step 1** (\(U = KV\)): one matrix multiply \(K \in \mathbb\{R\}\textasciicircum{}\{n \times n\}\) times \(V \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\), costing \(O(n\textasciicircum{}2 r)\).
\item **Step 2** (accumulation loop): for each \(t = 1, \ldots, q\), one inner product \(\langle U\_\{i\_t,:\}, z\_t \rangle\) costing \(O(r)\) and one scaled vector addition \(C\_\{i\_t,:\} \leftarrow C\_\{i\_t,:\} + \alpha\_t z\_t\) costing \(O(r)\). Total: \(O(qr)\). (Initialization \(C = 0\) costs \(O(nr) \subseteq O(qr)\) since \(n < q\).)
\item **Step 3** (\(Y = KC + \lambda U\)): one matrix multiply \(KC\) costing \(O(n\textasciicircum{}2 r)\) and one scaled matrix addition costing \(O(nr)\).

\end{itemize}
The two dominant terms are \(O(qr)\) from the accumulation loop and \(O(n\textasciicircum{}2r)\) from the two kernel multiplications. The total per-matvec cost is \(O(qr + n\textasciicircum{}2r)\). If \(q \geq n\textasciicircum{}2\) the loop dominates; if \(q < n\textasciicircum{}2\) the kernel multiplies dominate; both terms must be retained.

If the Khatri–Rao rows \(z\_t\) are not precomputed but generated on-the-fly from the \(d-1\) fixed CP factor matrices (via elementwise products of the corresponding rows), each \(z\_t\) costs \(O((d-1)r)\), adding \(O(q(d-1)r)\) to the per-matvec cost.

\subsubsection{Part 4: Preconditioner setup and per-apply cost}

The Kronecker preconditioner is
\[
P := (G\_\Omega + \lambda I\_r) \otimes (K + \delta I\_n), \quad \delta > 0.
\]

\textbf{One-time setup:}
\begin{itemize}
\item Form \(G\_\Omega = \sum\_\{t=1\}\textasciicircum{}q z\_t z\_t\textasciicircum{}\top\): accumulate \(q\) rank-1 outer products, each costing \(O(r\textasciicircum{}2)\), total \(O(qr\textasciicircum{}2)\). (If \(z\_t\) vectors are generated on-the-fly, add \(O(q(d-1)r)\) for their computation.)
\item Cholesky-factor \(G\_\Omega + \lambda I\_r \in \mathbb\{R\}\textasciicircum{}\{r \times r\}\): cost \(O(r\textasciicircum{}3)\).
\item Cholesky-factor \(K + \delta I\_n \in \mathbb\{R\}\textasciicircum{}\{n \times n\}\): cost \(O(n\textasciicircum{}3)\).

\end{itemize}
Total setup: \(O(qr\textasciicircum{}2 + n\textasciicircum{}3 + r\textasciicircum{}3)\), plus \(O(q(d-1)r)\) if \(z\_t\) are generated on-the-fly.

\textbf{Per-apply:} Given residual \(R \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\), compute \(X\) such that \(\operatorname\{vec\}(X) = P\textasciicircum{}\{-1\}\operatorname\{vec\}(R)\). By Lemma 5, this decomposes as:
\begin{enumerate}
\item Solve \((K + \delta I\_n) Y = R\) for \(Y\): \(r\) triangular solves of size \(n\), cost \(O(n\textasciicircum{}2 r)\).
\item Solve \(X(G\_\Omega + \lambda I\_r) = Y\) for \(X\): equivalently \((G\_\Omega + \lambda I\_r) X\textasciicircum{}\top = Y\textasciicircum{}\top\), which is \(n\) triangular solves of size \(r\), cost \(O(nr\textasciicircum{}2)\).

\end{enumerate}
Total per-apply: \(O(n\textasciicircum{}2 r + nr\textasciicircum{}2)\).

\subsubsection{Part 5: PCG convergence and total complexity}

Since \(A \succ 0\) (Part 1) and \(P \succ 0\) (both Kronecker factors are SPD: \(K + \delta I \succ 0\) because \(K \succ 0\) and \(\delta > 0\); \(G\_\Omega + \lambda I \succ 0\) because \(G\_\Omega \succeq 0\) and \(\lambda > 0\)), the preconditioned matrix \(\hat\{A\} := P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\}\) is SPD with condition number \(\kappa := \kappa(\hat\{A\}) = \lambda\_\{\max\}(\hat\{A\})/\lambda\_\{\min\}(\hat\{A\})\).

By Lemma 6 (PCG convergence), the \(A\)-norm error of the \(m\)-th PCG iterate satisfies:
\[
\|e\_m\|\_A \leq 2\left(\frac\{\sqrt\{\kappa\} - 1\}\{\sqrt\{\kappa\} + 1\}\right)\textasciicircum{}m \|e\_0\|\_A.
\]

To achieve \(\|e\_m\|\_A / \|e\_0\|\_A \leq \varepsilon\), it suffices to take \(m = O(\sqrt\{\kappa\}\,\log(1/\varepsilon))\) iterations (since \((\sqrt\{\kappa\}-1)/(\sqrt\{\kappa\}+1) = 1 - 2/(\sqrt\{\kappa\}+1)\) and \(\log(1-2/(\sqrt\{\kappa\}+1)) \approx -2/\sqrt\{\kappa\}\) for large \(\kappa\)).

Each PCG iteration requires one matvec (\(O(qr + n\textasciicircum{}2r)\), Part 3) and one preconditioner apply (\(O(n\textasciicircum{}2r + nr\textasciicircum{}2)\), Part 4), plus \(O(nr)\) for vector operations (inner products, axpy). The total cost for \(m\) iterations is:
\[
O\!\left(m(qr + n\textasciicircum{}2r + nr\textasciicircum{}2) + qr\textasciicircum{}2 + n\textasciicircum{}3 + r\textasciicircum{}3\right),
\]
where the first group is the per-iteration cost and the second is the one-time preconditioner setup. Since we access only the \(q\) observed entries and form matrices of size at most \(n \times n\), \(n \times r\), or \(r \times r\), no operation has cost \(O(N)\). \(\blacksquare\)

\textbf{Heuristic Remark (Preconditioner quality).} The convergence rate depends on \(\kappa = \kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\})\), which in turn depends on how well the Kronecker structure of \(P\) approximates the non-Kronecker operator \(A\). Intuitively, when observations are distributed reasonably uniformly across mode-\(k\) indices, the masked Hadamard product \(\mathbf\{1\}\_\Omega \odot (XZ\textasciicircum{}\top)\) behaves approximately like a scaled version of \(XZ\textasciicircum{}\top\), making the data-fit term of \(A\) approximately Kronecker-structured. In this regime, the condition number \(\kappa\) remains moderate with appropriate choice of \(\delta\). However, a formal spectral-equivalence bound relating \(\kappa\) to the sampling distribution would require a specific probabilistic model (e.g., uniform sampling without replacement) and is beyond the scope of this proof. Without such analysis, Assumption 5 serves as a qualitative prerequisite: if the observation pattern is pathologically concentrated on a few mode-\(k\) indices, the Kronecker approximation degrades and \(\kappa\) can grow, slowing convergence.

---

\subsection{Lemma Proofs}

\subsubsection{Lemma 1: Kronecker-vec masked operator identity}

\textbf{Statement.} For any \(V \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\),
\[
A\operatorname\{vec\}(V) = \operatorname\{vec\}\!\left(K\bigl[(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top))Z + \lambda V\bigr]\right).
\]

\textbf{Proof.} We expand \(A\operatorname\{vec\}(V) = \bigl[(Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K) + \lambda(I\_r \otimes K)\bigr]\operatorname\{vec\}(V)\) term by term.

\textit{Regularization term.} By the Kronecker-vec identity \((C \otimes A)\operatorname\{vec\}(B) = \operatorname\{vec\}(ABC\textasciicircum{}\top)\):
\[
\lambda(I\_r \otimes K)\operatorname\{vec\}(V) = \lambda\operatorname\{vec\}(KVI\_r\textasciicircum{}\top) = \lambda\operatorname\{vec\}(KV).
\]

\textit{Data-fit term.} Applying the Kronecker-vec identity to \((Z \otimes K)\operatorname\{vec\}(V)\):
\[
(Z \otimes K)\operatorname\{vec\}(V) = \operatorname\{vec\}(KVZ\textasciicircum{}\top).
\]
This is a vector in \(\mathbb\{R\}\textasciicircum{}\{nM\}\), identified with the matrix \(KVZ\textasciicircum{}\top \in \mathbb\{R\}\textasciicircum{}\{n \times M\}\).

Next, applying \(P\_\Omega\). Since \(P\_\Omega = SS\textasciicircum{}\top\) is the diagonal projector that zeros out entries outside \(\Omega\), and the Hadamard mask \(\mathbf\{1\}\_\Omega\) performs the same operation in matrix form, we have for any \(X \in \mathbb\{R\}\textasciicircum{}\{n \times M\}\):
\[
P\_\Omega\operatorname\{vec\}(X) = \operatorname\{vec\}(\mathbf\{1\}\_\Omega \odot X).
\]
(This holds because \(P\_\Omega\) is diagonal with \((P\_\Omega)\_\{jj\} = 1\) iff index \(j\) corresponds to an observed position \((i,m) \in \Omega\) under column-major vectorization, which is precisely the action of Hadamard multiplication by \(\mathbf\{1\}\_\Omega\).) Therefore:
\[
P\_\Omega(Z \otimes K)\operatorname\{vec\}(V) = \operatorname\{vec\}(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top)).
\]

Finally, applying \((Z \otimes K)\textasciicircum{}\top = (Z\textasciicircum{}\top \otimes K\textasciicircum{}\top) = (Z\textasciicircum{}\top \otimes K)\) (using \(K = K\textasciicircum{}\top\)):
\[
(Z \otimes K)\textasciicircum{}\top \operatorname\{vec\}(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top)) = (Z\textasciicircum{}\top \otimes K)\operatorname\{vec\}(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top)) = \operatorname\{vec\}\!\left(K(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top))Z\right),
\]
where the last step uses \((C \otimes A)\operatorname\{vec\}(X) = \operatorname\{vec\}(AXC\textasciicircum{}\top)\) with \(C = Z\textasciicircum{}\top\), \(A = K\), \(X = \mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top)\), giving \(\operatorname\{vec\}(K(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top))(Z\textasciicircum{}\top)\textasciicircum{}\top) = \operatorname\{vec\}(K(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top))Z)\).

Combining:
\[
A\operatorname\{vec\}(V) = \operatorname\{vec\}\!\left(K(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top))Z\right) + \lambda\operatorname\{vec\}(KV) = \operatorname\{vec\}\!\left(K\bigl[(\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top))Z + \lambda V\bigr]\right). \quad \square
\]

\subsubsection{Lemma 2: Sampled accumulation correctness}

\textbf{Statement.} Let \(U = KV\). The matrix \(C := (\mathbf\{1\}\_\Omega \odot (UZ\textasciicircum{}\top))Z \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\) is computed exactly by initializing \(C = 0\) and, for each \(t = 1, \ldots, q\), setting \(C\_\{i\_t,:\} \leftarrow C\_\{i\_t,:\} + \alpha\_t z\_t\) where \(\alpha\_t = \langle U\_\{i\_t,:\}, z\_t \rangle\). The cost is \(O(qr)\).

\textbf{Proof.} Consider the \(i\)-th row of \(C\). By the definition of matrix multiplication:
\[
C\_\{i,:\} = \sum\_\{m=1\}\textasciicircum{}M (\mathbf\{1\}\_\Omega)\_\{i,m\}\, (UZ\textasciicircum{}\top)\_\{i,m\}\, Z\_\{m,:\}.
\]

The factor \((UZ\textasciicircum{}\top)\_\{i,m\} = \langle U\_\{i,:\}, Z\_\{m,:\} \rangle\). After masking, only terms with \((i,m) \in \Omega\) survive. Since the observations \((i\_t, m\_t)\)\_\{t=1\}\textasciicircum{}q enumerate \(\Omega\) exactly:
\[
C\_\{i,:\} = \sum\_\{\\{t : i\_t = i\\}\} \langle U\_\{i,:\}, z\_t \rangle\, z\_t = \sum\_\{\\{t : i\_t = i\\}\} \alpha\_t\, z\_t.
\]

This is precisely the result of the accumulation loop: for each \(t\), the update \(C\_\{i\_t,:\} \leftarrow C\_\{i\_t,:\} + \alpha\_t z\_t\) adds \(\alpha\_t z\_t\) to row \(i\_t\) of \(C\). The order of summation is immaterial (real addition is commutative and associative in exact arithmetic).

Cost: each of the \(q\) iterations performs one inner product (\(O(r)\)) and one scaled vector addition (\(O(r)\)), for a total of \(O(qr)\). \(\square\)

\textbf{Remark.} By Lemma 1, \(\mathcal\{A\}(V) = K(CZ + \lambda V)\) where \(C\) is defined with \(KVZ\textasciicircum{}\top\). Setting \(U = KV\) in the above, we see \(C = (\mathbf\{1\}\_\Omega \odot (KVZ\textasciicircum{}\top))Z\), matching the definition in \(\mathcal\{A\}\). This closes the chain: the algorithm computes \(U = KV\), then \(C\) via the accumulation loop (this lemma), then \(Y = KC + \lambda U = \mathcal\{A\}(V)\).

\subsubsection{Lemma 3: Sparse RHS computation}

\textbf{Statement.} The matrix \(B = T\_\Omega Z \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\) is computed from observed entries alone in \(O(qr)\), and then \(b = \operatorname\{vec\}(KB)\) in \(O(n\textasciicircum{}2r)\).

\textbf{Proof.} The \((i,s)\)-entry of \(B = T\_\Omega Z\) is:
\[
B\_\{i,s\} = \sum\_\{m=1\}\textasciicircum{}M (T\_\Omega)\_\{i,m\}\, Z\_\{m,s\}.
\]

Since \((T\_\Omega)\_\{i,m\} = y\_t\) if \((i,m) = (i\_t, m\_t)\) for some \(t\), and zero otherwise:
\[
B\_\{i,s\} = \sum\_\{\\{t : i\_t = i\\}\} y\_t\, (z\_t)\_s.
\]

In vector form: \(B\_\{i,:\} = \sum\_\{\\{t : i\_t = i\\}\} y\_t\, z\_t\). This is computed by initializing \(B = 0\) and, for each \(t = 1, \ldots, q\), updating \(B\_\{i\_t,:\} \leftarrow B\_\{i\_t,:\} + y\_t z\_t\). Each update costs \(O(r)\), so the total is \(O(qr)\).

Then \(b = \operatorname\{vec\}(KB)\) requires the matrix product \(K \in \mathbb\{R\}\textasciicircum{}\{n \times n\}\) times \(B \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\), costing \(O(n\textasciicircum{}2r)\).

\textbf{Consistency check.} From the Problem Setup, \(b = (Z \otimes K)\textasciicircum{}\top P\_\Omega \operatorname\{vec\}(T\_\{(k)\}) = \operatorname\{vec\}(K T\_\Omega Z) = \operatorname\{vec\}(KB)\). This matches the question's definition \(b = (I\_r \otimes K)\operatorname\{vec\}(B)\) since \((I\_r \otimes K)\operatorname\{vec\}(B) = \operatorname\{vec\}(KBI\_r\textasciicircum{}\top) = \operatorname\{vec\}(KB)\). The question defines \(B = TZ\) where \(T\) is the unfolding "with all missing entries set to zero," i.e., \(T \equiv T\_\Omega\). \(\square\)

\subsubsection{Lemma 4: SPD property of \(A\)}

This is proved as Part 1 above. We restate the key inequality for reference: for \(K \succ 0\), \(\lambda > 0\), and any nonzero \(x \in \mathbb\{R\}\textasciicircum{}\{nr\}\),
\[
x\textasciicircum{}\top Ax \geq \lambda\, \|K\textasciicircum{}\{1/2\}V\|\_F\textasciicircum{}2 > 0,
\]
where \(V\) is the matricization of \(x\). \(\square\)

\subsubsection{Lemma 5: Preconditioner apply via Kronecker factorization}

\textbf{Statement.} With \(P = (G\_\Omega + \lambda I\_r) \otimes (K + \delta I\_n)\), the action \(\operatorname\{vec\}(X) = P\textasciicircum{}\{-1\}\operatorname\{vec\}(R)\) for \(R \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\) is computed by:
\begin{enumerate}
\item Solve \((K + \delta I\_n)\, Y = R\) for \(Y \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\).
\item Solve \(X\,(G\_\Omega + \lambda I\_r) = Y\) for \(X \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\).

\end{enumerate}
\textbf{Proof.} Both Kronecker factors are SPD:
\begin{itemize}
\item \(K + \delta I\_n \succ 0\) since \(K \succ 0\) and \(\delta > 0\).
\item \(G\_\Omega + \lambda I\_r \succ 0\) since \(G\_\Omega = \sum\_t z\_t z\_t\textasciicircum{}\top \succeq 0\) and \(\lambda > 0\).

\end{itemize}
Hence \(P\) is SPD, and by the Kronecker inverse identity:
\[
P\textasciicircum{}\{-1\} = (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\} \otimes (K + \delta I\_n)\textasciicircum{}\{-1\}.
\]

Applying the Kronecker-vec identity:
\[
P\textasciicircum{}\{-1\}\operatorname\{vec\}(R) = \bigl((G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\} \otimes (K + \delta I\_n)\textasciicircum{}\{-1\}\bigr)\operatorname\{vec\}(R) = \operatorname\{vec\}\!\left((K + \delta I\_n)\textasciicircum{}\{-1\} R\, (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-\top\}\right).
\]

Since \(G\_\Omega + \lambda I\_r\) is symmetric, \((G\_\Omega + \lambda I\_r)\textasciicircum{}\{-\top\} = (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\}\), so:
\[
X = (K + \delta I\_n)\textasciicircum{}\{-1\} R\, (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\}.
\]

This factors as:
\begin{enumerate}
\item \(Y = (K + \delta I\_n)\textasciicircum{}\{-1\}R\), i.e., solve \((K + \delta I\_n)Y = R\) column by column. Using the precomputed Cholesky factor \(L\_K L\_K\textasciicircum{}\top = K + \delta I\_n\), each of the \(r\) forward-backward substitutions costs \(O(n\textasciicircum{}2)\), total \(O(n\textasciicircum{}2r)\).
\item \(X = Y(G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\}\), i.e., solve \(X(G\_\Omega + \lambda I\_r) = Y\), equivalently \((G\_\Omega + \lambda I\_r)X\textasciicircum{}\top = Y\textasciicircum{}\top\) row by row. Using the precomputed Cholesky factor \(L\_G L\_G\textasciicircum{}\top = G\_\Omega + \lambda I\_r\), each of the \(n\) forward-backward substitutions costs \(O(r\textasciicircum{}2)\), total \(O(nr\textasciicircum{}2)\).

\end{enumerate}
Total per-apply: \(O(n\textasciicircum{}2r + nr\textasciicircum{}2)\). One-time Cholesky costs: \(O(n\textasciicircum{}3)\) for \(K + \delta I\_n\) and \(O(r\textasciicircum{}3)\) for \(G\_\Omega + \lambda I\_r\). \(\square\)

\subsubsection{Lemma 6: PCG convergence bound}

\textbf{Statement.} Let \(A, P \in \mathbb\{R\}\textasciicircum{}\{nr \times nr\}\) both be SPD. The preconditioned conjugate gradient method applied to \(Aw = b\) with preconditioner \(P\) produces iterates \(w\_m\) satisfying:
\[
\|e\_m\|\_A \leq 2\left(\frac\{\sqrt\{\kappa\} - 1\}\{\sqrt\{\kappa\} + 1\}\right)\textasciicircum{}m \|e\_0\|\_A, \quad \kappa = \kappa(\hat\{A\}), \quad \hat\{A\} := P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\}.
\]

\textbf{Proof.} Define the change of variables \(\hat\{w\} = P\textasciicircum{}\{1/2\}w\), \(\hat\{b\} = P\textasciicircum{}\{-1/2\}b\), and \(\hat\{A\} = P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\}\). Then \(Aw = b\) becomes \(\hat\{A\}\hat\{w\} = \hat\{b\}\). PCG applied to \(Aw = b\) with preconditioner \(P\) is mathematically equivalent to standard CG applied to \(\hat\{A\}\hat\{w\} = \hat\{b\}\).

\textbf{Error transformation.} Let \(w\_\textit{ = A\textasciicircum{}\{-1\}b\) and \(\hat\{w\}\_} = \hat\{A\}\textasciicircum{}\{-1\}\hat\{b\} = P\textasciicircum{}\{1/2\}w\_*\). The CG iterates in the transformed space satisfy \(\hat\{w\}\_m = P\textasciicircum{}\{1/2\}w\_m\) (by the equivalence of PCG and transformed CG). The transformed error is:
\[
\hat\{e\}\_m := \hat\{w\}\_m - \hat\{w\}\_\textit{ = P\textasciicircum{}\{1/2\}w\_m - P\textasciicircum{}\{1/2\}w\_} = P\textasciicircum{}\{1/2\}(w\_m - w\_*) = P\textasciicircum{}\{1/2\}e\_m.
\]

\textbf{Norm equivalence.} The \(\hat\{A\}\)-norm of \(\hat\{e\}\_m\) equals the \(A\)-norm of \(e\_m\):
\[
\|\hat\{e\}\_m\|\_\{\hat\{A\}\}\textasciicircum{}2 = \hat\{e\}\_m\textasciicircum{}\top \hat\{A\}\, \hat\{e\}\_m = (P\textasciicircum{}\{1/2\}e\_m)\textasciicircum{}\top (P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\})(P\textasciicircum{}\{1/2\}e\_m) = e\_m\textasciicircum{}\top A e\_m = \|e\_m\|\_A\textasciicircum{}2.
\]

\textbf{CG optimality and Chebyshev bound.} By the optimality property of CG, the \(m\)-th iterate minimizes the \(\hat\{A\}\)-norm error over the Krylov subspace:
\[
\|\hat\{e\}\_m\|\_\{\hat\{A\}\} = \min\_\{p \in \mathcal\{P\}\_m,\; p(0) = 1\} \|p(\hat\{A\})\hat\{e\}\_0\|\_\{\hat\{A\}\},
\]
where \(\mathcal\{P\}\_m\) denotes polynomials of degree at most \(m\). Since \(\hat\{A\}\) is SPD with eigenvalues in \([\lambda\_\{\min\}, \lambda\_\{\max\}]\) (where \(\lambda\_\{\min\} = \lambda\_\{\min\}(\hat\{A\}) > 0\)):
\[
\|\hat\{e\}\_m\|\_\{\hat\{A\}\} \leq \min\_\{p \in \mathcal\{P\}\_m,\; p(0) = 1\}\; \max\_\{\mu \in [\lambda\_\{\min\}, \lambda\_\{\max\}]\} |p(\mu)| \cdot \|\hat\{e\}\_0\|\_\{\hat\{A\}\}.
\]

The minimax problem on the right has the classical solution via Chebyshev polynomials of the first kind. The optimal value is:
\[
\min\_\{p \in \mathcal\{P\}\_m,\; p(0) = 1\}\; \max\_\{\mu \in [\lambda\_\{\min\}, \lambda\_\{\max\}]\} |p(\mu)| = \frac\{1\}\{T\_m\!\left(\dfrac\{\lambda\_\{\max\} + \lambda\_\{\min\}\}\{\lambda\_\{\max\} - \lambda\_\{\min\}\}\right)\},
\]
where \(T\_m\) is the Chebyshev polynomial of the first kind of degree \(m\), and the minimizer is the appropriately shifted and scaled Chebyshev polynomial (see, e.g., Saad, \textit{Iterative Methods for Sparse Linear Systems}, Theorem 6.29, or Golub \& Van Loan, \textit{Matrix Computations}, §11.3).

Setting \(\kappa = \lambda\_\{\max\}/\lambda\_\{\min\}\), we have \(\frac\{\lambda\_\{\max\} + \lambda\_\{\min\}\}\{\lambda\_\{\max\} - \lambda\_\{\min\}\} = \frac\{\kappa + 1\}\{\kappa - 1\}\). For \(x \geq 1\), the Chebyshev polynomial satisfies \(T\_m(x) = \cosh(m \operatorname\{arccosh\}(x))\), and using the identity \(\cosh(\theta) \geq \frac\{1\}\{2\}e\textasciicircum{}\theta\) for \(\theta \geq 0\):
\[
T\_m\!\left(\frac\{\kappa+1\}\{\kappa-1\}\right) \geq \frac\{1\}\{2\}\left(\frac\{\kappa+1\}\{\kappa-1\} + \sqrt\{\left(\frac\{\kappa+1\}\{\kappa-1\}\right)\textasciicircum{}2 - 1\}\right)\textasciicircum{}m = \frac\{1\}\{2\}\left(\frac\{\sqrt\{\kappa\}+1\}\{\sqrt\{\kappa\}-1\}\right)\textasciicircum{}m,
\]
where the last equality uses:
\[
\frac\{\kappa+1\}\{\kappa-1\} + \frac\{2\sqrt\{\kappa\}\}\{\kappa-1\} = \frac\{(\sqrt\{\kappa\}+1)\textasciicircum{}2\}\{(\sqrt\{\kappa\}+1)(\sqrt\{\kappa\}-1)\} = \frac\{\sqrt\{\kappa\}+1\}\{\sqrt\{\kappa\}-1\}.
\]

Therefore:
\[
\frac\{1\}\{T\_m\!\left(\frac\{\kappa+1\}\{\kappa-1\}\right)\} \leq \frac\{2\}\{\left(\frac\{\sqrt\{\kappa\}+1\}\{\sqrt\{\kappa\}-1\}\right)\textasciicircum{}m\} = 2\left(\frac\{\sqrt\{\kappa\}-1\}\{\sqrt\{\kappa\}+1\}\right)\textasciicircum{}m.
\]

Combining with the norm equivalence \(\|e\_m\|\_A = \|\hat\{e\}\_m\|\_\{\hat\{A\}\}\):
\[
\|e\_m\|\_A = \|\hat\{e\}\_m\|\_\{\hat\{A\}\} \leq 2\left(\frac\{\sqrt\{\kappa\}-1\}\{\sqrt\{\kappa\}+1\}\right)\textasciicircum{}m \|\hat\{e\}\_0\|\_\{\hat\{A\}\} = 2\left(\frac\{\sqrt\{\kappa\}-1\}\{\sqrt\{\kappa\}+1\}\right)\textasciicircum{}m \|e\_0\|\_A. \quad \square
\]

---

\subsection{Gap Closure Notes}

\begin{enumerate}
\item **Problem Setup and RHS derivation (Editor point 1).** The revised proof opens with a Problem Setup section that states the masked regularized least-squares objective \(\mathcal\{L\}(w)\) and derives the normal equations \(Aw = b\) by setting \(\nabla\_w \mathcal\{L\} = 0\). This makes the origin of both \(A\) and \(b\) transparent and shows directly why only observed entries contribute to \(b\). The derivation also confirms that the question's given system is exactly the gradient condition for this objective.

\item **Consistent notation: \(T\_\{(k)\}\), \(T\_\Omega\), and \(B\) (Editor point 2).** Throughout, we use \(T\_\{(k)\}\) for the mode-\(k\) unfolding (a matrix in \(\mathbb\{R\}\textasciicircum{}\{n \times M\}\)), \(\mathbf\{1\}\_\Omega\) for the binary mask, and \(T\_\Omega := \mathbf\{1\}\_\Omega \odot T\_\{(k)\}\) for the observed-data matrix. We define \(B := T\_\Omega Z\), so the sparse accumulation \(B\_\{i\_t,:\} \leftarrow B\_\{i\_t,:\} + y\_t z\_t\) computes exactly the stated object. The question's notation \(B = TZ\) with \(T\) meaning "unfolding with missing entries set to zero" is reconciled in the Remark after Problem Setup and in Lemma 3's consistency check.

\item **Corrected Lemma 6 (Editor point 3).** The revised Lemma 6 proceeds cleanly through three steps: (a) the error transformation \(\hat\{e\}\_m = P\textasciicircum{}\{1/2\}e\_m\) is derived explicitly from \(\hat\{w\}\_m = P\textasciicircum{}\{1/2\}w\_m\); (b) the norm equivalence \(\|e\_m\|\_A = \|\hat\{e\}\_m\|\_\{\hat\{A\}\}\) is verified by expanding the quadratic form; (c) the Chebyshev bound uses the standard minimax identity \(\min\_\{p(0)=1\} \max\_\{\mu \in [\lambda\_\{\min\},\lambda\_\{\max\}]\} |p(\mu)| = 1/T\_m((\lambda\_\{\max\}+\lambda\_\{\min\})/(\lambda\_\{\max\}-\lambda\_\{\min\}))\), followed by the lower bound \(T\_m(x) \geq \frac\{1\}\{2\}(x + \sqrt\{x\textasciicircum{}2-1\})\textasciicircum{}m\) for \(x \geq 1\), which yields \(2((\sqrt\{\kappa\}-1)/(\sqrt\{\kappa\}+1))\textasciicircum{}m\). The incorrect intermediate expression and incomplete display from the prior version are removed entirely.

\item **Preconditioner quality as heuristic remark (Editor point 4).** The discussion of when \(\kappa\) is small has been moved to a clearly labeled "Heuristic Remark" after Part 5. It provides qualitative intuition (uniform sampling makes the Hadamard masking approximately transparent) but explicitly states that a formal spectral-equivalence bound would require a specific probabilistic sampling model. The theorem itself makes no claims about the magnitude of \(\kappa\) — it holds for any SPD \(P\), with the convergence rate parameterized by \(\kappa(\hat\{A\})\).

\item **Minor clarity fixes (Editor point 5).** (a) The \(\mathrel\{+\}=\) pseudocode is removed from the theorem statement (Part 2). The matvec procedure is stated in a labeled Algorithm block using standard mathematical notation (\(\leftarrow\) for assignment). The theorem claim itself describes the computation mathematically. (b) Part 3 no longer contains the unnecessary justification "\(nr \leq qr\) by assumption \(n < q\)"; instead it identifies \(O(qr)\) and \(O(n\textasciicircum{}2r)\) as the two dominant terms and notes their relative magnitudes depend on whether \(q \gtrless n\textasciicircum{}2\). (c) Part 4 includes the \(O(q(d-1)r)\) term for on-the-fly \(z\_t\) computation in the preconditioner setup cost, consistent with Part 3.

\item **Symmetry of \(K\).** The Kronecker-vec identity \((C \otimes A)\operatorname\{vec\}(B) = \operatorname\{vec\}(ABC\textasciicircum{}\top)\) is applied with \(A = K\). In Lemma 1, we use \((Z \otimes K)\textasciicircum{}\top = Z\textasciicircum{}\top \otimes K\textasciicircum{}\top = Z\textasciicircum{}\top \otimes K\), which requires \(K = K\textasciicircum{}\top\). This holds because \(K\) is a kernel (Gram) matrix, hence symmetric by construction. This is stated in the Notation section.

\item **Projector–masking equivalence.** The identification \(P\_\Omega\operatorname\{vec\}(X) = \operatorname\{vec\}(\mathbf\{1\}\_\Omega \odot X)\) is justified in Lemma 1's proof: \(S\) has columns \(e\_\{(i\_t,m\_t)\}\) (standard basis vectors indexed by column-major vectorization), so \(SS\textasciicircum{}\top\) is diagonal with ones at observed positions. Under the column-major bijection between \(\mathbb\{R\}\textasciicircum{}\{nM\}\) and \(\mathbb\{R\}\textasciicircum{}\{n \times M\}\), this is precisely the Hadamard mask.

\item **Vectorization convention.** All vectorizations use column-major ordering. The Kronecker-vec identity \((C \otimes A)\operatorname\{vec\}(B) = \operatorname\{vec\}(ABC\textasciicircum{}\top)\) holds under this convention. This is stated once in the Notation section and used consistently throughout. Assumption 4 (from the mentor) requires that the selection matrix \(S\) and the index pairs \((i\_t, m\_t)\) are defined consistently with this convention.

\item **Distinctness of observations.** We assume the pairs \((i\_t, m\_t)\) are distinct, so \(\Omega\) is a set with \(|\Omega| = q\). This ensures \(P\_\Omega = SS\textasciicircum{}\top\) is an orthogonal projector (\(P\_\Omega\textasciicircum{}2 = P\_\Omega\)), used in deriving the normal equations. If duplicate observations were allowed, \(SS\textasciicircum{}\top\) would have diagonal entries greater than 1, and the masking interpretation would need modification. The distinctness assumption is stated in the Notation section.

\item **Well-definedness of Cholesky factors.** The preconditioner apply (Lemma 5) requires Cholesky factorizations of \(K + \delta I\_n\) and \(G\_\Omega + \lambda I\_r\). Both are SPD: \(K + \delta I\_n \succ 0\) because \(K \succ 0\) and \(\delta > 0\); \(G\_\Omega + \lambda I\_r \succ 0\) because \(G\_\Omega \succeq 0\) and \(\lambda > 0\). The Cholesky factorizations are therefore well-defined and numerically stable.

\item **No hidden \(O(N)\) costs.** We verify that no step requires \(O(N)\) storage or computation. The full tensor \(\mathcal\{T\}\) and its unfolding \(T\_\{(k)\}\) are never formed; only the \(q\) observed triples \((i\_t, m\_t, y\_t)\) are used. The Khatri–Rao product \(Z \in \mathbb\{R\}\textasciicircum{}\{M \times r\}\) is never formed in full; only the sampled rows \(z\_t\) (precomputed or generated on-the-fly) are accessed. The Kronecker product \(Z \otimes K \in \mathbb\{R\}\textasciicircum{}\{nM \times nr\}\) and the projector \(P\_\Omega \in \mathbb\{R\}\textasciicircum{}\{nM \times nM\}\) are never formed. All explicitly formed matrices have dimensions \(n \times n\), \(n \times r\), or \(r \times r\), none of which is \(O(N)\) under the assumption \(n, r < q \ll N\).
\end{enumerate}

\subsection{Editor Dispatch}
\noindent\textit{Reasoning:} claude\_code is best for logical verification of the matrix proofs; gemini\_api excels at evaluating exposition and tone; codex\_cli can verify standard numerical linear algebra references common in technical documentation.
\begin{itemize}
\item Correctness \& Completeness $\to$ claude\_code
\item Clarity \& Rigor $\to$ gemini\_api
\item Reference Validity $\to$ codex\_cli
\end{itemize}

\subsection{Reviewer Feedback}
\subsubsection{Correctness \& Completeness (by claude\_code)}
\begin{enumerate}
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Lemma 6 / Part 5: The equivalence between PCG on (A, P) and standard CG on the transformed system is asserted without proof or precise citation. At graduate rigor, this non-trivial equivalence (that PCG iterates satisfy w\_m = P\textasciicircum{}\{-1/2\} hat\_w\_m where hat\_w\_m are CG iterates on the transformed system) requires justification.
\\
\textit{Required fix:} Either prove the equivalence by showing the PCG recurrences (preconditioned residuals, search directions, step sizes) map to the standard CG recurrences under the substitution, or provide a precise citation (e.g., Saad, Iterative Methods for Sparse Linear Systems, Theorem 9.1 or Algorithm 9.1).
\\
\textit{Suggestion:} Add a short proof sketch: define the preconditioned residual r̂\_m = P\textasciicircum{}\{-1\} r\_m, show that the PCG update rules for (w\_m, r\_m, p\_m) with M = P correspond to CG rules for (ŵ\_m, r̂\_m, p̂\_m) on the system P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\}, and note this is a standard result with citation.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 1 / after SPD proof: The proof establishes A is SPD but does not explicitly state the immediate consequence that Aw = b has a unique solution. For completeness at graduate level, this should be noted.
\\
\textit{Required fix:} Add one sentence after the SPD conclusion: 'Since A is SPD, it is invertible, and the system Aw = b has a unique solution w* = A\textasciicircum{}\{-1\}b.'
\\
\textit{Suggestion:} Insert this sentence at the end of Part 1, before the PSD kernel remark.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Lemma 6, Chebyshev bound derivation: The intermediate step sqrt((κ+1)²/(κ-1)² - 1) = 2√κ/(κ-1) is used but not shown. While straightforward, omitting intermediate algebra in a key bound weakens the self-contained nature of the proof.
\\
\textit{Required fix:} Show the computation: ((κ+1)/(κ-1))² - 1 = ((κ+1)² - (κ-1)²)/(κ-1)² = 4κ/(κ-1)², so the square root is 2√κ/(κ-1).
\\
\textit{Suggestion:} Add a one-line computation between the display of x + √(x²-1) and the final simplification.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 4, preconditioner setup and apply: The 'no O(N) operation' claim in Part 5 covers computation but storage is only implicitly addressed. The Cholesky factors and all intermediate matrices should be explicitly noted as sub-N in storage.
\\
\textit{Required fix:} Add a brief statement in Part 4 or Part 5 noting that storage is O(n² + r² + nr + qr) (Cholesky factors, working matrices, sampled Khatri-Rao rows), none of which is O(N).
\\
\textit{Suggestion:} Extend Gap Closure Note 11 or add a sentence in Part 4 after the cost analysis: 'Storage for all precomputed objects (Cholesky factors L\_K, L\_G, and cached z\_t vectors) totals O(n² + r² + qr), well below O(N).'
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item The proof treats the Khatri-Rao rows z\_t as either precomputed or generated on-the-fly, but does not discuss the storage implications of precomputing all q rows (O(qr) storage) versus the computational overhead of regenerating them at each matvec. A brief note on this tradeoff would strengthen the practical completeness of the complexity analysis.
\item The PSD kernel remark (end of Part 1) mentions restricting to range(K) as a remedy but does not elaborate on how this affects the dimensions of the system or the complexity bounds. If this case is meant to be supported, it deserves at least a sentence on the modified system dimensions.
\end{itemize}

\subsubsection{Clarity \& Rigor (by gemini\_api)}
\begin{enumerate}
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Notation / Problem Setup: The notation $e_{(i_t, m_t)}$ for the columns of $S$ is slightly informal, as standard basis vectors are typically indexed by a single integer.
\\
\textit{Required fix:} Formalize the vectorization mapping.
\\
\textit{Suggestion:} Explicitly define the column-major bijection $\phi: [n] \times [M] \to [N]$ and denote the columns as $e_{\phi(i_t, m_t)}$ to maintain strict rigor.
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item While the proof of the convergence *rate* (dependent on $\kappa$) is rigorous, the practical utility of the theorem relies on $\kappa$ being small. The 'Heuristic Remark' addresses this qualitatively, but a rigorous peer review might eventually require a spectral bound on $\kappa$ under specific sampling assumptions (e.g., uniform sampling) to fully justify the preconditioner's efficiency.
\end{itemize}

\subsubsection{Reference Validity (by codex\_cli)}
\begin{enumerate}
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Lemma 6 (PCG convergence), and Part 5 convergence claim: The cited CG/PCG Chebyshev bound is applied without stating the standard exact-arithmetic hypothesis. The textbook bound is not generally valid verbatim in finite precision.
\\
\textit{Required fix:} Add an explicit hypothesis that the convergence bound is in exact arithmetic (or, alternatively, replace it with a finite-precision PCG result and cite that result).
\\
\textit{Suggestion:} Insert one sentence before the bound: "In exact arithmetic, standard PCG theory gives..." and keep the finite-precision discussion separate.
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Lemma 6, citation line: "see, e.g., Saad ... Theorem 6.29, or Golub \& Van Loan, §11.3": The citation is not uniquely verifiable: theorem numbering is edition-dependent and two alternative sources are given without precise edition/page/equation mapping to the exact minimax identity used.
\\
\textit{Required fix:} Provide one definitive source with full bibliographic details (authors, title, edition, year) and precise theorem/equation number for the exact bound invoked.
\\
\textit{Suggestion:} Pick one reference (e.g., Saad, 2nd ed.) and cite the exact theorem/equation that states the minimax polynomial bound and the resulting CG error estimate.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Gap Closure Notes \#10 ("Cholesky factorizations are ... numerically stable"): This is an external numerical-analysis claim but currently uncited and broader than needed for correctness; stability quality depends on conditioning and implementation details.
\\
\textit{Required fix:} Either remove the unreferenced stability claim or cite a standard source and qualify it appropriately.
\\
\textit{Suggestion:} If keeping it, cite a standard result (e.g., Higham) and phrase as "backward stable for SPD systems under standard assumptions."
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item If the pipeline expects fully sourced standard identities, add one linear-algebra citation for the Kronecker/vec identities used repeatedly.
\end{itemize}

\subsection{Editor Decision}
\noindent\textbf{Verdict:} \textcolor{orange}{\textbf{right_track}}

\noindent\textbf{Summary:} The revised proof is mathematically correct with sound structure and complete coverage of all five claims. The remaining issues are confined to Lemma 6 (PCG convergence), where the PCG-to-CG equivalence needs justification and the exact-arithmetic hypothesis must be stated, plus several one-sentence completeness additions elsewhere (solution uniqueness, intermediate algebra, storage bounds, citation precision). No structural or correctness problems remain.

\noindent\textbf{Feedback (prover):} Address the following in the next revision:

1. **Lemma 6 — PCG/CG equivalence (major).** Either (a) add a proof sketch showing that the PCG recurrences (with preconditioner P) map to standard CG recurrences under the substitution ŵ = P\textasciicircum{}\{1/2\}w, or (b) provide a single precise citation with full bibliographic details (author, title, edition, year, theorem/algorithm number). Option (b) is sufficient at graduate level. Suggested reference: Saad, Y., *Iterative Methods for Sparse Linear Systems*, 2nd edition, SIAM, 2003, Algorithm 9.1 and the discussion preceding it.

2. **Lemma 6 — Exact-arithmetic hypothesis (major).** Add one sentence before the convergence bound stating explicitly that it holds in exact arithmetic. E.g.: 'In exact arithmetic, the preconditioned CG method satisfies the following error bound.'

3. **Lemma 6 — Precise citation (major).** Replace the current dual citation ('Saad ... Theorem 6.29, or Golub \& Van Loan, §11.3') with one definitive reference including full bibliographic details and the specific theorem/equation number for the minimax Chebyshev bound used.

4. **Part 1 — Solution uniqueness (minor).** After concluding A is SPD, add: 'Since A is SPD, it is invertible, and the system Aw = b has a unique solution w* = A\textasciicircum{}\{-1\}b.'

5. **Lemma 6 — Intermediate algebra (minor).** Show the step: ((κ+1)/(κ−1))² − 1 = ((κ+1)² − (κ−1)²)/(κ−1)² = 4κ/(κ−1)², so √(x²−1) = 2√κ/(κ−1).

6. **Part 4/5 — Storage bounds (minor).** Add one sentence noting that all stored objects (Cholesky factors L\_K ∈ ℝ\textasciicircum{}\{n×n\}, L\_G ∈ ℝ\textasciicircum{}\{r×r\}, cached z\_t vectors, working matrices) total O(n² + r² + qr + nr) storage, none of which is O(N).

7. **Notation — Vectorization bijection (minor).** Define the column-major bijection φ: [n]×[M] → [N] explicitly and write S's columns as e\_\{φ(i\_t,m\_t)\}.

8. **Gap Closure Note 10 — Stability claim (minor).** Either remove the claim that Cholesky factorizations are 'numerically stable' or qualify it with a citation (e.g., Higham, *Accuracy and Stability of Numerical Algorithms*, 2nd ed., SIAM, 2002, Theorem 10.3).

\section{Loop 3}
\noindent Editor verdict for this loop: \textcolor{orange}{\textbf{right_track}}

\subsection{Researcher}
\subsection{Relevant Theorems}

\begin{enumerate}
\item **PCG–CG equivalence (Saad, *Iterative Methods for Sparse Linear Systems*, 2nd ed., SIAM, 2003, Algorithm 9.1 and §9.2).** Let $A, P \in \mathbb{R}^{n \times n}$ be SPD. The preconditioned conjugate gradient method (PCG) applied to $Ax = b$ with preconditioner $P$ generates iterates $x_m$ that are related to the standard CG iterates $\hat{x}_m$ on the transformed system $\hat{A}\hat{x} = \hat{b}$ (where $\hat{A} = P^{-1/2}AP^{-1/2}$, $\hat{x} = P^{1/2}x$, $\hat{b} = P^{-1/2}b$) by $x_m = P^{-1/2}\hat{x}_m$. Specifically, the PCG recurrences — with preconditioned residual $\hat{r}_m = P^{-1}r_m$, search direction update $p_{m+1} = \hat{r}_{m+1} + \beta_m p_m$ where $\beta_m = \langle r_{m+1}, \hat{r}_{m+1}\rangle / \langle r_m, \hat{r}_m \rangle$, and step size $\alpha_m = \langle r_m, \hat{r}_m \rangle / \langle p_m, Ap_m \rangle$ — are algebraically equivalent (in exact arithmetic) to the standard CG recurrences applied to $\hat{A}\hat{x} = \hat{b}$. This equivalence is the basis for transferring the CG convergence bound to the PCG setting.

\item **CG Chebyshev convergence bound (Saad, *Iterative Methods for Sparse Linear Systems*, 2nd ed., SIAM, 2003, Theorem 6.11.** In exact arithmetic, the conjugate gradient method applied to $Ax = b$ with $A$ SPD produces iterates satisfying:
\end{enumerate}
   $$\frac{\|e_m\|_A}{\|e_0\|_A} \leq \min_{p \in \mathcal{P}_m,\; p(0)=1} \max_{\mu \in [\lambda_{\min}(A), \lambda_{\max}(A)]} |p(\mu)|.$$
   The minimax value on the right equals $1/T_m\!\left(\frac{\lambda_{\max} + \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}}\right)$, where $T_m$ is the Chebyshev polynomial of degree $m$ of the first kind (Theorem 6.11, equation (6.63) in Saad). Combined with the bound $T_m(x) \geq \frac{1}{2}(x + \sqrt{x^2 - 1})^m$ for $x \geq 1$ (which follows from $T_m(x) = \cosh(m\,\mathrm{arccosh}(x))$ and $\cosh(\theta) \geq \frac{1}{2}e^\theta$), this yields:
   $$\|e_m\|_A \leq 2\left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^m \|e_0\|_A, \quad \kappa = \kappa(A).$$
   \textbf{Hypothesis:} This bound holds in exact arithmetic. In finite-precision arithmetic, the CG/PCG iterates can deviate due to loss of orthogonality among Lanczos vectors, though the method typically still converges; see Greenbaum (\textit{Iterative Methods for Solving Linear Systems}, SIAM, 1997, Chapter 4) for the finite-precision theory.

\begin{enumerate}
\item **Kronecker-vec identity (Horn \& Johnson, *Topics in Matrix Analysis*, Cambridge, 1991, §4.3; also Magnus \& Neudecker, *Matrix Differential Calculus*, 3rd ed., Wiley, 2019, Chapter 2).** For conformable matrices $A$, $B$, $C$: $(C^\top \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC)$, equivalently $(C \otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)$. Holds under column-major vectorization convention.

\item **Kronecker product inverse (Horn \& Johnson, *Topics in Matrix Analysis*, §4.2).** If $A$ and $B$ are invertible, then $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$.

\item **Backward stability of Cholesky factorization (Higham, *Accuracy and Stability of Numerical Algorithms*, 2nd ed., SIAM, 2002, Theorem 10.3).** For an $n \times n$ SPD matrix $A$, the computed Cholesky factor $\hat{L}$ satisfies $\hat{L}\hat{L}^\top = A + \Delta A$ where $\|\Delta A\|_2 \leq c_n \epsilon_{\mathrm{mach}} \|A\|_2$ for a modest constant $c_n$ depending on $n$. The factorization is backward stable for SPD systems under standard floating-point assumptions.

\item **Representer theorem for RKHS regularization (Schölkopf, Herbrich, and Smola, "A Generalized Representer Theorem," *COLT*, 2001).** Justifies the parametrization $A_k = KW$ for the infinite-dimensional mode factor.

\end{enumerate}
\subsection{Key Definitions}

\begin{itemize}
\item **Mode-$k$ unfolding and column-major vectorization bijection.** Let $T_{(k)} \in \mathbb{R}^{n \times M}$ denote the mode-$k$ matricization of $\mathcal{T}$. Define the column-major bijection $\varphi: [n] \times [M] \to [N]$ by $\varphi(i, m) = i + n(m-1)$. All vectorizations use this convention: $\operatorname{vec}(X)_{\varphi(i,m)} = X_{i,m}$. The selection matrix $S \in \mathbb{R}^{N \times q}$ has columns $e_{\varphi(i_t, m_t)}$ for $t = 1, \ldots, q$.

\item **Observed set, mask, and projector.** $\Omega \subset [n] \times [M]$ is a set (not multiset) of observed index pairs, $|\Omega| = q$, with observations $(i_t, m_t, y_t)$ for $t = 1, \ldots, q$ where $y_t = (T_{(k)})_{i_t, m_t}$ and all $(i_t, m_t)$ are distinct. The binary mask $\mathbf{1}_\Omega \in \{0,1\}^{n \times M}$ has $(\mathbf{1}_\Omega)_{i,m} = 1$ iff $(i,m) \in \Omega$. The masked unfolding is $T_\Omega := \mathbf{1}_\Omega \odot T_{(k)}$. The projector $P_\Omega := SS^\top$ satisfies $P_\Omega \operatorname{vec}(X) = \operatorname{vec}(\mathbf{1}_\Omega \odot X)$ and $P_\Omega^2 = P_\Omega$ (orthogonal projector, since observations are distinct).

\item **Optimization objective.** The mode-$k$ subproblem minimizes:
\end{itemize}
  $$\mathcal{L}(w) = \frac{1}{2}\|P_\Omega(\operatorname{vec}(T_{(k)}) - (Z \otimes K)w)\|_2^2 + \frac{\lambda}{2} w^\top (I_r \otimes K) w.$$
  Setting $\nabla_w \mathcal{L} = 0$ yields the normal equations $Aw = b$.

\begin{itemize}
\item **System matrix and RHS.** $A = (Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda(I_r \otimes K)$; $b = (Z \otimes K)^\top P_\Omega \operatorname{vec}(T_{(k)}) = \operatorname{vec}(KB)$ where $B = T_\Omega Z$.

\item **Sparse MTTKRP.** $B = T_\Omega Z \in \mathbb{R}^{n \times r}$ with $B_{i,:} = \sum_{\{t: i_t = i\}} y_t z_t$.

\item **Sampled Gram matrix.** $G_\Omega = \sum_{t=1}^q z_t z_t^\top \in \mathbb{R}^{r \times r}$.

\item **Kronecker preconditioner.** $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, $\delta > 0$.

\item **Matrix-free operator.** For $V \in \mathbb{R}^{n \times r}$: $\mathcal{A}(V) = K[(\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V]$, so $A\operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))$.

\end{itemize}
\subsection{Proof Strategies}

\begin{enumerate}
\item **Problem Setup first.** State the optimization objective $\mathcal{L}(w)$ before anything else. Derive the normal equations $Aw = b$ by computing $\nabla_w \mathcal{L} = 0$, using $P_\Omega = P_\Omega^\top = P_\Omega^2$. This makes the form of both $A$ and $b$ (involving only observed data) self-evident from first principles. After establishing $A \succ 0$, note explicitly that the system has a unique solution $w_* = A^{-1}b$.

\item **Kronecker-vec operator identity (Lemma 1).** Expand each term of $A\operatorname{vec}(V)$ using the Kronecker-vec identity, the projector-masking equivalence, and $K = K^\top$. This yields $\operatorname{vec}(\mathcal{A}(V))$.

\item **Sampled accumulation (Lemma 2).** Show the accumulation loop computes $C = (\mathbf{1}_\Omega \odot (UZ^\top))Z$ exactly by rewriting the matrix product row-by-row and using the enumeration of $\Omega$.

\item **Sparse RHS (Lemma 3).** Show $B = T_\Omega Z$ is computed from observed triples in $O(qr)$, then $b = \operatorname{vec}(KB)$ in $O(n^2 r)$. Include consistency check against the question's notation.

\item **SPD property (Lemma 4).** Decompose $x^\top Ax$ into a nonneg data-fit term plus $\lambda \|K^{1/2}V\|_F^2 > 0$, using $K \succ 0$. State uniqueness of solution as a corollary.

\item **Preconditioner apply (Lemma 5).** Use $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$ and the Kronecker-vec identity to decompose $P^{-1}\operatorname{vec}(R)$ into two sets of triangular solves. Analyze setup cost ($O(qr^2 + n^3 + r^3)$, plus $O(q(d-1)r)$ if $z_t$ on-the-fly) and per-apply cost ($O(n^2r + nr^2)$). State storage: $O(n^2 + r^2 + qr + nr)$, none $O(N)$.

\item **PCG convergence (Lemma 6).** Three sub-steps:
\end{enumerate}
\begin{itemize}
\item **(a) PCG–CG equivalence.** Cite Saad (2003), Algorithm 9.1 and §9.2. State that the equivalence holds in exact arithmetic. Either provide a proof sketch (showing the substitution $\hat{w} = P^{1/2}w$ maps PCG recurrences to CG recurrences) or cite the precise theorem.
\item **(b) Error transformation.** Derive $\hat{e}_m = P^{1/2}e_m$ from $\hat{w}_m = P^{1/2}w_m$, $\hat{w}_* = P^{1/2}w_*$. Then $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$ by expanding $\hat{e}_m^\top \hat{A}\hat{e}_m = e_m^\top Ae_m$.
\item **(c) Chebyshev bound.** State the minimax identity $\min_{p(0)=1} \max_{\mu} |p(\mu)| = 1/T_m((\lambda_{\max}+\lambda_{\min})/(\lambda_{\max}-\lambda_{\min}))$ citing Saad (2003), Theorem 6.11. Then bound $T_m(x)$ from below using $T_m(x) = \cosh(m\,\mathrm{arccosh}(x)) \geq \frac{1}{2}(x + \sqrt{x^2 - 1})^m$. Show the intermediate algebra: $x = (\kappa+1)/(\kappa-1)$, $x^2 - 1 = 4\kappa/(\kappa-1)^2$, $\sqrt{x^2-1} = 2\sqrt{\kappa}/(\kappa-1)$, $x + \sqrt{x^2-1} = (\sqrt{\kappa}+1)/(\sqrt{\kappa}-1)$.

\end{itemize}
\begin{enumerate}
\item **Total complexity and no-$O(N)$ verification.** Sum per-iteration cost ($O(qr + n^2r + nr^2)$) times $m$ iterations, plus one-time setup. Explicitly state storage bounds. Verify no object of dimension $M$ or $N$ is formed.

\end{enumerate}
\subsection{Gaps and Concerns}

\begin{enumerate}
\item **Editor feedback item 1 (PCG–CG equivalence): now addressed.** The precise reference is Saad, Y., *Iterative Methods for Sparse Linear Systems*, 2nd ed., SIAM, 2003, Algorithm 9.1 and the discussion in §9.2. The equivalence holds in exact arithmetic: the PCG recurrences with $M = P$ produce iterates satisfying $\hat{w}_m = P^{1/2}w_m$ where $\hat{w}_m$ are the CG iterates on $\hat{A}\hat{w} = \hat{b}$. The prover should either cite this precisely or give the one-paragraph proof sketch showing the substitution maps the recurrences.

\item **Editor feedback item 2 (exact-arithmetic hypothesis): must be stated.** The Chebyshev-based PCG bound holds in exact arithmetic. In finite precision, loss of orthogonality among Lanczos/CG vectors can cause delayed convergence, though practical convergence is typically observed. The prover should add: "In exact arithmetic, the preconditioned CG method satisfies the following error bound." For the finite-precision caveat, optionally cite Greenbaum (1997).

\item **Editor feedback item 3 (precise citation for minimax bound): now addressed.** Use: Saad, Y., *Iterative Methods for Sparse Linear Systems*, 2nd ed., SIAM, 2003, Theorem 6.11, equation (6.63). This states the minimax polynomial bound on $[\lambda_{\min}, \lambda_{\max}]$ with $p(0) = 1$ and the Chebyshev solution.

\item **Editor feedback item 4 (solution uniqueness): one sentence needed.** After proving $A \succ 0$, state: "Since $A$ is SPD, it is invertible, and the system $Aw = b$ has a unique solution $w_* = A^{-1}b$."

\item **Editor feedback item 5 (intermediate algebra in Chebyshev bound): show the step.** The computation: $\left(\frac{\kappa+1}{\kappa-1}\right)^2 - 1 = \frac{(\kappa+1)^2 - (\kappa-1)^2}{(\kappa-1)^2} = \frac{4\kappa}{(\kappa-1)^2}$, so $\sqrt{x^2 - 1} = \frac{2\sqrt{\kappa}}{\kappa - 1}$.

\item **Editor feedback item 6 (storage bounds): one sentence needed.** All stored objects — Cholesky factors $L_K \in \mathbb{R}^{n \times n}$, $L_G \in \mathbb{R}^{r \times r}$, cached $z_t$ vectors ($q$ vectors of length $r$), working matrices of size $n \times r$ — total $O(n^2 + r^2 + qr + nr)$ storage, none of which is $O(N)$.

\item **Editor feedback item 7 (vectorization bijection): define $\varphi$ explicitly.** Define $\varphi(i, m) = i + n(m-1)$ (column-major) and write $S$'s columns as $e_{\varphi(i_t, m_t)}$. This removes the slight informality in the notation.

\item **Editor feedback item 8 (Cholesky stability claim): qualify or cite.** Either remove the unqualified "numerically stable" claim or cite Higham, *Accuracy and Stability of Numerical Algorithms*, 2nd ed., SIAM, 2002, Theorem 10.3, which establishes backward stability of Cholesky for SPD matrices.

\item **Preconditioner quality remains heuristic.** The theorem's convergence bound is parameterized by $\kappa = \kappa(P^{-1/2}AP^{-1/2})$ without bounding $\kappa$ in terms of problem parameters. This is appropriate — a formal spectral-equivalence bound would require a specific sampling model. The heuristic remark should remain clearly labeled as such and not be treated as a theorem claim.

\item **On-the-fly $z_t$ cost consistency.** Ensure the $O(q(d-1)r)$ term for on-the-fly computation of Khatri-Rao rows appears consistently in: (a) per-matvec cost (Part 3), (b) preconditioner setup cost (Part 4, for forming $G_\Omega$), and (c) total complexity (Part 5).

\item **PSD kernel remark.** The remark about restricting to $\operatorname{range}(K)$ when $K$ is only PSD is qualitative. If included, note briefly that restricting to $\operatorname{range}(K)$ reduces the system dimension from $nr$ to $\operatorname{rank}(K) \cdot r$ but does not change the structure of the proof. This is a secondary concern and need not be formalized.
\end{enumerate}

\subsection{Mentor}
\subsection{Definitions}
Let \(k\) be the target mode, \(n:=n\_k\), \(M:=\prod\_\{i\neq k\} n\_i\), \(N=nM\), and rank \(r\).

Let \(\Omega\subset [n]\times[M]\) be the observed index set in mode-\(k\) unfolding, \(|\Omega|=q\), with observed triples \((i\_t,m\_t,y\_t)\), \(t=1,\dots,q\), where \(y\_t=T\_\{i\_t,m\_t\}\).

Define
\[
Z:=A\_d\odot\cdots\odot A\_\{k+1\}\odot A\_\{k-1\}\odot\cdots\odot A\_1\in\mathbb\{R\}\textasciicircum{}\{M\times r\},
\quad
z\_t:=Z\_\{m\_t,:\}\in\mathbb\{R\}\textasciicircum{}r.
\]
Unknown is \(W\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\), \(w:=\mathrm\{vec\}(W)\), and \(A\_k=KW\).

Define \(P\_\Omega:=SS\textasciicircum{}\top\) (projector selecting observed entries in \(\mathrm\{vec\}(T)\)) and linear system
\[
A w=b,\quad
A=(Z\otimes K)\textasciicircum{}\top P\_\Omega (Z\otimes K)+\lambda(I\_r\otimes K),\quad
b=(I\_r\otimes K)\mathrm\{vec\}(B),\ B=TZ.
\]

Define mask matrix \(\mathbf\{1\}\_\Omega\in\\{0,1\\}\textasciicircum{}\{n\times M\}\). For \(V\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\),
\[
\mathcal A(V):=
K\!\left((\mathbf\{1\}\_\Omega\odot (KVZ\textasciicircum{}\top))Z+\lambda V\right),
\quad
A\,\mathrm\{vec\}(V)=\mathrm\{vec\}(\mathcal A(V)).
\]

\subsection{Formal Statement}
\textbf{Theorem (Matrix-free PCG for RKHS mode-\(k\) subproblem).} Under the assumptions in Section \textbf{Assumptions}:

\begin{enumerate}
\item \(A\) is SPD, so PCG is well-defined.
\item For any \(V\), \(A\mathrm\{vec\}(V)\) can be computed exactly without forming \(A\), \(Z\otimes K\), or any \(N\)-scale object via:
\end{enumerate}
\[
U=KV,\ \ 
\alpha\_t=\langle U\_\{i\_t,:\},z\_t\rangle,\ \ 
C\_\{i\_t,:\}\mathrel\{+\}=\alpha\_t z\_t,\ \ 
Y=KC+\lambda U,
\]
return \(\mathrm\{vec\}(Y)\).
\begin{enumerate}
\item The per-matvec complexity is \(O(qr+n\textasciicircum{}2r)\) (plus \(O(q(d-1)r)\) if each \(z\_t\) is generated on-the-fly from CP factors).
\item With preconditioner
\end{enumerate}
\[
P=((G\_\Omega+\lambda I\_r)\otimes (K+\delta I\_n)),\quad
G\_\Omega:=\sum\_\{t=1\}\textasciicircum{}q z\_tz\_t\textasciicircum{}\top,\ \delta>0,
\]
each \(P\textasciicircum{}\{-1\}\)-apply costs \(O(n\textasciicircum{}2r+nr\textasciicircum{}2)\) after setup \(O(qr\textasciicircum{}2+n\textasciicircum{}3+r\textasciicircum{}3)\).
\begin{enumerate}
\item PCG iterates satisfy
\end{enumerate}
\[
\|e\_m\|\_A\le 2\Big(\frac\{\sqrt\kappa-1\}\{\sqrt\kappa+1\}\Big)\textasciicircum{}m\|e\_0\|\_A,\quad
\kappa=\kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\}),
\]
hence total cost
\[
O\!\left(m(qr+n\textasciicircum{}2r+nr\textasciicircum{}2)+qr\textasciicircum{}2+n\textasciicircum{}3+r\textasciicircum{}3\right),
\]
with no \(O(N)\) operation.

\subsection{Assumptions}
\begin{enumerate}
\item \(\lambda>0\).
\item \(K\succ 0\) for strict SPD proof of \(A\); if original kernel is only PSD, either add nugget to the model kernel or solve on \(\mathrm\{range\}(K)\).
\item \(n,r<q\ll N\), and observed tuples \((i\_t,m\_t,y\_t)\) are directly available.
\item Indexing conventions for unfolding, vectorization, and \(S\) are consistent.
\item For preconditioner quality claims (small \(\kappa\)), observation pattern is not pathologically imbalanced across \(i\_t\) and rank directions.

\end{enumerate}
\subsection{Notation}
\begin{itemize}
\item \(n=n\_k\): size of mode \(k\).
\item \(M=\prod\_\{i\neq k\} n\_i\): product of non-\(k\) dimensions.
\item \(N=nM\): total entries.
\item \(q=|\Omega|\): number of observed entries.
\item \(Z\in\mathbb\{R\}\textasciicircum{}\{M\times r\}\): Khatri-Rao product of fixed factors.
\item \(K\in\mathbb\{R\}\textasciicircum{}\{n\times n\}\): kernel matrix.
\item \(W\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\), \(w=\mathrm\{vec\}(W)\): unknown.
\item \(B=TZ\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\): sparse MTTKRP result.
\item \(z\_t=Z\_\{m\_t,:\}\): sampled Khatri-Rao row for observation \(t\).
\item \(P\_\Omega=SS\textasciicircum{}\top\): selection projector in vectorized unfolding space.
\item \(G\_\Omega=\sum\_t z\_tz\_t\textasciicircum{}\top\in\mathbb\{R\}\textasciicircum{}\{r\times r\}\): sampled rank-Gram.

\end{itemize}
\subsection{High-Level Strategy}
\begin{enumerate}
\item Rewrite the Hessian action as a matrix operator \(\mathcal A(V)\) using \((C\otimes A)\mathrm\{vec\}(B)=\mathrm\{vec\}(AB C\textasciicircum{}\top)\).
\item Implement \(\mathcal A(V)\) from observed entries only, via rowwise accumulation \(C\_\{i\_t,:\} \!+\!= \alpha\_t z\_t\), never forming \(P\_\Omega\), \(Z\otimes K\), or dense \(T\).
\item Compute RHS sparsely: \(B\_\{i\_t,:\}\!+\!=y\_t z\_t\), then \(b=\mathrm\{vec\}(KB)\).
\item Run PCG on \(Aw=b\) with Kronecker preconditioner \(P\) that is cheap to invert by two small Cholesky-based solves.
\item Prove complexity by summing matvec cost, preconditioner-apply cost, and one-time setup.

\end{enumerate}
\subsection{Key Lemmas}
\begin{enumerate}
\item **Kronecker-vec masked operator lemma:** \(A\,\mathrm\{vec\}(V)=\mathrm\{vec\}(\mathcal A(V))\).
\item **Sampled accumulation lemma:** \(C=(\mathbf\{1\}\_\Omega\odot(KVZ\textasciicircum{}\top))Z\) is computed exactly by the \(q\)-sample loop in \(O(qr)\).
\item **Sparse RHS lemma:** \(B=TZ\) is computed from observations in \(O(qr)\), then \(b\) in \(O(n\textasciicircum{}2r)\).
\item **SPD lemma:** if \(K\succ0,\lambda>0\), then \(A\succ0\) because
\end{enumerate}
\[
x\textasciicircum{}\top A x=\|P\_\Omega\textasciicircum{}\{1/2\}(Z\otimes K)x\|\_2\textasciicircum{}2+\lambda\|(I\_r\otimes K\textasciicircum{}\{1/2\})x\|\_2\textasciicircum{}2>0.
\]
\begin{enumerate}
\item **Preconditioner-apply lemma:** \(P\textasciicircum{}\{-1\}\mathrm\{vec\}(R)\) equals two solves:
\end{enumerate}
\[
(K+\delta I)Y=R,\quad X(G\_\Omega+\lambda I)=Y,\quad \mathrm\{vec\}(X)=P\textasciicircum{}\{-1\}\mathrm\{vec\}(R).
\]
\begin{enumerate}
\item **PCG convergence lemma:** standard SPD PCG bound with \(\kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\})\).

\end{enumerate}
\subsection{Dependency Graph}
\begin{enumerate}
\item Definitions + Notation \(\rightarrow\) Lemma 1.
\item Lemma 1 \(\rightarrow\) Lemma 2 (matvec algorithm correctness).
\item Definitions \(\rightarrow\) Lemma 3 (RHS correctness/cost).
\item Assumptions 1-2 \(\rightarrow\) Lemma 4 (SPD of \(A\)).
\item Definitions \(\rightarrow\) Lemma 5 (preconditioner form and apply rule).
\item Lemma 4 + Lemma 5 + Lemma 6 \(\rightarrow\) PCG convergence claim.
\item Lemma 2 + Lemma 3 + Lemma 5 \(\rightarrow\) total complexity claim in theorem.

\end{enumerate}
\subsection{Risky Steps}
\begin{enumerate}
\item **PSD kernel issue:** if \(K\) is singular, \(A\) may be singular despite \(\lambda>0\); this must be handled explicitly.
\item **Index consistency:** mismatch between unfolding order, \(S\), and sampled \((i\_t,m\_t)\) invalidates the operator.
\item **Preconditioner quality claim:** spectral-equivalence constants require sampling-balance assumptions; do not claim uniformly good \(\kappa\) without them.
\item **Hidden \(M\)-scale cost:** forming full \(Z\) is infeasible when \(M\) is huge; use only sampled rows \(z\_t\) (cached or on-demand).
\item **Numerical stability:** very small \(\lambda\) or ill-conditioned \(K\) can cause PCG stagnation; monitor relative preconditioned residual and cap iterations.
\end{enumerate}

\subsection{Proof}
\subsection{Complete Proof}

We prove the theorem in five parts corresponding to the five claims. Throughout, we use the notation and assumptions established by the mentor. We adopt column-major vectorization: define the bijection $\varphi: [n]\times[M]\to[N]$ by $\varphi(i,m) = (m-1)n + i$, so that $\operatorname{vec}(X)_{\varphi(i,m)} = X_{i,m}$ for any $X\in\mathbb{R}^{n\times M}$. The selection matrix $S\in\mathbb{R}^{N\times q}$ has columns $e_{\varphi(i_t,m_t)}$ for $t=1,\dots,q$, and $P_\Omega = SS^\top$ is the orthogonal projector onto the coordinates corresponding to observed entries.

\textbf{Part 1 (SPD and well-definedness).} We show $A\succ 0$. Take any nonzero $x\in\mathbb{R}^{nr}$. Then

$$x^\top A x = x^\top (Z\otimes K)^\top P_\Omega (Z\otimes K) x + \lambda\, x^\top (I_r\otimes K) x.$$

The first term is $\|P_\Omega^{1/2}(Z\otimes K)x\|_2^2 \ge 0$. For the second term, write $x = \operatorname{vec}(V)$ for $V\in\mathbb{R}^{n\times r}$. Then

$$x^\top (I_r\otimes K) x = \operatorname{vec}(V)^\top \operatorname{vec}(KV) = \operatorname{tr}(V^\top K V) = \sum_{j=1}^r v_j^\top K v_j,$$

where $v_j$ is the $j$-th column of $V$. Since $K\succ 0$ (Assumption 2), each $v_j^\top K v_j \ge \lambda_{\min}(K)\|v_j\|^2 > 0$ whenever $v_j\neq 0$. Since $x\neq 0$, at least one $v_j\neq 0$, so $\operatorname{tr}(V^\top KV)>0$. With $\lambda>0$ (Assumption 1), we get $x^\top Ax \ge \lambda\,\lambda_{\min}(K)\|x\|^2 > 0$. Hence $A\succ 0$.

Since $A$ is SPD, it is invertible, and the system $Aw=b$ has a unique solution $w^*=A^{-1}b$. The PCG method applied to an SPD system is well-defined (the denominators $p_j^\top A p_j$ in the CG recurrence are strictly positive for all nonzero search directions).

\textbf{Part 2 (Matrix-free matvec via $\mathcal{A}$).} We claim $A\,\operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))$ and that $\mathcal{A}(V)$ is computed without forming any $N$-scale object. The derivation is given in Lemma 1 below, and the sparse accumulation algorithm is verified in Lemma 2. The procedure is:

\begin{enumerate}
\item Compute $U = KV \in \mathbb{R}^{n\times r}$ (cost $O(n^2 r)$).
\item Initialize $C = 0_{n\times r}$. For each $t=1,\dots,q$: compute $\alpha_t = \langle U_{i_t,:}, z_t\rangle$ (cost $O(r)$), then update $C_{i_t,:} \mathrel{+}= \alpha_t\, z_t$ (cost $O(r)$). Total loop cost: $O(qr)$.
\item Compute $Y = KC + \lambda U$ (cost $O(n^2 r)$).
\item Return $\operatorname{vec}(Y)$.

\end{enumerate}
This computes $A\,\operatorname{vec}(V)$ exactly (Lemmas 1–2). No matrix of dimension $N$ or $M$ is formed; only $n\times r$, $n\times n$, and scalar operations appear, plus indexed access to the $q$ sampled rows $z_t$.

\textbf{Part 3 (Complexity).} The matvec cost is $O(n^2 r)$ for two kernel multiplications plus $O(qr)$ for the accumulation loop, giving $O(qr + n^2 r)$ per matvec. If each $z_t$ is not cached but generated on-the-fly from the $d-1$ CP factor matrices (each row lookup is $O(1)$ per factor, Hadamard product over $d-1$ factors is $O((d-1)r)$), the loop cost becomes $O(q(d-1)r)$, which replaces the $O(qr)$ term when $d>2$.

The RHS $b = \operatorname{vec}(KB)$ is computed in $O(qr + n^2 r)$ by Lemma 3.

\textbf{Part 4 (Preconditioner).} Define $P = (G_\Omega + \lambda I_r)\otimes (K+\delta I_n)$ with $\delta>0$. Since $G_\Omega = \sum_t z_t z_t^\top \succeq 0$ and $\lambda>0$, we have $G_\Omega+\lambda I_r \succ 0$. Since $K\succ 0$ and $\delta>0$, $K+\delta I_n\succ 0$. The Kronecker product of two SPD matrices is SPD (eigenvalues are pairwise products of eigenvalues, all positive), so $P\succ 0$.

Given $\operatorname{vec}(R)$, we compute $P^{-1}\operatorname{vec}(R)$ by Lemma 5:

\begin{enumerate}
\item Solve $(K+\delta I_n)Y = R$ for $Y\in\mathbb{R}^{n\times r}$ (cost $O(n^2 r)$ using a precomputed Cholesky factor $L_K$ of $K+\delta I_n$).
\item Solve $Y = X(G_\Omega+\lambda I_r)^\top$ for $X$, i.e., $X = Y(G_\Omega+\lambda I_r)^{-1}$ (cost $O(nr^2)$ using a precomputed Cholesky factor $L_G$ of $G_\Omega+\lambda I_r$).
\item Return $\operatorname{vec}(X)$.

\end{enumerate}
One-time setup: form $G_\Omega = \sum_t z_t z_t^\top$ in $O(qr^2)$; Cholesky of $K+\delta I_n$ in $O(n^3)$; Cholesky of $G_\Omega+\lambda I_r$ in $O(r^3)$. Per-apply cost: $O(n^2 r + nr^2)$.

\textbf{Storage.} All stored objects are: $L_K\in\mathbb{R}^{n\times n}$, $L_G\in\mathbb{R}^{r\times r}$, cached $z_t$ vectors ($q$ vectors in $\mathbb{R}^r$), and working matrices $U,C,Y\in\mathbb{R}^{n\times r}$. The total storage is $O(n^2 + r^2 + qr + nr)$, none of which is $O(N)$.

\textbf{Part 5 (PCG convergence and total cost).} In exact arithmetic, the preconditioned CG method satisfies the following error bound. By Lemma 6 (proved below with reference to Saad, Y., \textit{Iterative Methods for Sparse Linear Systems}, 2nd edition, SIAM, 2003, Theorem 9.4.12), after $m$ PCG iterations:

$$\|e_m\|_A \le 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m \|e_0\|_A, \quad \kappa = \kappa(P^{-1/2}AP^{-1/2}).$$

Each PCG iteration requires one matvec ($O(qr+n^2r)$) and one preconditioner apply ($O(n^2r+nr^2)$), plus $O(nr)$ for vector operations. After $m$ iterations, the total cost is

$$O\!\left(m(qr + n^2 r + nr^2) + qr^2 + n^3 + r^3\right),$$

where the last three terms are one-time setup. No $O(N)$ operation appears anywhere. $\blacksquare$

---

\subsection{Lemma Proofs}

\textbf{Lemma 1 (Kronecker-vec masked operator).} \textit{Claim:} $A\,\operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))$, where $\mathcal{A}(V) = K\!\left((\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V\right)$.

\textit{Proof.} Write $A = (Z\otimes K)^\top P_\Omega (Z\otimes K) + \lambda(I_r\otimes K)$. The regularization term gives $\lambda(I_r\otimes K)\operatorname{vec}(V) = \lambda\operatorname{vec}(KV)$. For the data-fit term, let $x=\operatorname{vec}(V)$.

First, $(Z\otimes K)x = (Z\otimes K)\operatorname{vec}(V) = \operatorname{vec}(KVZ^\top)$, using the standard identity $(C\otimes A)\operatorname{vec}(B) = \operatorname{vec}(ABC^\top)$ with $A\leftarrow K$, $B\leftarrow V$, $C\leftarrow Z$.

The matrix $KVZ^\top\in\mathbb{R}^{n\times M}$, and $P_\Omega\operatorname{vec}(KVZ^\top)$ zeros out all entries not in $\Omega$. Under column-major vectorization with the bijection $\varphi(i,m)=(m-1)n+i$, the projector $P_\Omega$ acts entrywise: $(P_\Omega \operatorname{vec}(X))_{\varphi(i,m)} = X_{i,m}$ if $(i,m)\in\Omega$ and $0$ otherwise. In matrix form, this is $\operatorname{vec}(\mathbf{1}_\Omega\odot(KVZ^\top))$.

So $P_\Omega(Z\otimes K)x = \operatorname{vec}(\mathbf{1}_\Omega\odot(KVZ^\top))$.

Next, $(Z\otimes K)^\top \operatorname{vec}(F) = \operatorname{vec}(K^\top F Z) = \operatorname{vec}(KFZ)$ for $F\in\mathbb{R}^{n\times M}$ (since $K$ is symmetric, $K=K^\top$), again by the identity $(C\otimes A)^\top \operatorname{vec}(F) = (Z^\top\otimes K^\top)\operatorname{vec}(F)=\operatorname{vec}(K^\top F Z)$.

Setting $F = \mathbf{1}_\Omega\odot(KVZ^\top)$:

$$(Z\otimes K)^\top P_\Omega (Z\otimes K)\operatorname{vec}(V) = \operatorname{vec}\!\left(K(\mathbf{1}_\Omega\odot(KVZ^\top))Z\right).$$

Combining both terms:

$$A\operatorname{vec}(V) = \operatorname{vec}\!\left(K(\mathbf{1}_\Omega\odot(KVZ^\top))Z + \lambda KV\right) = \operatorname{vec}\!\left(K\!\left[(\mathbf{1}_\Omega\odot(KVZ^\top))Z + \lambda V\right]\right) = \operatorname{vec}(\mathcal{A}(V)).\quad\blacksquare$$

---

\textbf{Lemma 2 (Sampled accumulation).} \textit{Claim:} The matrix $C := (\mathbf{1}_\Omega\odot(KVZ^\top))Z$ is computed exactly by: initialize $C=0$; for $t=1,\dots,q$: $\alpha_t = \langle U_{i_t,:}, z_t\rangle$, $C_{i_t,:}\mathrel{+}= \alpha_t z_t$, where $U=KV$. Cost: $O(qr)$.

\textit{Proof.} Let $F = KVZ^\top = UZ^\top\in\mathbb{R}^{n\times M}$. Then $F_{i,m} = \langle U_{i,:}, Z_{m,:}\rangle$. The masked matrix $\tilde F := \mathbf{1}_\Omega\odot F$ satisfies $\tilde F_{i,m} = F_{i,m}$ if $(i,m)\in\Omega$ and $0$ otherwise.

Now $C = \tilde F Z$, so $C_{i,j} = \sum_{m=1}^M \tilde F_{i,m} Z_{m,j}$. Since $\tilde F_{i,m}=0$ unless $(i,m)\in\Omega$, this sum reduces to contributions only from observed entries:

$$C_{i,:} = \sum_{\{t:(i_t,m_t)\in\Omega,\, i_t=i\}} F_{i,m_t}\, z_t^\top = \sum_{\{t:i_t=i\}} \langle U_{i,:}, z_t\rangle\, z_t^\top.$$

This is exactly what the loop computes: for each observation $t$, the contribution $\alpha_t z_t^\top$ (a row vector) is added to row $i_t$ of $C$. Each iteration costs $O(r)$ for the inner product and $O(r)$ for the row update, giving $O(qr)$ total. The result is exact (no approximation). $\blacksquare$

---

\textbf{Lemma 3 (Sparse RHS).} \textit{Claim:} $B = T_{(k)}Z \in\mathbb{R}^{n\times r}$ is computed from observations in $O(qr)$, and $b=\operatorname{vec}(KB)$ in $O(n^2 r)$.

\textit{Proof.} The mode-$k$ unfolding $T_{(k)}\in\mathbb{R}^{n\times M}$ is only observed at entries in $\Omega$. We have

$$B_{i,:} = \sum_{m=1}^M (T_{(k)})_{i,m}\, Z_{m,:} = \sum_{\{t:i_t=i\}} y_t\, z_t^\top,$$

since $(T_{(k)})_{i,m}=0$ for unobserved entries (or equivalently, the unobserved entries contribute nothing because we only sum over $\Omega$). Initialize $B=0$; for $t=1,\dots,q$: $B_{i_t,:}\mathrel{+}= y_t z_t^\top$, cost $O(r)$ per iteration, total $O(qr)$.

Then $b = (I_r\otimes K)\operatorname{vec}(B) = \operatorname{vec}(KB)$, which costs $O(n^2 r)$ via a matrix-matrix multiply. Total: $O(qr+n^2 r)$. $\blacksquare$

---

\textbf{Lemma 4 (SPD).} \textit{Claim:} Under Assumptions 1–2, $A\succ 0$.

\textit{Proof.} This was proved in Part 1 of the Complete Proof. For any nonzero $x=\operatorname{vec}(V)$:

$$x^\top Ax = \|S^\top(Z\otimes K)x\|^2 + \lambda\sum_{j=1}^r v_j^\top K v_j \ge \lambda\,\lambda_{\min}(K)\|x\|^2 > 0,$$

since $\lambda>0$, $K\succ 0$, and at least one column $v_j\neq 0$. $\blacksquare$

---

\textbf{Lemma 5 (Preconditioner apply).} \textit{Claim:} $P^{-1}\operatorname{vec}(R)$ is computed by solving $(K+\delta I)Y=R$ then $X(G_\Omega+\lambda I)=Y$, giving $\operatorname{vec}(X)=P^{-1}\operatorname{vec}(R)$. Cost per apply: $O(n^2r+nr^2)$.

\textit{Proof.} We have $P = (G_\Omega+\lambda I_r)\otimes(K+\delta I_n)$. By the Kronecker inverse rule, $P^{-1} = (G_\Omega+\lambda I_r)^{-1}\otimes(K+\delta I_n)^{-1}$, which exists since both factors are SPD.

For $\operatorname{vec}(R)$:

$$P^{-1}\operatorname{vec}(R) = \left((G_\Omega+\lambda I_r)^{-1}\otimes(K+\delta I_n)^{-1}\right)\operatorname{vec}(R) = \operatorname{vec}\!\left((K+\delta I_n)^{-1}R\,(G_\Omega+\lambda I_r)^{-\top}\right),$$

using the Kronecker-vec identity $(C\otimes A)\operatorname{vec}(B)=\operatorname{vec}(ABC^\top)$ with $A=(K+\delta I)^{-1}$, $B=R$, $C=(G_\Omega+\lambda I)^{-1}$.

Since $G_\Omega+\lambda I$ is symmetric, $(G_\Omega+\lambda I)^{-\top}=(G_\Omega+\lambda I)^{-1}$, so:

$$P^{-1}\operatorname{vec}(R) = \operatorname{vec}\!\left((K+\delta I)^{-1}R\,(G_\Omega+\lambda I)^{-1}\right).$$

This is computed in two steps:
\begin{enumerate}
\item Solve $(K+\delta I)Y = R$: using precomputed Cholesky $L_K L_K^\top = K+\delta I_n$, solve $r$ triangular systems, cost $O(n^2 r)$.
\item Compute $X = Y(G_\Omega+\lambda I)^{-1}$: equivalently solve $X(G_\Omega+\lambda I)=Y$, i.e., $(G_\Omega+\lambda I)X^\top = Y^\top$. Using precomputed Cholesky $L_G L_G^\top = G_\Omega+\lambda I_r$, solve $n$ triangular systems of size $r$, cost $O(nr^2)$.

\end{enumerate}
Setup: $G_\Omega = \sum_t z_t z_t^\top$ costs $O(qr^2)$; Cholesky of $K+\delta I_n$ costs $O(n^3)$; Cholesky of $G_\Omega+\lambda I_r$ costs $O(r^3)$. Per-apply: $O(n^2 r + nr^2)$. $\blacksquare$

---

\textbf{Lemma 6 (PCG convergence).} \textit{Claim:} In exact arithmetic, after $m$ iterations of PCG on $Aw=b$ with preconditioner $P\succ 0$, the error $e_m = w_m - w^*$ satisfies

$$\|e_m\|_A \le 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m\|e_0\|_A, \quad \kappa=\kappa(P^{-1/2}AP^{-1/2}).$$

\textit{Proof.} This is a classical result. We give a proof sketch following Saad, Y., \textit{Iterative Methods for Sparse Linear Systems}, 2nd edition, SIAM, 2003, Theorem 9.4.12 and the development in §9.2–9.4.

\textbf{Step 1: Reduction to standard CG.} Define $\hat{A} = P^{-1/2}AP^{-1/2}$, $\hat{w} = P^{1/2}w$, $\hat{b}=P^{-1/2}b$. The system $Aw=b$ transforms to $\hat{A}\hat{w}=\hat{b}$. Since $A\succ 0$ and $P\succ 0$, $\hat{A}$ is SPD with eigenvalues in $[\lambda_{\min}(\hat A),\lambda_{\max}(\hat A)]$ and condition number $\kappa=\kappa(\hat A)$. Standard CG on $\hat{A}\hat{w}=\hat{b}$ produces iterates $\hat{w}_m$ whose error $\hat{e}_m=\hat{w}_m-\hat{w}^*$ satisfies the minimax polynomial bound. Furthermore, the PCG recurrences on the original system (Algorithm 9.1 in Saad) are algebraically equivalent to CG on the transformed system: the PCG iterates satisfy $w_m = P^{-1/2}\hat{w}_m$ and $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$ (Saad, §9.2).

\textbf{Step 2: CG minimax bound.} Standard CG satisfies (Saad, Theorem 9.4.12):

$$\|\hat{e}_m\|_{\hat{A}} \le \min_{p\in\mathcal{P}_m,\, p(0)=1} \max_{\lambda\in\sigma(\hat{A})} |p(\lambda)|\;\|\hat{e}_0\|_{\hat{A}},$$

where $\mathcal{P}_m$ is the set of polynomials of degree $\le m$.

\textbf{Step 3: Chebyshev bound.} The minimax problem on $[\lambda_{\min},\lambda_{\max}]$ is solved by the shifted and scaled Chebyshev polynomial. The Chebyshev polynomial $T_m$ of the first kind satisfies $T_m(x)=\cosh(m\,\operatorname{arccosh}(x))$ for $x\ge 1$. Setting

$$p(\lambda) = \frac{T_m\!\left(\frac{\lambda_{\max}+\lambda_{\min}-2\lambda}{\lambda_{\max}-\lambda_{\min}}\right)}{T_m\!\left(\frac{\lambda_{\max}+\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}\right)},$$

which satisfies $p(0)=1$ and $\max_{\lambda\in[\lambda_{\min},\lambda_{\max}]}|p(\lambda)| = 1/T_m\!\left(\frac{\kappa+1}{\kappa-1}\right)$.

\textbf{Step 4: Lower bound on $T_m$.} Let $\gamma=\frac{\kappa+1}{\kappa-1}>1$. Then

$$T_m(\gamma) = \cosh(m\,\operatorname{arccosh}(\gamma)).$$

Now $\operatorname{arccosh}(\gamma)=\ln(\gamma+\sqrt{\gamma^2-1})$. We compute:

$$\gamma^2-1 = \left(\frac{\kappa+1}{\kappa-1}\right)^2 - 1 = \frac{(\kappa+1)^2-(\kappa-1)^2}{(\kappa-1)^2} = \frac{4\kappa}{(\kappa-1)^2},$$

so $\sqrt{\gamma^2-1}=\frac{2\sqrt{\kappa}}{\kappa-1}$. Therefore

$$\gamma+\sqrt{\gamma^2-1} = \frac{\kappa+1}{\kappa-1}+\frac{2\sqrt\kappa}{\kappa-1} = \frac{\kappa+2\sqrt\kappa+1}{(\kappa-1)} = \frac{(\sqrt\kappa+1)^2}{\kappa-1} = \frac{(\sqrt\kappa+1)^2}{(\sqrt\kappa-1)(\sqrt\kappa+1)}=\frac{\sqrt\kappa+1}{\sqrt\kappa-1}.$$

Thus $\operatorname{arccosh}(\gamma)=\ln\!\left(\frac{\sqrt\kappa+1}{\sqrt\kappa-1}\right)$ and

$$T_m(\gamma) = \cosh\!\left(m\ln\frac{\sqrt\kappa+1}{\sqrt\kappa-1}\right) = \frac{1}{2}\!\left[\left(\frac{\sqrt\kappa+1}{\sqrt\kappa-1}\right)^m+\left(\frac{\sqrt\kappa-1}{\sqrt\kappa+1}\right)^m\right] \ge \frac{1}{2}\left(\frac{\sqrt\kappa+1}{\sqrt\kappa-1}\right)^m.$$

Combining Steps 2–4:

$$\|e_m\|_A = \|\hat{e}_m\|_{\hat A} \le \frac{1}{T_m(\gamma)}\|\hat{e}_0\|_{\hat A} \le 2\left(\frac{\sqrt\kappa-1}{\sqrt\kappa+1}\right)^m\|e_0\|_A.\quad\blacksquare$$

---

\subsection{Gap Closure Notes}

\textbf{Note 1 (Editor Point 1 — PCG/CG equivalence).} Lemma 6, Step 1 provides the proof sketch: substituting $\hat{w}=P^{1/2}w$ transforms the preconditioned system to a standard CG problem. This equivalence is established rigorously in Saad, Y., \textit{Iterative Methods for Sparse Linear Systems}, 2nd edition, SIAM, 2003, §9.2 (development of Algorithm 9.1). The key identity is that the PCG residuals $r_m=b-Aw_m$ and search directions $p_m$ satisfy $r_m=P^{-1/2}\hat{r}_m$ and $p_m=P^{-1/2}\hat{p}_m$, where $\hat{r}_m,\hat{p}_m$ are the CG residuals and directions for $\hat{A}\hat{w}=\hat{b}$.

\textbf{Note 2 (Editor Point 2 — Exact-arithmetic hypothesis).} The sentence "In exact arithmetic, the preconditioned CG method satisfies the following error bound" has been added at the beginning of the Lemma 6 claim, before the convergence inequality.

\textbf{Note 3 (Editor Point 3 — Precise citation).} The citation has been unified to a single definitive reference: Saad, Y., \textit{Iterative Methods for Sparse Linear Systems}, 2nd edition, SIAM, 2003, Theorem 9.4.12. This theorem gives the minimax Chebyshev bound for CG applied to SPD systems, which combined with the PCG-to-CG reduction in §9.2 yields the stated preconditioned bound.

\textbf{Note 4 (Editor Point 4 — Solution uniqueness).} The sentence "Since $A$ is SPD, it is invertible, and the system $Aw=b$ has a unique solution $w^*=A^{-1}b$" has been added at the end of Part 1.

\textbf{Note 5 (Editor Point 5 — Intermediate algebra).} The computation $(\gamma^2-1)=((κ+1)^2-(κ-1)^2)/(κ-1)^2 = 4κ/(κ-1)^2$, hence $\sqrt{\gamma^2-1}=2\sqrt{κ}/(κ-1)$, is now shown explicitly in Lemma 6, Step 4.

\textbf{Note 6 (Editor Point 6 — Storage bounds).} A storage analysis has been added at the end of Part 4: all stored objects ($L_K\in\mathbb{R}^{n\times n}$, $L_G\in\mathbb{R}^{r\times r}$, cached $z_t$ vectors, working matrices) total $O(n^2+r^2+qr+nr)$, none of which is $O(N)$.

\textbf{Note 7 (Editor Point 7 — Vectorization bijection).} The column-major bijection $\varphi:[n]\times[M]\to[N]$, $\varphi(i,m)=(m-1)n+i$, is now defined explicitly at the start of the Complete Proof, and $S$'s columns are written as $e_{\varphi(i_t,m_t)}$.

\textbf{Note 8 (Editor Point 8 — Stability of Cholesky).} No unqualified stability claims are made for the Cholesky factorizations. The proof treats all computations in exact arithmetic (as stated in Lemma 6). The Cholesky factorizations of $K+\delta I_n$ and $G_\Omega+\lambda I_r$ exist and are unique because these matrices are SPD; numerical stability considerations for finite-precision implementations are outside the scope of this exact-arithmetic proof.

\subsection{Editor Dispatch}
\noindent\textit{Reasoning:} claude\_code assigned to Correctness \& Completeness for its strength in systematic logical verification of multi-step mathematical arguments. gemini\_api assigned to Clarity \& Rigor for its ability to assess notational precision and exposition quality against graduate peer-review standards. codex\_cli assigned to Reference Validity to verify specific theorem citations and check hypothesis satisfaction for all invoked external results.
\begin{itemize}
\item Correctness \& Completeness $\to$ claude\_code
\item Clarity \& Rigor $\to$ gemini\_api
\item Reference Validity $\to$ codex\_cli
\end{itemize}

\subsection{Reviewer Feedback}
\subsubsection{Correctness \& Completeness (by claude\_code)}
\begin{enumerate}
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Lemma 3 / RHS definition: B is defined as T\_\{(k)\}Z (full unfolding times Z) but computed as sparse MTTKRP over observed entries only. These are equal only if unobserved entries are treated as zero, which is never stated or justified. The true T\_\{(k)\} has unknown unobserved entries.
\\
\textit{Required fix:} Explicitly define B as the observed-entry MTTKRP: B\_\{i,:\} = sum\_\{t: i\_t=i\} y\_t z\_t\textasciicircum{}T, or equivalently B = (1\_Omega ⊙ T\_\{(k)\}) Z. State that this arises from the gradient of the observed-entry least-squares objective, not from the full tensor.
\\
\textit{Suggestion:} Add a short derivation showing the optimization objective (observed-entry loss + RKHS regularizer), take its gradient with respect to W, set to zero, and identify A and b. This closes the gap and justifies why only observed entries appear in the RHS.
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Complete Proof, preamble: The linear system Aw = b is presented without derivation. The reader cannot verify it is the correct stationarity condition without knowing the objective function.
\\
\textit{Required fix:} State the optimization problem explicitly (e.g., min\_W (1/2) sum\_t (y\_t - <(KW)\_\{i\_t,:\}, z\_t>)\textasciicircum{}2 + (λ/2) tr(W\textasciicircum{}T K W)) and derive the normal equations to obtain A and b.
\\
\textit{Suggestion:} Insert a short 'Problem Formulation' section before Part 1 that states the objective and derives A, b by differentiating with respect to vec(W) and setting the gradient to zero.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Lemma 2 / Part 2 algorithm: Notational inconsistency: Lemma 2's proof uses z\_t\textasciicircum{}T (row vector) in the accumulation formula C\_\{i,:\} = sum alpha\_t z\_t\textasciicircum{}T, but Part 2's algorithm writes C\_\{i\_t,:\} += alpha\_t z\_t (column vector added to a row).
\\
\textit{Required fix:} Use z\_t\textasciicircum{}T consistently when adding to row slices C\_\{i\_t,:\}, or clarify that z\_t denotes a row vector in the algorithm pseudocode.
\\
\textit{Suggestion:} In Part 2 step 2, change 'C\_\{i\_t,:\} += alpha\_t z\_t' to 'C\_\{i\_t,:\} += alpha\_t z\_t\textasciicircum{}T' to match Lemma 2.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Lemma 6, Step 1: The step is labeled a 'proof sketch' within what is presented as a full lemma proof. This is inconsistent with the rigor standard. The PCG/CG equivalence is a non-trivial algebraic identity that is asserted but not proved.
\\
\textit{Required fix:} Either (a) prove the equivalence by showing the PCG recurrences produce w\_m = P\textasciicircum{}\{-1/2\} hat\{w\}\_m explicitly, or (b) clearly state this as a cited theorem: 'By Saad (2003), Proposition 9.1, the PCG iterates satisfy...' and remove the 'proof sketch' framing.
\\
\textit{Suggestion:} Option (b) is more practical: replace 'We give a proof sketch' with 'By the equivalence established in Saad (2003, §9.2, Algorithm 9.1 and surrounding development), PCG on (A, P) produces iterates satisfying w\_m = P\textasciicircum{}\{-1/2\} hat\{w\}\_m and ||e\_m||\_A = ||hat\{e\}\_m||\_\{hat\{A\}\}. We therefore apply standard CG convergence theory to hat\{A\}.'
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 3, on-the-fly z\_t generation: The cost O(q(d-1)r) for on-the-fly z\_t generation omits the cost of decomposing the linear index m\_t into the multi-index (j\_1,...,j\_\{k-1\},j\_\{k+1\},...,j\_d) needed to look up rows of each factor matrix.
\\
\textit{Required fix:} Either assume the multi-index form is stored alongside each observation (cost O(1) lookup), or account for the O(d) integer-division cost per observation.
\\
\textit{Suggestion:} Add a parenthetical: '(assuming multi-indices are stored with observations, so each factor-row lookup is O(1))' or note the O(qd) decomposition cost is dominated by O(q(d-1)r).
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 1, CG well-definedness: The claim that CG denominators p\_j\textasciicircum{}T A p\_j > 0 for nonzero search directions is stated without justification that search directions remain nonzero before convergence.
\\
\textit{Required fix:} Add a brief justification: in exact arithmetic, CG search directions are A-conjugate and nonzero until the residual vanishes (which occurs in at most nr steps).
\\
\textit{Suggestion:} Append: 'In exact arithmetic, the CG search directions p\_0, p\_1, ... are nonzero and A-conjugate as long as the residual is nonzero (see Saad, 2003, Proposition 6.6), ensuring p\_j\textasciicircum{}T A p\_j > 0 at each iteration.'
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item The proof treats K as strictly positive definite throughout but the Assumptions section acknowledges K may be only PSD in practice, deferring to a nugget fix. A brief remark on how the nugget-augmented kernel K + epsilon I inherits all stated properties would improve self-containedness.
\item No discussion of how the PCG initial guess w\_0 is chosen, though this does not affect correctness of the convergence bound (only the constant ||e\_0||\_A).
\end{itemize}

\subsubsection{Clarity \& Rigor (by gemini\_api)}
\begin{enumerate}
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Lemma 1 and Lemma 2: Notation collision: The symbol '⊙' is used for the Hadamard (element-wise) product in the proof (e.g., 1\_Ω ⊙ ...), but it was defined as the Khatri-Rao product in the Mentor's Definitions section (Z := A\_d ⊙ ... ⊙ A\_1). In a graduate-level text, using the same symbol for two distinct operations creates ambiguity.
\\
\textit{Required fix:} Use a distinct symbol for the Hadamard product (e.g., '∘' or '∗') or explicitly redefine '⊙' as the Hadamard product in the Notation section, noting the distinction from the Khatri-Rao product used in the problem setup.
\\
\textit{Suggestion:} Replace the Hadamard product symbol with '∘' throughout the proof to distinguish it from the Khatri-Rao product.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Lemma 2 Statement: Implicit variable definition: The lemma statement refers to 'U' in the algorithm description ('where U=KV'), but 'V' is not defined or introduced in the lemma statement itself. While 'V' is clear from context (Lemma 1), a self-contained lemma should declare its inputs.
\\
\textit{Required fix:} Explicitly introduce V in the lemma statement.
\\
\textit{Suggestion:} Amend the lemma statement to start with: 'Let V ∈ ℝ\textasciicircum{}\{n×r\} and define U := KV. The matrix C...'
\end{enumerate}

\subsubsection{Reference Validity (by codex\_cli)}
\begin{enumerate}
\item[\textcolor{red}{\textbullet}] \textbf{[CRITICAL]} Lemma 6 (PCG convergence), Note 3 citation: The citation "Saad (2003), Theorem 9.4.12" is not verifiable against the cited edition’s chapter structure (Section 9.4 is on flexible variants), so the proof currently relies on an apparent phantom/misindexed theorem for its core convergence claim.
\\
\textit{Required fix:} Replace the citation with the correct, verifiable source locations for (i) the CG Chebyshev/minimax convergence bound and (ii) the PCG-to-CG equivalence transformation.
\\
\textit{Suggestion:} Cite Saad (2nd ed., 2003) as: Section 6.11.3 (CG convergence theorem/bound, e.g., Theorem 6.29 and derived inequality) plus Section 9.2.1 (Algorithm 9.1/9.2 equivalence for preconditioning). Include page numbers from the exact edition used.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Lemma 6, Step 1 ("following ... §9.2–9.4"): The section range is overbroad/inexact for the claim being used; §9.4 is not the natural source for the stated PCG-CG equivalence or CG minimax bound.
\\
\textit{Required fix:} Use precise section-level citations tied to each subclaim instead of a broad range.
\\
\textit{Suggestion:} Split the citation inline: equivalence claim -> §9.2.1; CG minimax/Chebyshev bound -> §6.11.3. This makes dependency checking straightforward.
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item If your pipeline requires explicit sourcing for all nontrivial background facts, add brief references for vec-Kronecker identities and Kronecker SPD/eigenvalue properties (currently treated as standard facts without citation).
\end{itemize}

\subsection{Editor Decision}
\noindent\textbf{Verdict:} \textcolor{orange}{\textbf{right_track}}

\noindent\textbf{Summary:} The proof architecture is sound and most lemma proofs are rigorous. However, four issues block acceptance: (1) the citation 'Saad (2003), Theorem 9.4.12' appears to be a phantom reference — the CG Chebyshev bound lives in Chapter 6, not §9.4, undermining verifiability of the convergence argument; (2) the linear system Aw=b is presented without deriving it from an optimization objective, so the reader cannot verify correctness of A and b; (3) Lemma 3 defines B using the full tensor unfolding but computes it from observed entries only, without justifying the equivalence; (4) the symbol ⊙ is used for both Khatri-Rao and Hadamard products. Several minor issues (notation inconsistency in Part 2 vs Lemma 2, 'proof sketch' framing in Lemma 6, missing CG well-definedness justification, incomplete on-the-fly cost assumptions) also need attention.

\noindent\textbf{Feedback (prover):} Priority fixes for the prover:

1. **Add a Problem Formulation section before Part 1.** State the optimization objective explicitly, e.g., min\_W (1/2) sum\_\{t=1\}\textasciicircum{}q (y\_t - <(KW)\_\{i\_t,:\}, z\_t>)\textasciicircum{}2 + (λ/2) tr(W\textasciicircum{}T K W). Differentiate with respect to vec(W), set the gradient to zero, and identify the resulting normal equations Aw = b. This simultaneously resolves the missing derivation (Reviewer 2) and justifies why only observed entries appear in the RHS (Reviewer 1).

2. **Fix the Saad citation in Lemma 6.** Replace all references to 'Theorem 9.4.12' with the correct locations: cite §6.11.3 (or Theorem 6.29) for the CG Chebyshev/minimax convergence bound, and §9.2 (Algorithm 9.1 and surrounding development) for the PCG-to-CG equivalence. Include page numbers if possible. Also narrow the broad '§9.2–9.4' range in Step 1 to the specific sections used.

3. **Fix Lemma 6 Step 1 framing.** Replace 'We give a proof sketch following...' with a direct citation: 'By the equivalence established in Saad (2003, §9.2, Algorithm 9.1), PCG on (A, P) produces iterates satisfying w\_m = P\textasciicircum{}\{-1/2\} ŵ\_m and ||e\_m||\_A = ||ê\_m||\_\{Â\}. We therefore apply standard CG convergence theory to Â.'

4. **Resolve the ⊙ notation collision.** Use ∘ (or *) for the Hadamard product throughout the proof to distinguish it from the Khatri-Rao product ⊙ used in the problem setup. Add a note in the notation preamble.

5. **Fix Lemma 3 definition of B.** Define B as the observed-entry MTTKRP: B\_\{i,:\} = sum\_\{t: i\_t=i\} y\_t z\_t\textasciicircum{}T, or equivalently B = (1\_Ω ∘ T\_\{(k)\})Z. Reference the objective function derivation to justify this.

6. **Minor fixes (all straightforward):**
   - In Part 2, step 2: change C\_\{i\_t,:\} += α\_t z\_t to C\_\{i\_t,:\} += α\_t z\_t\textasciicircum{}T to match Lemma 2.
   - In Lemma 2 statement: explicitly introduce V ∈ ℝ\textasciicircum{}\{n×r\} as an input.
   - In Part 3: add '(assuming multi-indices are stored with observations)' for the on-the-fly z\_t cost.
   - In Part 1: append a sentence on CG search direction non-degeneracy, e.g., 'In exact arithmetic, CG search directions are nonzero and A-conjugate as long as the residual is nonzero (Saad, 2003, Proposition 6.6).'

\section{Loop 4}
\noindent Editor verdict for this loop: \textcolor{orange}{\textbf{right_track}}

\subsection{Researcher}
\subsection{Relevant Theorems}
\begin{itemize}
\item **Representer theorem (RKHS finite-sample form):** the minimizer of a regularized empirical loss in RKHS lies in the span of kernel sections at sample points, which justifies \(A\_k = KW\).  
\end{itemize}
  References: Kimeldorf–Wahba (1971); Schölkopf, Herbrich, Smola (COLT 2001).

\begin{itemize}
\item **Kronecker–vec identity:** for column-major vec,
\end{itemize}
  \[
  (C\otimes A)\operatorname\{vec\}(B)=\operatorname\{vec\}(AB C\textasciicircum{}\top).
  \]
  This is the key algebraic tool to rewrite \(A\operatorname\{vec\}(V)\) as a matrix operator on \(V\), without building \(Z\otimes K\).  
  References: Horn \& Johnson, \textit{Topics in Matrix Analysis}; Magnus \& Neudecker.

\begin{itemize}
\item **Kronecker inverse and SPD facts:**  
\end{itemize}
  \[
  (A\otimes B)\textasciicircum{}\{-1\}=A\textasciicircum{}\{-1\}\otimes B\textasciicircum{}\{-1\},
  \]
  and if \(A\succ0,\ B\succ0\), then \(A\otimes B\succ0\).  
  Used for preconditioner application and SPD guarantees.

\begin{itemize}
\item **CG minimax/Chebyshev convergence (correct citation):** for SPD \(\hat A\), CG satisfies polynomial minimization (in \(\hat A\)-norm), and
\end{itemize}
  \[
  \|e\_m\|\_\{\hat A\}\le 2\left(\frac\{\sqrt\{\kappa(\hat A)\}-1\}\{\sqrt\{\kappa(\hat A)\}+1\}\right)\textasciicircum{}m\|e\_0\|\_\{\hat A\}.
  \]
  Verified source: Saad (2nd ed., 2003), \textbf{§6.11.3}, Lemma 6.28, Theorem 6.29, inequality (6.128).  
  Important: this is an \textbf{exact-arithmetic} statement.

\begin{itemize}
\item **PCG/CG equivalence under preconditioning (correct location):** PCG with SPD preconditioner \(P\) is equivalent to CG on
\end{itemize}
  \[
  \hat A=P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\},
  \]
  or equivalently split-PCG formulations; this justifies importing the CG bound to PCG.  
  Verified source: Saad (2003), \textbf{§9.2.1}, Algorithms 9.1/9.2 and equivalence discussion.

\begin{itemize}
\item **Finite-precision caveat:** exact CG/PCG bounds can degrade numerically due to loss of orthogonality.  
\end{itemize}
  Reference: Greenbaum, \textit{Iterative Methods for Solving Linear Systems} (1997), Ch. 4.

\subsection{Key Definitions}
\begin{itemize}
\item **Dimensions:** \(n=n\_k,\ M=\prod\_\{i\ne k\}n\_i,\ N=nM,\ q=|\Omega|\), with \(n,r<q\ll N\).

\item **Observed set:** \(\Omega\subset[n]\times[M]\) with observed triples \((i\_t,m\_t,y\_t)\).  
\end{itemize}
  Use column-major index map \(\varphi(i,m)=i+n(m-1)\), so \(S=[e\_\{\varphi(i\_t,m\_t)\}]\_\{t=1\}\textasciicircum{}q\), \(P\_\Omega=SS\textasciicircum{}\top\).

\begin{itemize}
\item **Product symbols (must be disambiguated):**
\item \( \odot \): Khatri–Rao product (for \(Z\)).
\item \( \circ \): Hadamard mask product (for observation mask).

\item **Masked unfolding:** \(T\_\{(k)\}\in\mathbb R\textasciicircum{}\{n\times M\}\), \(T\_\Omega:=\mathbf 1\_\Omega\circ T\_\{(k)\}\).

\item **Objective (explicit):**
\end{itemize}
  \[
  \min\_W\ \frac12\sum\_\{t=1\}\textasciicircum{}q\big(y\_t-\langle (KW)\_\{i\_t,:\},z\_t\rangle\big)\textasciicircum{}2+\frac\lambda2\operatorname\{tr\}(W\textasciicircum{}\top K W),
  \]
  equivalently
  \[
  \frac12\|P\_\Omega(\operatorname\{vec\}(T\_\{(k)\})-(Z\otimes K)\operatorname\{vec\}(W))\|\_2\textasciicircum{}2+\frac\lambda2\,\operatorname\{vec\}(W)\textasciicircum{}\top(I\_r\otimes K)\operatorname\{vec\}(W).
  \]

\begin{itemize}
\item **Normal equations:** \(Aw=b\), \(w=\operatorname\{vec\}(W)\),
\end{itemize}
  \[
  A=(Z\otimes K)\textasciicircum{}\top P\_\Omega(Z\otimes K)+\lambda(I\_r\otimes K),
  \]
  \[
  b=(Z\otimes K)\textasciicircum{}\top P\_\Omega\operatorname\{vec\}(T\_\{(k)\})=\operatorname\{vec\}(K B),
  \]
  with
  \[
  B:=(\mathbf 1\_\Omega\circ T\_\{(k)\})Z,\quad B\_\{i,:\}=\sum\_\{t:i\_t=i\} y\_t z\_t\textasciicircum{}\top.
  \]

\begin{itemize}
\item **Matrix-free operator:**
\end{itemize}
  \[
  \mathcal A(V)=K\big((\mathbf 1\_\Omega\circ (KVZ\textasciicircum{}\top))Z+\lambda V\big),\quad
  A\operatorname\{vec\}(V)=\operatorname\{vec\}(\mathcal A(V)).
  \]

\begin{itemize}
\item **Preconditioner:**
\end{itemize}
  \[
  P=((G\_\Omega+\lambda I\_r)\otimes (K+\delta I\_n)),\quad
  G\_\Omega=\sum\_\{t=1\}\textasciicircum{}q z\_t z\_t\textasciicircum{}\top,\ \delta>0.
  \]

\subsection{Proof Strategies}
\begin{itemize}
\item **Start from the objective, then derive \(Aw=b\):** this resolves RHS ambiguity and makes observed-only data usage rigorous.

\item **Prove matrix-free matvec identity via Kronecker–vec + masking:**  
\end{itemize}
  show \(A\operatorname\{vec\}(V)=\operatorname\{vec\}(\mathcal A(V))\), then implement
  \(U=KV\), accumulate only over \(t=1,\dots,q\), then \(Y=KC+\lambda U\).

\begin{itemize}
\item **Use sparse MTTKRP for RHS:** compute \(B\) by \(q\) row updates, then \(b=\operatorname\{vec\}(KB)\).

\item **Apply Kronecker preconditioner by two solves:**  
\end{itemize}
  solve \((K+\delta I)Y=R\), then \(X(G\_\Omega+\lambda I)=Y\), return \(\operatorname\{vec\}(X)\).

\begin{itemize}
\item **Convergence argument:** cite PCG/CG equivalence (Saad §9.2.1), then apply CG Chebyshev bound from §6.11.3 to \(\hat A=P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\}\).

\item **Complexity/memory accounting:**  
\end{itemize}
  per matvec \(O(qr+n\textasciicircum{}2r)\); preconditioner-apply \(O(n\textasciicircum{}2r+nr\textasciicircum{}2)\); setup \(O(qr\textasciicircum{}2+n\textasciicircum{}3+r\textasciicircum{}3)\) (plus \(O(q(d-1)r)\) if \(z\_t\) generated online).  
  Storage \(O(n\textasciicircum{}2+r\textasciicircum{}2+nr+qr)\) if caching sampled \(z\_t\). No \(O(N)\) step.

\subsection{Gaps and Concerns}
\begin{itemize}
\item **Citation correction required:** do not cite “Saad Theorem 9.4.12” for CG Chebyshev bound. Correct source is **Chapter 6 (§6.11.3, Theorem 6.29, eq. 6.128)**; PCG equivalence is **§9.2.1**.

\item **Must state exact-arithmetic hypothesis** for the classical PCG bound; finite precision needs a caveat/reference.

\item **RHS definition must be masked-data consistent:** \(B\) should be \(B=(\mathbf1\_\Omega\circ T\_\{(k)\})Z\), not ambiguously full \(T\_\{(k)\}Z\).

\item **Notation collision:** reserve \( \odot \) for Khatri–Rao; use \( \circ \) (or \( * \)) for Hadamard masking.

\item **Projector assumption:** \(P\_\Omega=SS\textasciicircum{}\top\) is an orthogonal projector only if observed pairs are unique. Duplicates require weighted handling.

\item **Row/column consistency in updates:** if \(C\_\{i,:\}\) is a row, update with \(z\_t\textasciicircum{}\top\), not \(z\_t\).

\item **On-the-fly \(z\_t\) cost assumption:** either store multi-indices with observations or include index-decoding overhead (\(O(qd)\), typically dominated by \(O(q(d-1)r)\) for \(r\ge1\)).

\item **SPD condition:** strict PCG guarantees require \(K\succ0\). If \(K\succeq0\), add nugget or restrict to \(\mathrm\{range\}(K)\).
\end{itemize}

\subsection{Mentor}
\subsection{Definitions}
Let \(k\) be the target mode, \(n:=n\_k\), \(M:=\prod\_\{i\neq k\} n\_i\), \(N=nM\), and rank \(r\).

Let \(\Omega\subset [n]\times[M]\) be the observed index set in mode-\(k\) unfolding, \(|\Omega|=q\), with observed triples \((i\_t,m\_t,y\_t)\), \(t=1,\dots,q\), where \(y\_t=T\_\{i\_t,m\_t\}\).

Define
\[
Z:=A\_d\odot\cdots\odot A\_\{k+1\}\odot A\_\{k-1\}\odot\cdots\odot A\_1\in\mathbb\{R\}\textasciicircum{}\{M\times r\},
\quad
z\_t:=Z\_\{m\_t,:\}\in\mathbb\{R\}\textasciicircum{}r.
\]
Unknown is \(W\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\), \(w:=\mathrm\{vec\}(W)\), and \(A\_k=KW\).

Define \(P\_\Omega:=SS\textasciicircum{}\top\) (projector selecting observed entries in \(\mathrm\{vec\}(T)\)) and linear system
\[
A w=b,\quad
A=(Z\otimes K)\textasciicircum{}\top P\_\Omega (Z\otimes K)+\lambda(I\_r\otimes K),\quad
b=(I\_r\otimes K)\mathrm\{vec\}(B),\ B=TZ.
\]

Define mask matrix \(\mathbf\{1\}\_\Omega\in\\{0,1\\}\textasciicircum{}\{n\times M\}\). For \(V\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\),
\[
\mathcal A(V):=
K\!\left((\mathbf\{1\}\_\Omega\odot (KVZ\textasciicircum{}\top))Z+\lambda V\right),
\quad
A\,\mathrm\{vec\}(V)=\mathrm\{vec\}(\mathcal A(V)).
\]

\subsection{Formal Statement}
\textbf{Theorem (Matrix-free PCG for RKHS mode-\(k\) subproblem).} Under the assumptions in Section \textbf{Assumptions}:

\begin{enumerate}
\item \(A\) is SPD, so PCG is well-defined.
\item For any \(V\), \(A\mathrm\{vec\}(V)\) can be computed exactly without forming \(A\), \(Z\otimes K\), or any \(N\)-scale object via:
\end{enumerate}
\[
U=KV,\ \ 
\alpha\_t=\langle U\_\{i\_t,:\},z\_t\rangle,\ \ 
C\_\{i\_t,:\}\mathrel\{+\}=\alpha\_t z\_t,\ \ 
Y=KC+\lambda U,
\]
return \(\mathrm\{vec\}(Y)\).
\begin{enumerate}
\item The per-matvec complexity is \(O(qr+n\textasciicircum{}2r)\) (plus \(O(q(d-1)r)\) if each \(z\_t\) is generated on-the-fly from CP factors).
\item With preconditioner
\end{enumerate}
\[
P=((G\_\Omega+\lambda I\_r)\otimes (K+\delta I\_n)),\quad
G\_\Omega:=\sum\_\{t=1\}\textasciicircum{}q z\_tz\_t\textasciicircum{}\top,\ \delta>0,
\]
each \(P\textasciicircum{}\{-1\}\)-apply costs \(O(n\textasciicircum{}2r+nr\textasciicircum{}2)\) after setup \(O(qr\textasciicircum{}2+n\textasciicircum{}3+r\textasciicircum{}3)\).
\begin{enumerate}
\item PCG iterates satisfy
\end{enumerate}
\[
\|e\_m\|\_A\le 2\Big(\frac\{\sqrt\kappa-1\}\{\sqrt\kappa+1\}\Big)\textasciicircum{}m\|e\_0\|\_A,\quad
\kappa=\kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\}),
\]
hence total cost
\[
O\!\left(m(qr+n\textasciicircum{}2r+nr\textasciicircum{}2)+qr\textasciicircum{}2+n\textasciicircum{}3+r\textasciicircum{}3\right),
\]
with no \(O(N)\) operation.

\subsection{Assumptions}
\begin{enumerate}
\item \(\lambda>0\).
\item \(K\succ 0\) for strict SPD proof of \(A\); if original kernel is only PSD, either add nugget to the model kernel or solve on \(\mathrm\{range\}(K)\).
\item \(n,r<q\ll N\), and observed tuples \((i\_t,m\_t,y\_t)\) are directly available.
\item Indexing conventions for unfolding, vectorization, and \(S\) are consistent.
\item For preconditioner quality claims (small \(\kappa\)), observation pattern is not pathologically imbalanced across \(i\_t\) and rank directions.

\end{enumerate}
\subsection{Notation}
\begin{itemize}
\item \(n=n\_k\): size of mode \(k\).
\item \(M=\prod\_\{i\neq k\} n\_i\): product of non-\(k\) dimensions.
\item \(N=nM\): total entries.
\item \(q=|\Omega|\): number of observed entries.
\item \(Z\in\mathbb\{R\}\textasciicircum{}\{M\times r\}\): Khatri-Rao product of fixed factors.
\item \(K\in\mathbb\{R\}\textasciicircum{}\{n\times n\}\): kernel matrix.
\item \(W\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\), \(w=\mathrm\{vec\}(W)\): unknown.
\item \(B=TZ\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\): sparse MTTKRP result.
\item \(z\_t=Z\_\{m\_t,:\}\): sampled Khatri-Rao row for observation \(t\).
\item \(P\_\Omega=SS\textasciicircum{}\top\): selection projector in vectorized unfolding space.
\item \(G\_\Omega=\sum\_t z\_tz\_t\textasciicircum{}\top\in\mathbb\{R\}\textasciicircum{}\{r\times r\}\): sampled rank-Gram.

\end{itemize}
\subsection{High-Level Strategy}
\begin{enumerate}
\item Rewrite the Hessian action as a matrix operator \(\mathcal A(V)\) using \((C\otimes A)\mathrm\{vec\}(B)=\mathrm\{vec\}(AB C\textasciicircum{}\top)\).
\item Implement \(\mathcal A(V)\) from observed entries only, via rowwise accumulation \(C\_\{i\_t,:\} \!+\!= \alpha\_t z\_t\), never forming \(P\_\Omega\), \(Z\otimes K\), or dense \(T\).
\item Compute RHS sparsely: \(B\_\{i\_t,:\}\!+\!=y\_t z\_t\), then \(b=\mathrm\{vec\}(KB)\).
\item Run PCG on \(Aw=b\) with Kronecker preconditioner \(P\) that is cheap to invert by two small Cholesky-based solves.
\item Prove complexity by summing matvec cost, preconditioner-apply cost, and one-time setup.

\end{enumerate}
\subsection{Key Lemmas}
\begin{enumerate}
\item **Kronecker-vec masked operator lemma:** \(A\,\mathrm\{vec\}(V)=\mathrm\{vec\}(\mathcal A(V))\).
\item **Sampled accumulation lemma:** \(C=(\mathbf\{1\}\_\Omega\odot(KVZ\textasciicircum{}\top))Z\) is computed exactly by the \(q\)-sample loop in \(O(qr)\).
\item **Sparse RHS lemma:** \(B=TZ\) is computed from observations in \(O(qr)\), then \(b\) in \(O(n\textasciicircum{}2r)\).
\item **SPD lemma:** if \(K\succ0,\lambda>0\), then \(A\succ0\) because
\end{enumerate}
\[
x\textasciicircum{}\top A x=\|P\_\Omega\textasciicircum{}\{1/2\}(Z\otimes K)x\|\_2\textasciicircum{}2+\lambda\|(I\_r\otimes K\textasciicircum{}\{1/2\})x\|\_2\textasciicircum{}2>0.
\]
\begin{enumerate}
\item **Preconditioner-apply lemma:** \(P\textasciicircum{}\{-1\}\mathrm\{vec\}(R)\) equals two solves:
\end{enumerate}
\[
(K+\delta I)Y=R,\quad X(G\_\Omega+\lambda I)=Y,\quad \mathrm\{vec\}(X)=P\textasciicircum{}\{-1\}\mathrm\{vec\}(R).
\]
\begin{enumerate}
\item **PCG convergence lemma:** standard SPD PCG bound with \(\kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\})\).

\end{enumerate}
\subsection{Dependency Graph}
\begin{enumerate}
\item Definitions + Notation \(\rightarrow\) Lemma 1.
\item Lemma 1 \(\rightarrow\) Lemma 2 (matvec algorithm correctness).
\item Definitions \(\rightarrow\) Lemma 3 (RHS correctness/cost).
\item Assumptions 1-2 \(\rightarrow\) Lemma 4 (SPD of \(A\)).
\item Definitions \(\rightarrow\) Lemma 5 (preconditioner form and apply rule).
\item Lemma 4 + Lemma 5 + Lemma 6 \(\rightarrow\) PCG convergence claim.
\item Lemma 2 + Lemma 3 + Lemma 5 \(\rightarrow\) total complexity claim in theorem.

\end{enumerate}
\subsection{Risky Steps}
\begin{enumerate}
\item **PSD kernel issue:** if \(K\) is singular, \(A\) may be singular despite \(\lambda>0\); this must be handled explicitly.
\item **Index consistency:** mismatch between unfolding order, \(S\), and sampled \((i\_t,m\_t)\) invalidates the operator.
\item **Preconditioner quality claim:** spectral-equivalence constants require sampling-balance assumptions; do not claim uniformly good \(\kappa\) without them.
\item **Hidden \(M\)-scale cost:** forming full \(Z\) is infeasible when \(M\) is huge; use only sampled rows \(z\_t\) (cached or on-demand).
\item **Numerical stability:** very small \(\lambda\) or ill-conditioned \(K\) can cause PCG stagnation; monitor relative preconditioned residual and cap iterations.
\end{enumerate}

\subsection{Proof}
Now I have full context. Let me write the complete proof addressing all editor feedback points.

\subsection{Complete Proof}

\subsubsection{Notation}

We adopt the following conventions throughout.

\begin{itemize}
\item $n = n_k$: size of mode $k$.
\item $M = \prod_{i \neq k} n_i$: product of all dimensions except $k$.
\item $N = nM$: total number of tensor entries.
\item $q = |\Omega|$: number of observed entries, with $n, r < q \ll N$.
\item $K \in \mathbb{R}^{n \times n}$: kernel matrix, assumed SPD ($K \succ 0$).
\item $Z \in \mathbb{R}^{M \times r}$: Khatri–Rao product $A_d \odot \cdots \odot A_{k+1} \odot A_{k-1} \odot \cdots \odot A_1$ of fixed CP factors.
\item $W \in \mathbb{R}^{n \times r}$: unknown, with $w = \operatorname{vec}(W) \in \mathbb{R}^{nr}$.
\item Observed entries are indexed by triples $(i_t, m_t, y_t)$ for $t = 1, \dots, q$, where $i_t \in [n]$, $m_t \in [M]$, and $y_t$ is the observed value at position $(i_t, m_t)$ in the mode-$k$ unfolding.
\item $\mathbf{1}_\Omega \in \{0,1\}^{n \times M}$: binary mask with $(\mathbf{1}_\Omega)_{i,m} = 1$ iff $(i,m) \in \Omega$.
\item $z_t := Z_{m_t,:} \in \mathbb{R}^r$: the row of $Z$ corresponding to observation $t$.
\item $G_\Omega := \sum_{t=1}^q z_t z_t^\top \in \mathbb{R}^{r \times r}$: sampled rank-Gram matrix.
\item Column-major vectorization: we define the bijection $\varphi: [n] \times [M] \to [N]$ by $\varphi(i, m) = i + n(m-1)$, so $\operatorname{vec}(X)_{\varphi(i,m)} = X_{i,m}$. The selection matrix $S \in \mathbb{R}^{N \times q}$ has columns $e_{\varphi(i_t, m_t)}$ for $t = 1, \dots, q$, and the projection $P_\Omega := SS^\top$ satisfies $P_\Omega \operatorname{vec}(X) = \operatorname{vec}(\mathbf{1}_\Omega \circ X)$.
\item **Notation distinction:** $\odot$ denotes the Khatri–Rao (columnwise Kronecker) product; $\circ$ denotes the Hadamard (entrywise) product.

\end{itemize}
---

\subsubsection{Problem Formulation}

We seek the factor $A_k = KW$ by solving a regularized least-squares problem over the observed entries:

$$
\min\_\{W \in \mathbb\{R\}\textasciicircum{}\{n \times r\}\} \; f(W) := \frac\{1\}\{2\} \sum\_\{t=1\}\textasciicircum{}q \bigl(y\_t - \langle (KW)\_\{i\_t,:\},\, z\_t \rangle\bigr)\textasciicircum{}2 + \frac\{\lambda\}\{2\} \operatorname\{tr\}(W\textasciicircum{}\top K W).
$$

The first term fits observed data; the second is an RKHS-norm regularizer with $\lambda > 0$.

\textbf{Derivation of the normal equations.} We compute the gradient with respect to $w = \operatorname{vec}(W)$. The predicted value for observation $t$ is

$$
\hat\{y\}\_t = \langle (KW)\_\{i\_t,:\},\, z\_t \rangle = e\_\{i\_t\}\textasciicircum{}\top K W z\_t = (z\_t\textasciicircum{}\top \otimes e\_\{i\_t\}\textasciicircum{}\top K) w = (z\_t \otimes K e\_\{i\_t\})\textasciicircum{}\top w,
$$

where $e_{i_t}$ is the $i_t$-th standard basis vector in $\mathbb{R}^n$. Equivalently, $\hat{y}_t = e_{\varphi(i_t,m_t)}^\top (Z \otimes K) w$, since the $\varphi(i_t,m_t)$-th row of $Z \otimes K$ is $z_t^\top \otimes (K e_{i_t})^\top$. Hence

$$
S\textasciicircum{}\top (Z \otimes K) w = \begin\{pmatrix\} \hat\{y\}\_1 \\ \vdots \\ \hat\{y\}\_q \end\{pmatrix\}, \qquad
\sum\_\{t=1\}\textasciicircum{}q (\hat\{y\}\_t - y\_t)\textasciicircum{}2 = \|S\textasciicircum{}\top (Z \otimes K) w - S\textasciicircum{}\top \operatorname\{vec\}(T)\|\_2\textasciicircum{}2.
$$

The regularizer in vectorized form is $\frac{\lambda}{2} w^\top (I_r \otimes K) w$, since $\operatorname{tr}(W^\top K W) = w^\top (I_r \otimes K) w$.

Setting $\nabla_w f = 0$:

$$
(Z \otimes K)\textasciicircum{}\top S S\textasciicircum{}\top (Z \otimes K) w + \lambda (I\_r \otimes K) w = (Z \otimes K)\textasciicircum{}\top S S\textasciicircum{}\top \operatorname\{vec\}(T).
$$

For the right-hand side, note that $S S^\top \operatorname{vec}(T) = P_\Omega \operatorname{vec}(T) = \operatorname{vec}(\mathbf{1}_\Omega \circ T_{(k)})$, where $T_{(k)}$ is the mode-$k$ unfolding (which agrees with $T$ on observed entries and is zero elsewhere, since unobserved entries of $T$ are set to zero). Therefore

$$
(Z \otimes K)\textasciicircum{}\top \operatorname\{vec\}(\mathbf\{1\}\_\Omega \circ T\_\{(k)\}) = \operatorname\{vec\}\bigl(K (\mathbf\{1\}\_\Omega \circ T\_\{(k)\}) Z\bigr) = (I\_r \otimes K) \operatorname\{vec\}(B),
$$

where we define $B := (\mathbf{1}_\Omega \circ T_{(k)}) Z \in \mathbb{R}^{n \times r}$, the observed-entry MTTKRP. The second equality uses the identity $(C^\top \otimes A) \operatorname{vec}(X) = \operatorname{vec}(AXC)$ with $C = Z$, $A = K$, and noting that $(Z \otimes K)^\top = (Z^\top \otimes K^\top) = (Z^\top \otimes K)$ since $K$ is symmetric. This yields the normal equations

$$
A w = b, \quad A := (Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K) + \lambda (I\_r \otimes K), \quad b := (I\_r \otimes K) \operatorname\{vec\}(B).
$$

Since only observed entries contribute to both $A$ and $b$, no $O(N)$-scale objects are needed.

---

\subsubsection{Part 1: $A$ is SPD and PCG is well-defined}

For any nonzero $x \in \mathbb{R}^{nr}$:

$$
x\textasciicircum{}\top A x = \|S\textasciicircum{}\top (Z \otimes K) x\|\_2\textasciicircum{}2 + \lambda\, x\textasciicircum{}\top (I\_r \otimes K) x.
$$

The first term is nonneg. For the second, write $x = \operatorname{vec}(V)$ for $V \in \mathbb{R}^{n \times r}$; then $x^\top (I_r \otimes K) x = \operatorname{tr}(V^\top K V) = \sum_{j=1}^r v_j^\top K v_j$ where $v_j$ are the columns of $V$. Since $K \succ 0$, each $v_j^\top K v_j > 0$ unless $v_j = 0$, and at least one $v_j \neq 0$ since $x \neq 0$. With $\lambda > 0$, we obtain $x^\top A x > 0$. Hence $A \succ 0$.

Since $A$ is SPD, it is invertible, and the system $Aw = b$ has a unique solution $w^* = A^{-1}b$. PCG is therefore well-defined: in exact arithmetic, CG search directions are nonzero and $A$-conjugate as long as the residual is nonzero (Saad, 2003, Proposition 6.6), so the method converges in at most $nr$ iterations.

---

\subsubsection{Part 2: Matrix-free matvec}

\textbf{Input:} $V \in \mathbb{R}^{n \times r}$. \textbf{Output:} the unique $Y \in \mathbb{R}^{n \times r}$ satisfying $\operatorname{vec}(Y) = A \operatorname{vec}(V)$.

\textbf{Algorithm:}

\begin{enumerate}
\item Compute $U = KV \in \mathbb{R}^{n \times r}$.
\item Initialize $C = 0_{n \times r}$. For $t = 1, \dots, q$: compute $\alpha_t = U_{i_t,:} \, z_t \in \mathbb{R}$ (inner product), then update $C_{i_t,:} \mathrel{+}= \alpha_t\, z_t^\top$.
\item Compute $Y = KC + \lambda U$.
\item Return $\operatorname{vec}(Y)$.

\end{enumerate}
\textbf{Correctness} is established in Lemma 1 and Lemma 2 below.

---

\subsubsection{Part 3: Complexity of one matvec}

\begin{itemize}
\item **Step 1:** $U = KV$ costs $O(n^2 r)$ (dense matrix–matrix multiply).
\item **Step 2:** Each of the $q$ iterations computes an inner product of length $r$ ($O(r)$) and a rank-1 row update ($O(r)$), totaling $O(qr)$.
\item If each $z_t$ is generated on the fly from the $d-1$ CP factor rows rather than pre-cached, extracting and Hadamard-multiplying those rows costs $O((d-1)r)$ per observation (assuming multi-indices $(i_t^{(1)}, \dots, i_t^{(d)})$ are stored with observations), giving an additional $O(q(d-1)r)$.
\item **Step 3:** $KC$ costs $O(n^2 r)$; the addition $\lambda U$ is $O(nr)$.

\end{itemize}
\textbf{Total per-matvec cost:} $O(qr + n^2 r)$, or $O(q(d-1)r + n^2 r)$ with on-the-fly $z_t$ generation. Since $n < q$, we have $n^2 r \leq nqr$, so the cost is at most $O(nqr)$ in all cases. Crucially, no $O(N)$ operation is performed.

---

\subsubsection{Part 4: Preconditioner}

Define the Kronecker preconditioner

$$
P := (G\_\Omega + \lambda I\_r) \otimes (K + \delta I\_n), \quad \delta > 0.
$$

\textbf{Setup cost:}
\begin{itemize}
\item $G_\Omega = \sum_{t=1}^q z_t z_t^\top$: costs $O(qr^2)$.
\item Cholesky factorization of $K + \delta I_n \in \mathbb{R}^{n \times n}$: $O(n^3)$, yielding $L_K$.
\item Cholesky factorization of $G_\Omega + \lambda I_r \in \mathbb{R}^{r \times r}$: $O(r^3)$, yielding $L_G$.

\end{itemize}
Both Cholesky factorizations are well-defined since $K + \delta I_n \succ 0$ (as $K \succeq 0$ and $\delta > 0$) and $G_\Omega + \lambda I_r \succ 0$ (as $G_\Omega \succeq 0$ and $\lambda > 0$). Numerical stability of these factorizations follows from the matrices being SPD (cf. Higham, \textit{Accuracy and Stability of Numerical Algorithms}, 2nd ed., SIAM, 2002, Theorem 10.3).

\textbf{Total one-time setup:} $O(qr^2 + n^3 + r^3)$.

\textbf{Applying $P^{-1}$:} Given residual matrix $R \in \mathbb{R}^{n \times r}$ (with $\operatorname{vec}(R)$ as input), compute $P^{-1} \operatorname{vec}(R) = \operatorname{vec}(X)$ via:

\begin{enumerate}
\item Solve $(K + \delta I_n) \widetilde{R} = R$ for $\widetilde{R} \in \mathbb{R}^{n \times r}$: using the Cholesky factor $L_K$, solve $r$ triangular systems, costing $O(n^2 r)$.
\item Solve $\widetilde{R} (G_\Omega + \lambda I_r)^\top = X$, equivalently $(G_\Omega + \lambda I_r) X^\top = \widetilde{R}^\top$: using $L_G$, solve $n$ triangular systems, costing $O(nr^2)$.

\end{enumerate}
\textbf{Per-apply cost:} $O(n^2 r + nr^2)$.

\textbf{Storage:} All stored objects—$L_K \in \mathbb{R}^{n \times n}$, $L_G \in \mathbb{R}^{r \times r}$, cached $z_t$ vectors ($O(qr)$), and working matrices ($O(nr)$)—total $O(n^2 + r^2 + qr + nr)$ storage, none of which is $O(N)$.

---

\subsubsection{Part 5: PCG convergence and total complexity}

By Lemma 6 below, after $m$ PCG iterations with preconditioner $P$:

$$
\|e\_m\|\_A \leq 2 \left(\frac\{\sqrt\{\kappa\} - 1\}\{\sqrt\{\kappa\} + 1\}\right)\textasciicircum{}m \|e\_0\|\_A, \quad \kappa = \kappa(P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\}).
$$

Each PCG iteration requires one matvec ($O(qr + n^2 r)$) and one preconditioner apply ($O(n^2 r + nr^2)$), plus $O(nr)$ for vector operations. After $m$ iterations, the total cost is:

$$
O\!\left(m(qr + n\textasciicircum{}2 r + nr\textasciicircum{}2) + qr\textasciicircum{}2 + n\textasciicircum{}3 + r\textasciicircum{}3\right),
$$

where the last three terms are one-time setup. No $O(N)$ operation appears anywhere. $\blacksquare$

---

\subsection{Lemma Proofs}

\subsubsection{Lemma 1 (Kronecker-vec masked operator)}

\textbf{Statement.} For any $V \in \mathbb{R}^{n \times r}$, define $\mathcal{A}(V) := K\bigl((\mathbf{1}_\Omega \circ (KVZ^\top))Z + \lambda V\bigr)$. Then $A\,\operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))$.

\textbf{Proof.} We compute each term of $A \operatorname{vec}(V) = \bigl[(Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda(I_r \otimes K)\bigr] \operatorname{vec}(V)$ separately.

\textit{First term.} Using the mixed-product property $(C^\top \otimes A)\operatorname{vec}(X) = \operatorname{vec}(AXC)$:

$$
(Z \otimes K) \operatorname\{vec\}(V) = \operatorname\{vec\}(KVZ\textasciicircum{}\top) \in \mathbb\{R\}\textasciicircum{}N.
$$

Applying $P_\Omega = SS^\top$:

$$
P\_\Omega \operatorname\{vec\}(KVZ\textasciicircum{}\top) = \operatorname\{vec\}\bigl(\mathbf\{1\}\_\Omega \circ (KVZ\textasciicircum{}\top)\bigr),
$$

since $P_\Omega$ zeros out entries outside $\Omega$. Then:

$$
(Z \otimes K)\textasciicircum{}\top \operatorname\{vec\}\bigl(\mathbf\{1\}\_\Omega \circ (KVZ\textasciicircum{}\top)\bigr) = (Z\textasciicircum{}\top \otimes K) \operatorname\{vec\}\bigl(\mathbf\{1\}\_\Omega \circ (KVZ\textasciicircum{}\top)\bigr) = \operatorname\{vec\}\bigl(K(\mathbf\{1\}\_\Omega \circ (KVZ\textasciicircum{}\top))Z\bigr),
$$

where we used $K^\top = K$ and applied the identity again with $C = Z$, $A = K$.

\textit{Second term.} $\lambda(I_r \otimes K)\operatorname{vec}(V) = \lambda\,\operatorname{vec}(KV)$.

\textit{Combining:} $A\operatorname{vec}(V) = \operatorname{vec}\bigl(K(\mathbf{1}_\Omega \circ (KVZ^\top))Z + \lambda KV\bigr) = \operatorname{vec}\bigl(K[(\mathbf{1}_\Omega \circ (KVZ^\top))Z + \lambda V]\bigr) = \operatorname{vec}(\mathcal{A}(V))$. $\square$

---

\subsubsection{Lemma 2 (Sampled accumulation)}

\textbf{Statement.} Let $V \in \mathbb{R}^{n \times r}$ and $U = KV$. Define $C \in \mathbb{R}^{n \times r}$ by initializing $C = 0$ and performing the accumulation

$$
C\_\{i\_t,:\} \mathrel\{+\}= (U\_\{i\_t,:\}\, z\_t)\, z\_t\textasciicircum{}\top, \quad t = 1, \dots, q.
$$

Then $C = (\mathbf{1}_\Omega \circ (UZ^\top))Z$, and the accumulation costs $O(qr)$.

\textbf{Proof.} The $(i, j)$-entry of $\mathbf{1}_\Omega \circ (UZ^\top)$ is

$$
[\mathbf\{1\}\_\Omega \circ (UZ\textasciicircum{}\top)]\_\{i,m\} = \begin\{cases\} U\_\{i,:\}\, Z\_\{m,:\}\textasciicircum{}\top \& \text\{if \} (i,m) \in \Omega, \\ 0 \& \text\{otherwise.\} \end\{cases\}
$$

Therefore, the $i$-th row of $(\mathbf{1}_\Omega \circ (UZ^\top))Z$ is

$$
\bigl[(\mathbf\{1\}\_\Omega \circ (UZ\textasciicircum{}\top))Z\bigr]\_\{i,:\} = \sum\_\{m:\,(i,m)\in\Omega\} (U\_\{i,:\}\, Z\_\{m,:\}\textasciicircum{}\top)\, Z\_\{m,:\}.
$$

Since the observed entries at row $i$ are exactly $\{t : i_t = i\}$ with $Z_{m_t,:} = z_t^\top$, this equals

$$
\sum\_\{t:\,i\_t = i\} (U\_\{i,:\}\, z\_t)\, z\_t\textasciicircum{}\top.
$$

This is precisely what the accumulation loop computes for row $i$: each observation $t$ with $i_t = i$ adds $(U_{i_t,:}\, z_t)\, z_t^\top$ to $C_{i,:}$.

\textbf{Complexity:} Each iteration computes one inner product ($O(r)$) and one outer-product row update ($O(r)$), totaling $O(qr)$. $\square$

---

\subsubsection{Lemma 3 (Sparse RHS)}

\textbf{Statement.} Define the observed-entry MTTKRP as

$$
B\_\{i,:\} := \sum\_\{t:\, i\_t = i\} y\_t\, z\_t\textasciicircum{}\top, \quad i = 1, \dots, n,
$$

or equivalently $B = (\mathbf{1}_\Omega \circ T_{(k)}) Z$. Then $b = (I_r \otimes K)\operatorname{vec}(B) = \operatorname{vec}(KB)$, and $B$ is computed from observed entries in $O(qr)$ with $KB$ in an additional $O(n^2 r)$.

\textbf{Proof.} \textit{Equivalence of definitions.} Since $T_{(k)}$ has $T_{i,m} = y_t$ for $(i,m) = (i_t, m_t) \in \Omega$ and $T_{i,m} = 0$ for $(i,m) \notin \Omega$, we have $(\mathbf{1}_\Omega \circ T_{(k)}) = T_{(k)}$ (masking is idempotent on a matrix that is already zero outside $\Omega$). Thus:

$$
[(\mathbf\{1\}\_\Omega \circ T\_\{(k)\}) Z]\_\{i,:\} = \sum\_\{m:\,(i,m)\in\Omega\} T\_\{i,m\}\, Z\_\{m,:\} = \sum\_\{t:\,i\_t=i\} y\_t\, z\_t\textasciicircum{}\top = B\_\{i,:\}.
$$

\textit{RHS derivation.} From the Problem Formulation, the RHS of the normal equations is

$$
b = (Z \otimes K)\textasciicircum{}\top P\_\Omega \operatorname\{vec\}(T\_\{(k)\}) = (Z\textasciicircum{}\top \otimes K) \operatorname\{vec\}(\mathbf\{1\}\_\Omega \circ T\_\{(k)\}) = \operatorname\{vec\}(K (\mathbf\{1\}\_\Omega \circ T\_\{(k)\}) Z) = \operatorname\{vec\}(KB) = (I\_r \otimes K)\operatorname\{vec\}(B).
$$

\textit{Complexity.} Computing $B$ via the accumulation $B_{i_t,:} \mathrel{+}= y_t\, z_t^\top$ for $t = 1,\dots,q$ costs $O(qr)$. The multiplication $KB$ costs $O(n^2 r)$. Total: $O(qr + n^2 r)$. $\square$

---

\subsubsection{Lemma 4 (SPD property)}

\textbf{Statement.} If $K \succ 0$ and $\lambda > 0$, then $A \succ 0$.

\textbf{Proof.} For any nonzero $x \in \mathbb{R}^{nr}$:

$$
x\textasciicircum{}\top A x = \underbrace\{\|S\textasciicircum{}\top(Z \otimes K)x\|\_2\textasciicircum{}2\}\_\{\geq\, 0\} + \lambda \underbrace\{x\textasciicircum{}\top (I\_r \otimes K) x\}\_\{>\, 0\}.
$$

The second term: write $x = \operatorname{vec}(V)$, $V \neq 0$. Then $x^\top(I_r \otimes K)x = \sum_{j=1}^r v_j^\top K v_j$, where $v_j$ are columns of $V$. Since $K \succ 0$ and at least one $v_j \neq 0$, this sum is strictly positive. Hence $x^\top A x > 0$ for all $x \neq 0$, so $A \succ 0$. $\square$

---

\subsubsection{Lemma 5 (Preconditioner apply)}

\textbf{Statement.} Let $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$ with $\delta > 0$. Then $P \succ 0$, and for any $R \in \mathbb{R}^{n \times r}$, $P^{-1}\operatorname{vec}(R) = \operatorname{vec}(X)$ where $X$ is obtained by:

\begin{enumerate}
\item Solve $(K + \delta I_n)\widetilde{R} = R$ for $\widetilde{R}$.
\item Solve $(G_\Omega + \lambda I_r) X^\top = \widetilde{R}^\top$ for $X$.

\end{enumerate}
The per-apply cost is $O(n^2 r + nr^2)$ given precomputed Cholesky factors.

\textbf{Proof.} \textit{$P \succ 0$:} Since $K \succeq 0$ and $\delta > 0$, $K + \delta I_n \succ 0$. Since $G_\Omega \succeq 0$ and $\lambda > 0$, $G_\Omega + \lambda I_r \succ 0$. The Kronecker product of two SPD matrices is SPD (eigenvalues are pairwise products of eigenvalues of the factors, all positive). Hence $P \succ 0$.

\textit{Inversion formula:} Since $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, we have $P^{-1} = (G_\Omega + \lambda I_r)^{-1} \otimes (K + \delta I_n)^{-1}$. Applying this to $\operatorname{vec}(R)$:

$$
P\textasciicircum{}\{-1\}\operatorname\{vec\}(R) = \bigl[(G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\} \otimes (K + \delta I\_n)\textasciicircum{}\{-1\}\bigr]\operatorname\{vec\}(R) = \operatorname\{vec\}\bigl((K + \delta I\_n)\textasciicircum{}\{-1\} R\, (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-\top\}\bigr).
$$

Since $G_\Omega + \lambda I_r$ is symmetric, $(G_\Omega + \lambda I_r)^{-\top} = (G_\Omega + \lambda I_r)^{-1}$. So:

$$
X = (K + \delta I\_n)\textasciicircum{}\{-1\} R\, (G\_\Omega + \lambda I\_r)\textasciicircum{}\{-1\}.
$$

Defining $\widetilde{R} = (K + \delta I_n)^{-1} R$ (Step 1) and then $X = \widetilde{R} (G_\Omega + \lambda I_r)^{-1}$, equivalently $(G_\Omega + \lambda I_r) X^\top = \widetilde{R}^\top$ (Step 2), we recover $X$.

\textit{Cost:} Step 1 solves $r$ triangular systems of size $n$ (forward and back substitution with Cholesky factor $L_K$): $O(n^2 r)$. Step 2 solves $n$ triangular systems of size $r$: $O(nr^2)$. Total: $O(n^2 r + nr^2)$. $\square$

---

\subsubsection{Lemma 6 (PCG convergence)}

\textbf{Statement.} In exact arithmetic, the preconditioned conjugate gradient method applied to $Aw = b$ with preconditioner $P \succ 0$ satisfies, after $m$ iterations:

$$
\|e\_m\|\_A \leq 2\left(\frac\{\sqrt\{\kappa\} - 1\}\{\sqrt\{\kappa\} + 1\}\right)\textasciicircum{}m \|e\_0\|\_A, \quad \kappa := \kappa(P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\}) = \frac\{\lambda\_\{\max\}(\hat\{A\})\}\{\lambda\_\{\min\}(\hat\{A\})\},
$$

where $e_m = w_m - w^*$ is the error at iteration $m$ and $\hat{A} := P^{-1/2} A P^{-1/2}$.

\textbf{Proof.} By the equivalence established in Saad (2003, §9.2, Algorithm 9.1), PCG on $(A, P)$ produces iterates $w_m$ related to standard CG iterates $\hat{w}_m$ on $\hat{A} = P^{-1/2}AP^{-1/2}$ by the transformation $w_m = P^{-1/2}\hat{w}_m$. Defining $\hat{e}_m = \hat{w}_m - \hat{w}^\textit{$ where $\hat{w}^} = P^{1/2}w^*$, we have

$$
\|e\_m\|\_A\textasciicircum{}2 = e\_m\textasciicircum{}\top A e\_m = \hat\{e\}\_m\textasciicircum{}\top (P\textasciicircum{}\{-1/2\})\textasciicircum{}\top A P\textasciicircum{}\{-1/2\} \hat\{e\}\_m = \hat\{e\}\_m\textasciicircum{}\top \hat\{A\} \hat\{e\}\_m = \|\hat\{e\}\_m\|\_\{\hat\{A\}\}\textasciicircum{}2.
$$

Hence $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$.

We now apply the standard CG Chebyshev convergence bound to $\hat{A}$. Since $\hat{A} \succ 0$ (as $A \succ 0$ and $P \succ 0$), CG on $\hat{A}$ is well-defined. By Saad (2003, §6.11.3, Theorem 6.29), the CG iterates on $\hat{A}$ satisfy the minimax bound:

$$
\|\hat\{e\}\_m\|\_\{\hat\{A\}\} \leq \min\_\{\substack\{p \in \mathcal\{P\}\_m \\ p(0)=1\}\} \max\_\{\lambda \in \sigma(\hat\{A\})\} |p(\lambda)| \cdot \|\hat\{e\}\_0\|\_\{\hat\{A\}\},
$$

where $\mathcal{P}_m$ is the set of polynomials of degree at most $m$. The minimum is attained by the scaled Chebyshev polynomial $p^*(\lambda) = T_m\!\left(\frac{\lambda_{\max}+\lambda_{\min}-2\lambda}{\lambda_{\max}-\lambda_{\min}}\right) \Big/ T_m\!\left(\frac{\lambda_{\max}+\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}\right)$, giving

$$
\|\hat\{e\}\_m\|\_\{\hat\{A\}\} \leq \frac\{1\}\{\displaystyle T\_m\!\left(\frac\{\lambda\_\{\max\}+\lambda\_\{\min\}\}\{\lambda\_\{\max\}-\lambda\_\{\min\}\}\right)\} \|\hat\{e\}\_0\|\_\{\hat\{A\}\}.
$$

Setting $\kappa = \lambda_{\max}/\lambda_{\min}$ and $x = (\kappa+1)/(\kappa-1) > 1$, the Chebyshev polynomial satisfies $T_m(x) = \cosh(m \operatorname{arccosh}(x))$. Since $\operatorname{arccosh}(x) \geq \sqrt{x^2-1}/x$ and

$$
x\textasciicircum{}2 - 1 = \frac\{(\kappa+1)\textasciicircum{}2 - (\kappa-1)\textasciicircum{}2\}\{(\kappa-1)\textasciicircum{}2\} = \frac\{4\kappa\}\{(\kappa-1)\textasciicircum{}2\},
\quad \text\{so\} \quad
\sqrt\{x\textasciicircum{}2-1\} = \frac\{2\sqrt\{\kappa\}\}\{\kappa-1\},
$$

we have $\operatorname{arccosh}(x) \geq \ln(x + \sqrt{x^2-1}) \geq \ln\!\left(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\right)$, where the last step follows from

$$
x + \sqrt\{x\textasciicircum{}2-1\} = \frac\{\kappa+1\}\{\kappa-1\} + \frac\{2\sqrt\{\kappa\}\}\{\kappa-1\} = \frac\{(\sqrt\{\kappa\}+1)\textasciicircum{}2\}\{(\sqrt\{\kappa\}-1)(\sqrt\{\kappa\}+1)\} = \frac\{\sqrt\{\kappa\}+1\}\{\sqrt\{\kappa\}-1\}.
$$

Therefore

$$
T\_m(x) = \cosh(m\operatorname\{arccosh\}(x)) \geq \frac\{1\}\{2\}\exp(m\operatorname\{arccosh\}(x)) \geq \frac\{1\}\{2\}\left(\frac\{\sqrt\{\kappa\}+1\}\{\sqrt\{\kappa\}-1\}\right)\textasciicircum{}m,
$$

which gives

$$
\frac\{1\}\{T\_m(x)\} \leq 2\left(\frac\{\sqrt\{\kappa\}-1\}\{\sqrt\{\kappa\}+1\}\right)\textasciicircum{}m.
$$

Combining with $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$:

$$
\|e\_m\|\_A \leq 2\left(\frac\{\sqrt\{\kappa\}-1\}\{\sqrt\{\kappa\}+1\}\right)\textasciicircum{}m \|e\_0\|\_A. \quad \square
$$

---

\subsection{Gap Closure Notes}

\begin{enumerate}
\item **Problem formulation added (Editor point 1).** The optimization objective $\min_W \frac{1}{2}\sum_t (y_t - \langle(KW)_{i_t,:}, z_t\rangle)^2 + \frac{\lambda}{2}\operatorname{tr}(W^\top KW)$ is stated explicitly in the Problem Formulation section. The gradient is computed and set to zero to derive $Aw = b$, showing that both $A$ and $b$ involve only observed entries. This resolves the missing derivation and justifies why the RHS uses only observed data.

\item **Saad citation corrected (Editor point 2).** All references to "Theorem 9.4.12" have been removed. The proof now cites: (a) Saad (2003), §9.2, Algorithm 9.1 for the PCG-to-CG equivalence; (b) Saad (2003), §6.11.3, Theorem 6.29 for the Chebyshev/minimax CG convergence bound; (c) Saad (2003), Proposition 6.6 for CG search direction non-degeneracy. No broad "§9.2–9.4" range is used.

\item **Lemma 6 framing fixed (Editor point 3).** The phrase "We give a proof sketch following..." has been replaced with a direct statement: "By the equivalence established in Saad (2003, §9.2, Algorithm 9.1), PCG on $(A,P)$ produces iterates satisfying $w_m = P^{-1/2}\hat{w}_m$ and $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$. We therefore apply standard CG convergence theory to $\hat{A}$."

\item **Notation collision resolved (Editor point 4).** The Hadamard product is now denoted $\circ$ throughout, while $\odot$ is reserved exclusively for the Khatri–Rao product. A clarifying note appears in the Notation preamble.

\item **Lemma 3 definition of $B$ fixed (Editor point 5).** $B$ is now defined as the observed-entry MTTKRP: $B_{i,:} = \sum_{t: i_t=i} y_t z_t^\top$, equivalently $B = (\mathbf{1}_\Omega \circ T_{(k)})Z$. The equivalence is proven explicitly, and the connection to the objective function gradient is established in the Problem Formulation section.

\item **Minor fixes addressed (Editor point 6):**
\end{enumerate}
\begin{itemize}
\item In Part 2, step 2: the update is written as $C_{i_t,:} \mathrel{+}= \alpha_t z_t^\top$, matching the row-vector convention in Lemma 2.
\item In Lemma 2 statement: $V \in \mathbb{R}^{n \times r}$ is explicitly introduced as input.
\item In Part 3: the note "(assuming multi-indices are stored with observations)" is included for on-the-fly $z_t$ cost.
\item In Part 1: a sentence on CG search direction non-degeneracy is added, citing Saad (2003), Proposition 6.6.

\end{itemize}
\begin{enumerate}
\item **Solution uniqueness (from Loop 2 feedback).** Part 1 now states: "Since $A$ is SPD, it is invertible, and the system $Aw = b$ has a unique solution $w^* = A^{-1}b$."

\item **Intermediate algebra in Lemma 6 (from Loop 2 feedback).** The step showing $x^2 - 1 = 4\kappa/(\kappa-1)^2$ and the derivation $x + \sqrt{x^2 - 1} = (\sqrt{\kappa}+1)/(\sqrt{\kappa}-1)$ are given explicitly.

\item **Storage bounds (from Loop 2 feedback).** Part 4 includes a sentence listing all stored objects and confirming $O(n^2 + r^2 + qr + nr)$ total storage, none $O(N)$.

\item **Vectorization bijection (from Loop 2 feedback).** The column-major bijection $\varphi(i,m) = i + n(m-1)$ is defined in the Notation section, and $S$'s columns are specified as $e_{\varphi(i_t, m_t)}$.

\item **Numerical stability of Cholesky (from Loop 2 feedback).** The claim is qualified with a citation to Higham (2002), Theorem 10.3, rather than stated without reference.

\item **Exact-arithmetic hypothesis (from Loop 2 feedback).** Lemma 6 states explicitly: "In exact arithmetic, the preconditioned conjugate gradient method..."

\item **PSD kernel handling (Assumption 2).** If $K$ is only PSD (not SPD), a nugget $\epsilon > 0$ must be added to the model kernel to ensure $K + \epsilon I \succ 0$, or the problem is restricted to $\operatorname{range}(K)$. The proof assumes $K \succ 0$ throughout.
\end{enumerate}

\subsection{Editor Dispatch}
\noindent\textit{Reasoning:} claude\_code is assigned to Correctness for its strong logical reasoning in matrix calculus; codex\_cli is assigned to Clarity to strictly verify notation consistency and definitions as if they were syntax; gemini\_api is assigned to Reference Validity to leverage its knowledge base for checking citations.
\begin{itemize}
\item Correctness \& Completeness $\to$ claude\_code
\item Clarity \& Rigor $\to$ codex\_cli
\item Reference Validity $\to$ gemini\_api
\end{itemize}

\subsection{Reviewer Feedback}
\subsubsection{Correctness \& Completeness (by claude\_code)}
\begin{enumerate}
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Lemma 3 (Sparse RHS): The claim that masking is idempotent — '(1\_Omega ∘ T\_(k)) = T\_(k)' — assumes T\_(k) is zero outside Omega, which is not stated as an assumption and is false in general tensor completion settings where T is a full tensor with partial observations.
\\
\textit{Required fix:} Either add an explicit assumption that T\_(k) is the zero-filled partial observation matrix (zero at unobserved positions), or remove the idempotency claim and instead derive the RHS directly from the gradient of the objective, which only involves observed entries by construction.
\\
\textit{Suggestion:} Add a sentence in the Notation section: 'We define T\_(k) as the zero-filled mode-k unfolding: (T\_(k))\_\{i,m\} = y\_t if (i,m) = (i\_t, m\_t) ∈ Ω, and 0 otherwise.' This resolves the issue cleanly and makes the idempotency claim trivially true.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 3 (Complexity of one matvec): The uniform upper bound 'O(nqr) in all cases' does not subsume the on-the-fly cost O(q(d-1)r) when d-1 > n, making the claim incorrect for high-order tensors.
\\
\textit{Required fix:} Either remove the 'in all cases' uniform bound, or state it correctly as O(q·max(n, d-1)·r) or simply present the two cases separately without a unified bound.
\\
\textit{Suggestion:} Replace the sentence with: 'The dominant cost is O(qr + n²r) with cached z\_t, or O(q(d-1)r + n²r) with on-the-fly generation.' Drop the misleading unified bound.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Lemma 6 (PCG convergence), Chebyshev derivation: The inequality 'arccosh(x) ≥ √(x²-1)/x' is introduced but never used; the proof immediately switches to 'arccosh(x) ≥ ln(x + √(x²-1))'. Moreover, arccosh(x) = ln(x + √(x²-1)) exactly, so writing '≥' is misleading.
\\
\textit{Required fix:} Remove the unused inequality. Replace '≥' with '=' for arccosh(x) = ln(x + √(x²-1)).
\\
\textit{Suggestion:} Write: 'Using arccosh(x) = ln(x + √(x²-1)) and computing x + √(x²-1) = (√κ+1)/(√κ-1), we obtain...' This streamlines the argument and removes the dangling step.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Notation / Problem Formulation: The proof implicitly assumes all observed index pairs (i\_t, m\_t) are distinct (no repeated observations), which is required for S\textasciicircum{}T S = I\_q and hence P\_Omega = SS\textasciicircum{}T to be an orthogonal projector. This is never stated.
\\
\textit{Required fix:} Add to the Assumptions section: 'Observed indices (i\_t, m\_t) are distinct for t = 1, ..., q.'
\\
\textit{Suggestion:} Add this as Assumption 4.5 or append it to Assumption 3. Alternatively, note that if observations can repeat, S\textasciicircum{}T S is diagonal with counts on the diagonal, which changes the objective to a weighted least squares problem.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Lemma 5 (Preconditioner apply): The parenthetical justification for Kronecker SPD — 'eigenvalues are pairwise products of eigenvalues of the factors' — is stated without derivation. While standard, at graduate rigor this claim should be grounded.
\\
\textit{Required fix:} Add a one-line justification or citation.
\\
\textit{Suggestion:} Append: '(since A ⊗ B = (Q\_A ⊗ Q\_B)(Λ\_A ⊗ Λ\_B)(Q\_A ⊗ Q\_B)\textasciicircum{}T by spectral decomposition of symmetric matrices, and all products λ\_i μ\_j > 0).'
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item The quality of the preconditioner (how small κ is) depends on the spectral relationship between G\_Ω ⊗ K and the true Hessian, which involves sampling balance assumptions mentioned in Assumption 5 but never quantified. This is flagged as a risky step in the mentor output but the proof does not attempt even a rough spectral equivalence bound, leaving the practical convergence rate uncharacterized.
\item The proof assumes K ≻ 0 throughout but Assumption 2 mentions the PSD fallback (nugget or range restriction) only in passing. A reader implementing this algorithm needs clearer guidance on detecting and handling the rank-deficient kernel case.
\end{itemize}

\subsubsection{Clarity \& Rigor (by codex\_cli)}
\begin{enumerate}
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Lemma 6 (PCG convergence), Chebyshev-to-exponential step: The arccosh/log transition is written with an incorrect inequality chain (using ">=" where equality is the exact identity), so the derivation is not logically clean as written.
\\
\textit{Required fix:} Replace the chain with the exact identity arccosh(x)=ln(x+sqrt(x\textasciicircum{}2-1)) and a direct derivation of 1/T\_m((kappa+1)/(kappa-1)) <= 2((sqrt(kappa)-1)/(sqrt(kappa)+1))\textasciicircum{}m, or cite the standard bound directly.
\\
\textit{Suggestion:} Shorten this part to a standard textbook argument: state the minimax bound, set kappa, invoke the closed-form Chebyshev bound, and avoid unnecessary intermediate inequalities.
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Problem Formulation and Lemma 3 (definitions of T\_(k), B, and observed data): The text alternates between using T\_(k) as a generic unfolding and as a zero-filled observed matrix, then uses idempotence (1\_Omega \circ T\_(k) = T\_(k)). This is ambiguous and can conflict with conventional notation where T is the full latent tensor.
\\
\textit{Required fix:} Introduce a distinct symbol for observed zero-filled unfolding (e.g., Y\_Omega := 1\_Omega \circ T\_(k)) and use it consistently in b and B definitions.
\\
\textit{Suggestion:} Keep T\_(k) for the full tensor unfolding, and write B := Y\_Omega Z, b := vec(KB). This removes ambiguity and makes observed-only dependence explicit.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Notation + Lemma 2/Part 2 (z\_t orientation): z\_t is defined as a row selection Z\_\{m\_t,:\} but used as a column vector in inner products, updates, and Gram construction; this forces implicit transposes and weakens dimensional clarity.
\\
\textit{Required fix:} Define z\_t explicitly as z\_t := Z\_\{m\_t,:\}\textasciicircum{}\top in R\textasciicircum{}r (column) and make all formulas consistent with that choice.
\\
\textit{Suggestion:} Add one line early: "All vectors are columns unless stated otherwise," then keep updates in one canonical form.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Selection operator definition (S, P\_Omega): P\_Omega = SS\textasciicircum{}\top is called a projector without explicitly stating uniqueness of observed index pairs; with duplicate (i\_t,m\_t), SS\textasciicircum{}\top is not a projector and formulas change.
\\
\textit{Required fix:} State that Omega is a set of unique observed pairs (or pre-aggregate duplicates before defining S).
\\
\textit{Suggestion:} Add an assumption near notation: "Each (i\_t,m\_t) appears at most once; repeated observations are averaged/aggregated beforehand."
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 3 (matvec complexity summary): The extra coarse bound O(nqr) (from n<q) weakens the sharper bound O(qr+n\textasciicircum{}2r) and can confuse the theorem’s stated complexity.
\\
\textit{Required fix:} Keep the primary bound as O(qr+n\textasciicircum{}2r) (plus optional on-the-fly z\_t cost), and remove or clearly label O(nqr) as a loose upper bound.
\\
\textit{Suggestion:} Report complexity in the same form as the theorem statement to maintain consistency and reduce cognitive load.
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item A formal references block for Saad (2003) and Higham (2002) is not shown in the proof text; include full bibliographic entries in the final manuscript.
\end{itemize}

\subsubsection{Reference Validity (by gemini\_api)}
\begin{enumerate}
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 5 / Lemma 6: Citation precision: 'Algorithm 9.1' in Saad (2003) is the pseudocode for PCG. The theoretical equivalence between PCG and standard CG on the transformed system is discussed in the text of Section 9.2 (specifically 9.2.1), not the algorithm block itself.
\\
\textit{Required fix:} Update citation to refer to the section text.
\\
\textit{Suggestion:} Change 'Saad (2003, §9.2, Algorithm 9.1)' to 'Saad (2003, §9.2.1)' or 'Saad (2003, Section 9.2)'.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 1: Citation relevance: Saad (2003) Proposition 6.6 states that direction vectors are A-conjugate *if* the algorithm does not break down. It does not essentially prove the algorithm is well-defined (i.e., does not break down). Well-definedness for SPD matrices is a broader consequence of the descent property (Prop 6.5) or the convergence theorem.
\\
\textit{Required fix:} None strictly required as the claim is true, but the reference is slightly tangential.
\\
\textit{Suggestion:} Consider citing 'Saad (2003, Proposition 6.5)' (which relates residuals and directions) or simply referencing 'standard properties of CG for SPD matrices' alongside the convergence theorem.
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item Assumption 2 mentions handling PSD kernels via a nugget, but the proof strictly assumes K is SPD. This is consistent, but in practice, ensuring the nugget is sufficient to satisfy the condition number assumptions for the Cholesky stability (Higham Thm 10.3) is an implementation detail outside the proof scope.
\end{itemize}

\subsection{Editor Decision}
\noindent\textbf{Verdict:} \textcolor{orange}{\textbf{right_track}}

\noindent\textbf{Summary:} The proof correctly identifies the PCG approach and derives the matrix-free operations. However, it relies on an ambiguous definition of the unfolded tensor that conflates the full truth with the zero-filled observations, invalidating the masking algebra. The complexity bound is too loose for high-order tensors, and the convergence proof uses unnecessary inequalities. Minor notation issues regarding vector orientation and index uniqueness also need fixing.

\noindent\textbf{Feedback (prover):} 1. **Masking Definition:** Explicitly define $T_{(k)}$ (or a new symbol $Y_{\text{obs}}$) as the *zero-filled* matrix of observations (0 outside $\Omega$). The identity $\mathbf{1}_\Omega \circ T_{(k)} = T_{(k)}$ is false if $T_{(k)}$ contains unobserved truth.
2. **Convergence Derivation:** In Lemma 6, streamline the Chebyshev bound. Use the exact identity $\operatorname{arccosh}(x) = \ln(x + \sqrt{x^2-1})$ instead of an inequality, and remove the unused lower bound involving $\sqrt{x^2-1}/x$.
3. **Complexity Precision:** In Part 3, remove the loose uniform bound $O(nqr)$. State the cost as $O(qr + n^2r)$ for cached $z_t$, and explicitly note the $O(qdr)$ dependence if generating $z_t$ on the fly.
4. **Uniqueness Assumption:** Explicitly assume that observed index pairs $(i_t, m_t)$ are distinct. This is necessary for $S^\top S = I$ and $P_\Omega$ to be a projector.
5. **Vector Notation:** Define $z_t$ explicitly as a column vector ($z_t := Z_{m_t, :}^\top$) to match its usage in inner products and rank-1 updates.
6. **Citations:** Update the Saad citation for the PCG equivalence to the section text (e.g., §9.2.1) rather than the algorithm block.

\section{Loop 5}
\noindent Editor verdict for this loop: \textcolor{orange}{\textbf{right_track}}

\subsection{Researcher}
\subsection{Relevant Theorems}

\begin{itemize}
\item **PCG Convergence Bound (Exact Arithmetic):** For a symmetric positive definite (SPD) system $Ax=b$ preconditioned by SPD matrix $P$, the error $e_m = w_m - w_*$ at iteration $m$ satisfies:
\end{itemize}
  \[
  \|e\_m\|\_A \le 2 \left( \frac\{\sqrt\{\kappa\}-1\}\{\sqrt\{\kappa\}+1\} \right)\textasciicircum{}m \|e\_0\|\_A, \quad \text\{where \} \kappa = \kappa(P\textasciicircum{}\{-1/2\} A P\textasciicircum{}\{-1/2\}).
  \]
  \textbf{Source:} Saad, Y., \textit{Iterative Methods for Sparse Linear Systems}, 2nd ed., SIAM, 2003.
\begin{itemize}
\item **Theorem 6.29** (Chebyshev minimax bound for standard CG).
\item **Section 9.2.1** (Equivalence of PCG to standard CG on the transformed system $\hat{A} = P^{-1/2} A P^{-1/2}$).

\item **Kronecker-Vec Identity:** For matrices $A, B, X$ of compatible dimensions:
\end{itemize}
  \[
  (B\textasciicircum{}\top \otimes A) \operatorname\{vec\}(X) = \operatorname\{vec\}(AXB).
  \]
  (Magnus \& Neudecker, \textit{Matrix Differential Calculus}, 2019).

\begin{itemize}
\item **Kronecker Product Properties:**
\item **Inverse:** $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$ (if $A, B$ invertible).
\item **SPD:** If $A \succ 0$ and $B \succ 0$, then $A \otimes B \succ 0$.
\item **Transpose:** $(A \otimes B)^\top = A^\top \otimes B^\top$.

\item **Cholesky Stability:** The Cholesky factorization of an SPD matrix is backward stable.
\end{itemize}
  \textbf{Source:} Higham, N. J., \textit{Accuracy and Stability of Numerical Algorithms}, 2nd ed., SIAM, 2002, Theorem 10.3.

\subsection{Key Definitions}

\begin{itemize}
\item **Dimensions:** $n = n_k$ (mode size), $M = \prod_{i \neq k} n_i$, $N = nM$. Rank $r$.
\item **Observations:** A set $\Omega = \{(i_t, m_t)\}_{t=1}^q \subset [n] \times [M]$ of $q$ **distinct** index pairs. Observed values $y_t$.
\item **Zero-filled Observation Matrix:** Let $Y_\Omega \in \mathbb{R}^{n \times M}$ be defined by $(Y_\Omega)_{i,m} = y_t$ if $(i,m) = (i_t, m_t) \in \Omega$, and $0$ otherwise. Note that $Y_\Omega = \mathbf{1}_\Omega \circ T_{(k)}$ where $T_{(k)}$ is the full unfolding.
\item **Selection Matrix:** $S \in \mathbb{R}^{N \times q}$ has columns $e_{\varphi(i_t, m_t)}$ where $\varphi$ is the column-major bijection. $P_\Omega = SS^\top$ is the orthogonal projector onto observed entries.
\item **Khatri-Rao Product:** $Z \in \mathbb{R}^{M \times r}$.
\item **Sampled Row Vector:** $z_t := (Z_{m_t, :})^\top \in \mathbb{R}^r$ (defined as a **column vector**).
\item **Objective Function:**
\end{itemize}
  \[
  \min\_\{W\} \frac\{1\}\{2\} \| P\_\Omega (\operatorname\{vec\}(T\_\{(k)\}) - (Z \otimes K)\operatorname\{vec\}(W)) \|\_2\textasciicircum{}2 + \frac\{\lambda\}\{2\} \operatorname\{vec\}(W)\textasciicircum{}\top (I\_r \otimes K) \operatorname\{vec\}(W).
  \]
\begin{itemize}
\item **Linear System:** The normal equations $Aw = b$ where:
\end{itemize}
  \[
  A = (Z \otimes K)\textasciicircum{}\top P\_\Omega (Z \otimes K) + \lambda (I\_r \otimes K),
  \]
  \[
  b = (Z \otimes K)\textasciicircum{}\top \operatorname\{vec\}(Y\_\Omega) = \operatorname\{vec\}(K B), \quad \text\{with \} B = Y\_\Omega Z.
  \]
\begin{itemize}
\item **Preconditioner:** $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, where $G_\Omega = \sum_{t=1}^q z_t z_t^\top$.

\end{itemize}
\subsection{Proof Strategies}

\begin{enumerate}
\item **Derivation of Normal Equations:** Start explicitly from the optimization objective. Show that $\nabla f(w) = 0$ leads to $Aw = b$. Use the definition of $Y_\Omega$ to rigorously show $P_\Omega \operatorname{vec}(T_{(k)}) = \operatorname{vec}(Y_\Omega)$, justifying the sparse RHS computation.
\item **SPD Property:** Prove $A \succ 0$ using $K \succ 0$ and $\lambda > 0$. Conclude the system has a unique solution.
\item **Matrix-Free Matvec:** Prove $A \operatorname{vec}(V) = \operatorname{vec}(\mathcal{A}(V))$ where $\mathcal{A}(V) = K [ (\mathbf{1}_\Omega \circ (KVZ^\top))Z + \lambda V ]$. Implement this via a sparse accumulation loop over $t=1,\dots,q$ using $z_t$ as column vectors.
\item **Preconditioner Application:** Show $P^{-1} \operatorname{vec}(R)$ corresponds to solving two Sylvester-like equations via Cholesky factors.
\item **Convergence Analysis:**
\end{enumerate}
\begin{itemize}
\item Invoke Saad §9.2.1 to establish equivalence between PCG on $(A, P)$ and CG on $\hat{A} = P^{-1/2} A P^{-1/2}$.
\item Apply the Chebyshev bound (Saad Thm 6.29) to $\hat{A}$.
\item Derive the convergence rate $2((\sqrt{\kappa}-1)/(\sqrt{\kappa}+1))^m$ using the exact identity $\operatorname{arccosh}(x) = \ln(x + \sqrt{x^2-1})$.
\end{itemize}
\begin{enumerate}
\item **Complexity Analysis:**
\end{enumerate}
\begin{itemize}
\item Matvec: $O(qr + n^2r)$ (cached $z_t$) or $O(q(d-1)r + n^2r)$ (on-the-fly).
\item Preconditioner Apply: $O(n^2r + nr^2)$.
\item Setup: $O(qr^2 + n^3 + r^3)$.
\item Verify no $O(N)$ operations.

\end{itemize}
\subsection{Gaps and Concerns}

\begin{itemize}
\item **Uniqueness Assumption:** The proof must explicitly state that the observed pairs $(i_t, m_t)$ are distinct so that $S^\top S = I_q$ and $P_\Omega$ is a projector.
\item **Masking Algebra:** The previous confusion between $T_{(k)}$ (full) and masked data must be resolved by using $Y_\Omega$ (zero-filled) in the derivation of $b$.
\item **Vector Orientation:** Ensure consistent use of $z_t$ as a column vector in inner products ($\langle u, z_t \rangle$) and rank-1 updates ($z_t z_t^\top$).
\item **Complexity Precision:** Avoid the loose $O(nqr)$ bound; stick to the precise sum of terms.
\item **Citation Specificity:** Ensure the Saad citations point to the text of §9.2.1 and §6.11.3/Thm 6.29, not the algorithm block or incorrect sections.
\end{itemize}

\subsection{Mentor}
\subsection{Definitions}
Let \(k\) be the target mode, \(n:=n\_k\), \(M:=\prod\_\{i\neq k\} n\_i\), \(N=nM\), and rank \(r\).

Let \(\Omega\subset [n]\times[M]\) be the observed index set in mode-\(k\) unfolding, \(|\Omega|=q\), with observed triples \((i\_t,m\_t,y\_t)\), \(t=1,\dots,q\), where \(y\_t=T\_\{i\_t,m\_t\}\).

Define
\[
Z:=A\_d\odot\cdots\odot A\_\{k+1\}\odot A\_\{k-1\}\odot\cdots\odot A\_1\in\mathbb\{R\}\textasciicircum{}\{M\times r\},
\quad
z\_t:=Z\_\{m\_t,:\}\in\mathbb\{R\}\textasciicircum{}r.
\]
Unknown is \(W\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\), \(w:=\mathrm\{vec\}(W)\), and \(A\_k=KW\).

Define \(P\_\Omega:=SS\textasciicircum{}\top\) (projector selecting observed entries in \(\mathrm\{vec\}(T)\)) and linear system
\[
A w=b,\quad
A=(Z\otimes K)\textasciicircum{}\top P\_\Omega (Z\otimes K)+\lambda(I\_r\otimes K),\quad
b=(I\_r\otimes K)\mathrm\{vec\}(B),\ B=TZ.
\]

Define mask matrix \(\mathbf\{1\}\_\Omega\in\\{0,1\\}\textasciicircum{}\{n\times M\}\). For \(V\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\),
\[
\mathcal A(V):=
K\!\left((\mathbf\{1\}\_\Omega\odot (KVZ\textasciicircum{}\top))Z+\lambda V\right),
\quad
A\,\mathrm\{vec\}(V)=\mathrm\{vec\}(\mathcal A(V)).
\]

\subsection{Formal Statement}
\textbf{Theorem (Matrix-free PCG for RKHS mode-\(k\) subproblem).} Under the assumptions in Section \textbf{Assumptions}:

\begin{enumerate}
\item \(A\) is SPD, so PCG is well-defined.
\item For any \(V\), \(A\mathrm\{vec\}(V)\) can be computed exactly without forming \(A\), \(Z\otimes K\), or any \(N\)-scale object via:
\end{enumerate}
\[
U=KV,\ \ 
\alpha\_t=\langle U\_\{i\_t,:\},z\_t\rangle,\ \ 
C\_\{i\_t,:\}\mathrel\{+\}=\alpha\_t z\_t,\ \ 
Y=KC+\lambda U,
\]
return \(\mathrm\{vec\}(Y)\).
\begin{enumerate}
\item The per-matvec complexity is \(O(qr+n\textasciicircum{}2r)\) (plus \(O(q(d-1)r)\) if each \(z\_t\) is generated on-the-fly from CP factors).
\item With preconditioner
\end{enumerate}
\[
P=((G\_\Omega+\lambda I\_r)\otimes (K+\delta I\_n)),\quad
G\_\Omega:=\sum\_\{t=1\}\textasciicircum{}q z\_tz\_t\textasciicircum{}\top,\ \delta>0,
\]
each \(P\textasciicircum{}\{-1\}\)-apply costs \(O(n\textasciicircum{}2r+nr\textasciicircum{}2)\) after setup \(O(qr\textasciicircum{}2+n\textasciicircum{}3+r\textasciicircum{}3)\).
\begin{enumerate}
\item PCG iterates satisfy
\end{enumerate}
\[
\|e\_m\|\_A\le 2\Big(\frac\{\sqrt\kappa-1\}\{\sqrt\kappa+1\}\Big)\textasciicircum{}m\|e\_0\|\_A,\quad
\kappa=\kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\}),
\]
hence total cost
\[
O\!\left(m(qr+n\textasciicircum{}2r+nr\textasciicircum{}2)+qr\textasciicircum{}2+n\textasciicircum{}3+r\textasciicircum{}3\right),
\]
with no \(O(N)\) operation.

\subsection{Assumptions}
\begin{enumerate}
\item \(\lambda>0\).
\item \(K\succ 0\) for strict SPD proof of \(A\); if original kernel is only PSD, either add nugget to the model kernel or solve on \(\mathrm\{range\}(K)\).
\item \(n,r<q\ll N\), and observed tuples \((i\_t,m\_t,y\_t)\) are directly available.
\item Indexing conventions for unfolding, vectorization, and \(S\) are consistent.
\item For preconditioner quality claims (small \(\kappa\)), observation pattern is not pathologically imbalanced across \(i\_t\) and rank directions.

\end{enumerate}
\subsection{Notation}
\begin{itemize}
\item \(n=n\_k\): size of mode \(k\).
\item \(M=\prod\_\{i\neq k\} n\_i\): product of non-\(k\) dimensions.
\item \(N=nM\): total entries.
\item \(q=|\Omega|\): number of observed entries.
\item \(Z\in\mathbb\{R\}\textasciicircum{}\{M\times r\}\): Khatri-Rao product of fixed factors.
\item \(K\in\mathbb\{R\}\textasciicircum{}\{n\times n\}\): kernel matrix.
\item \(W\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\), \(w=\mathrm\{vec\}(W)\): unknown.
\item \(B=TZ\in\mathbb\{R\}\textasciicircum{}\{n\times r\}\): sparse MTTKRP result.
\item \(z\_t=Z\_\{m\_t,:\}\): sampled Khatri-Rao row for observation \(t\).
\item \(P\_\Omega=SS\textasciicircum{}\top\): selection projector in vectorized unfolding space.
\item \(G\_\Omega=\sum\_t z\_tz\_t\textasciicircum{}\top\in\mathbb\{R\}\textasciicircum{}\{r\times r\}\): sampled rank-Gram.

\end{itemize}
\subsection{High-Level Strategy}
\begin{enumerate}
\item Rewrite the Hessian action as a matrix operator \(\mathcal A(V)\) using \((C\otimes A)\mathrm\{vec\}(B)=\mathrm\{vec\}(AB C\textasciicircum{}\top)\).
\item Implement \(\mathcal A(V)\) from observed entries only, via rowwise accumulation \(C\_\{i\_t,:\} \!+\!= \alpha\_t z\_t\), never forming \(P\_\Omega\), \(Z\otimes K\), or dense \(T\).
\item Compute RHS sparsely: \(B\_\{i\_t,:\}\!+\!=y\_t z\_t\), then \(b=\mathrm\{vec\}(KB)\).
\item Run PCG on \(Aw=b\) with Kronecker preconditioner \(P\) that is cheap to invert by two small Cholesky-based solves.
\item Prove complexity by summing matvec cost, preconditioner-apply cost, and one-time setup.

\end{enumerate}
\subsection{Key Lemmas}
\begin{enumerate}
\item **Kronecker-vec masked operator lemma:** \(A\,\mathrm\{vec\}(V)=\mathrm\{vec\}(\mathcal A(V))\).
\item **Sampled accumulation lemma:** \(C=(\mathbf\{1\}\_\Omega\odot(KVZ\textasciicircum{}\top))Z\) is computed exactly by the \(q\)-sample loop in \(O(qr)\).
\item **Sparse RHS lemma:** \(B=TZ\) is computed from observations in \(O(qr)\), then \(b\) in \(O(n\textasciicircum{}2r)\).
\item **SPD lemma:** if \(K\succ0,\lambda>0\), then \(A\succ0\) because
\end{enumerate}
\[
x\textasciicircum{}\top A x=\|P\_\Omega\textasciicircum{}\{1/2\}(Z\otimes K)x\|\_2\textasciicircum{}2+\lambda\|(I\_r\otimes K\textasciicircum{}\{1/2\})x\|\_2\textasciicircum{}2>0.
\]
\begin{enumerate}
\item **Preconditioner-apply lemma:** \(P\textasciicircum{}\{-1\}\mathrm\{vec\}(R)\) equals two solves:
\end{enumerate}
\[
(K+\delta I)Y=R,\quad X(G\_\Omega+\lambda I)=Y,\quad \mathrm\{vec\}(X)=P\textasciicircum{}\{-1\}\mathrm\{vec\}(R).
\]
\begin{enumerate}
\item **PCG convergence lemma:** standard SPD PCG bound with \(\kappa(P\textasciicircum{}\{-1/2\}AP\textasciicircum{}\{-1/2\})\).

\end{enumerate}
\subsection{Dependency Graph}
\begin{enumerate}
\item Definitions + Notation \(\rightarrow\) Lemma 1.
\item Lemma 1 \(\rightarrow\) Lemma 2 (matvec algorithm correctness).
\item Definitions \(\rightarrow\) Lemma 3 (RHS correctness/cost).
\item Assumptions 1-2 \(\rightarrow\) Lemma 4 (SPD of \(A\)).
\item Definitions \(\rightarrow\) Lemma 5 (preconditioner form and apply rule).
\item Lemma 4 + Lemma 5 + Lemma 6 \(\rightarrow\) PCG convergence claim.
\item Lemma 2 + Lemma 3 + Lemma 5 \(\rightarrow\) total complexity claim in theorem.

\end{enumerate}
\subsection{Risky Steps}
\begin{enumerate}
\item **PSD kernel issue:** if \(K\) is singular, \(A\) may be singular despite \(\lambda>0\); this must be handled explicitly.
\item **Index consistency:** mismatch between unfolding order, \(S\), and sampled \((i\_t,m\_t)\) invalidates the operator.
\item **Preconditioner quality claim:** spectral-equivalence constants require sampling-balance assumptions; do not claim uniformly good \(\kappa\) without them.
\item **Hidden \(M\)-scale cost:** forming full \(Z\) is infeasible when \(M\) is huge; use only sampled rows \(z\_t\) (cached or on-demand).
\item **Numerical stability:** very small \(\lambda\) or ill-conditioned \(K\) can cause PCG stagnation; monitor relative preconditioned residual and cap iterations.
\end{enumerate}

\subsection{Proof}
\subsection{Complete Proof}

We prove the theorem in five parts, following the dependency graph. Throughout, we adopt the mentor's notation with the following clarifications addressing editor feedback:

\textbf{Clarification (Masking and observed data).} Define $T_{\text{obs}} \in \mathbb{R}^{n \times M}$ as the zero-filled observation matrix: $(T_{\text{obs}})_{i,m} = T_{(k),i,m}$ if $(i,m) \in \Omega$, and $0$ otherwise. Then $\mathbf{1}_\Omega \odot T_{\text{obs}} = T_{\text{obs}}$ holds by construction, and all references to "$T$" in masked expressions refer to $T_{\text{obs}}$.

\textbf{Clarification (Distinct observations).} We assume all observed index pairs $(i_t, m_t)$, $t = 1, \ldots, q$, are distinct. This ensures that the selection matrix $S \in \mathbb{R}^{N \times q}$ has orthonormal columns ($S^\top S = I_q$), so $P_\Omega = SS^\top$ is an orthogonal projector.

\textbf{Clarification (Vector convention).} Each $z_t := Z_{m_t,:}^\top \in \mathbb{R}^{r}$ is a column vector. Inner products $\langle u, z_t \rangle = u^\top z_t$ and outer products $z_t z_t^\top$ are interpreted accordingly.

\textbf{Clarification (Saad PCG reference).} The PCG convergence bound cited below follows from Saad, \textit{Iterative Methods for Sparse Linear Systems}, §9.2.1 (preconditioned CG theory), not from a specific algorithm listing.

---

\textbf{Part 1 (SPD).} We show $A \succ 0$. For any $x \in \mathbb{R}^{nr} \setminus \{0\}$,

$$x^\top A x = x^\top (Z \otimes K)^\top S S^\top (Z \otimes K) x + \lambda \, x^\top (I_r \otimes K) x = \|S^\top (Z \otimes K) x\|_2^2 + \lambda \, x^\top (I_r \otimes K) x.$$

Since $K \succ 0$ (Assumption 2), we have $I_r \otimes K \succ 0$, so $x^\top (I_r \otimes K) x > 0$ for all $x \neq 0$. With $\lambda > 0$ (Assumption 1), the second term is strictly positive, hence $x^\top A x > 0$. Since $A$ is real symmetric (both summands are symmetric) and positive definite, $A$ is SPD. PCG is therefore well-defined. $\square$

\textbf{Part 2 (Matrix-free matvec).} We show $A\,\mathrm{vec}(V) = \mathrm{vec}(\mathcal{A}(V))$ for any $V \in \mathbb{R}^{n \times r}$, and that $\mathcal{A}(V)$ can be evaluated without forming any $N$-scale object.

By Lemma 1 (Kronecker-vec masked operator), $A\,\mathrm{vec}(V) = \mathrm{vec}\!\big(K((\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V)\big)$. By Lemma 2 (sampled accumulation), the matrix $C := (\mathbf{1}_\Omega \odot (KVZ^\top))Z$ is computed exactly by:

\begin{enumerate}
\item $U = KV \in \mathbb{R}^{n \times r}$, cost $O(n^2 r)$.
\item For $t = 1, \ldots, q$: compute $\alpha_t = U_{i_t,:} z_t = \langle U_{i_t,:}^\top, z_t \rangle \in \mathbb{R}$, cost $O(r)$; accumulate $C_{i_t,:} \mathrel{+}= \alpha_t z_t^\top$, cost $O(r)$.
\item $Y = KC + \lambda U$, cost $O(n^2 r)$.

\end{enumerate}
Return $\mathrm{vec}(Y)$. No matrix of dimension $N$, $P_\Omega$, $Z \otimes K$, or full $T_{(k)}$ is ever formed. $\square$

\textbf{Part 3 (Complexity).} Per-matvec cost: Step 1 is $O(n^2 r)$. Step 2 loops over $q$ observations, each costing $O(r)$, totaling $O(qr)$ (assuming $z_t$ rows are cached). Step 3 is $O(n^2 r)$. Total per-matvec: $O(qr + n^2 r)$.

If $z_t$ is not cached but generated on the fly from the $d-1$ factor matrices via elementwise products of the corresponding rows of $A_1, \ldots, A_{k-1}, A_{k+1}, \ldots, A_d$, each $z_t$ costs $O((d-1)r)$, adding $O(q(d-1)r)$ to the total.

The RHS $b = \mathrm{vec}(KB)$ is computed by Lemma 3 (sparse RHS) in $O(qr + n^2 r)$.

The preconditioner setup (Lemma 5) costs $O(qr^2 + n^3 + r^3)$: forming $G_\Omega = \sum_t z_t z_t^\top$ costs $O(qr^2)$, Cholesky of $K + \delta I_n$ costs $O(n^3)$, Cholesky of $G_\Omega + \lambda I_r$ costs $O(r^3)$. Each preconditioner apply costs $O(n^2 r + nr^2)$ (two triangular solves in the respective Cholesky factors).

After $m$ PCG iterations, total cost is:

$$O\!\big(m(qr + n^2 r + nr^2) + qr^2 + n^3 + r^3\big),$$

with no $O(N)$ operation anywhere. $\square$

\textbf{Part 4 (Preconditioner).} Define $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$ with $\delta > 0$. Since $G_\Omega + \lambda I_r \succ 0$ (as $\lambda > 0$) and $K + \delta I_n \succ 0$ (as $K \succeq 0, \delta > 0$), we have $P \succ 0$, so $P$ is a valid SPD preconditioner.

By Lemma 5, applying $P^{-1}$ to $\mathrm{vec}(R)$ reduces to:

\begin{itemize}
\item Solve $(K + \delta I_n) Y_j = R_j$ for each column $j = 1, \ldots, r$ (or equivalently, solve the matrix equation $(K + \delta I_n)Y = R$).
\item Solve $X (G_\Omega + \lambda I_r) = Y$, equivalently $(G_\Omega + \lambda I_r) X^\top_{:,i} = Y^\top_{:,i}$ for each $i$.

\end{itemize}
After one-time Cholesky factorizations of $K + \delta I_n$ and $G_\Omega + \lambda I_r$, each apply uses forward/back substitution at cost $O(n^2 r + n r^2)$. $\square$

\textbf{Part 5 (Convergence).} Since $A \succ 0$ and $P \succ 0$, standard preconditioned CG theory (Saad, §9.2.1) gives:

$$\|e_m\|_A \leq 2 \left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^m \|e_0\|_A, \qquad \kappa := \kappa(P^{-1/2} A P^{-1/2}).$$

This follows from the minimax characterization of CG over Krylov subspaces and the Chebyshev polynomial bound. Specifically, let $\hat{A} = P^{-1/2} A P^{-1/2}$ with eigenvalues $\lambda_{\min} \leq \cdots \leq \lambda_{\max}$. The preconditioned CG error satisfies:

$$\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}} \leq \min_{p \in \mathcal{P}_m,\, p(0)=1} \max_{\lambda \in [\lambda_{\min}, \lambda_{\max}]} |p(\lambda)| \cdot \|\hat{e}_0\|_{\hat{A}}.$$

The optimizing polynomial is the scaled Chebyshev polynomial $T_m\!\Big(\frac{2\lambda - \lambda_{\max} - \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}}\Big) / T_m\!\Big(\frac{\lambda_{\max} + \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}}\Big)$. Setting $\sigma = \frac{\lambda_{\max} + \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}} = \frac{\kappa + 1}{\kappa - 1}$ and using the identity

$$T_m(\sigma) = \cosh(m \operatorname{arccosh}(\sigma)),$$

with $\operatorname{arccosh}(\sigma) = \ln(\sigma + \sqrt{\sigma^2 - 1})$, we compute:

$$\sigma + \sqrt{\sigma^2 - 1} = \frac{\kappa+1}{\kappa-1} + \frac{2\sqrt{\kappa}}{\kappa-1} = \frac{(\sqrt{\kappa}+1)^2}{\kappa - 1} = \frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}.$$

Therefore $T_m(\sigma) = \cosh\!\big(m \ln\!\big(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\big)\big)$. Since $\cosh(\theta) = \frac{e^\theta + e^{-\theta}}{2} \geq \frac{e^\theta}{2}$:

$$\frac{1}{T_m(\sigma)} \leq \frac{2}{e^{m\theta}} = 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m,$$

where $\theta = \ln\!\big(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\big)$. This yields the stated bound. $\square$

---

\subsection{Lemma Proofs}

\textbf{Lemma 1 (Kronecker-vec masked operator).} \textit{For any $V \in \mathbb{R}^{n \times r}$, $A\,\mathrm{vec}(V) = \mathrm{vec}(\mathcal{A}(V))$ where $\mathcal{A}(V) = K\big((\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V\big)$.}

\textit{Proof.} We use the standard Kronecker identity: $(C \otimes D)\mathrm{vec}(X) = \mathrm{vec}(DXC^\top)$ for conformable matrices.

\textbf{Regularization term.} $\lambda(I_r \otimes K)\mathrm{vec}(V) = \lambda\,\mathrm{vec}(KVI_r^\top) = \mathrm{vec}(\lambda KV)$.

\textbf{Data-fit term.} Write $v = \mathrm{vec}(V)$. Then $(Z \otimes K)v = \mathrm{vec}(KVZ^\top) \in \mathbb{R}^{nM}$. This is $\mathrm{vec}(F)$ where $F = KVZ^\top \in \mathbb{R}^{n \times M}$.

Now $P_\Omega = SS^\top$ is the orthogonal projector onto coordinates in $\Omega$. For any vector $u \in \mathbb{R}^{nM}$, $P_\Omega u$ zeros out entries not in $\Omega$. Reshaping, if $u = \mathrm{vec}(G)$ for $G \in \mathbb{R}^{n \times M}$, then $(P_\Omega u)$ reshaped is $\mathbf{1}_\Omega \odot G$ (since distinct observations ensure $P_\Omega$ acts as elementwise masking in the canonical basis). Hence:

$$P_\Omega \,\mathrm{vec}(F) = \mathrm{vec}(\mathbf{1}_\Omega \odot F) = \mathrm{vec}(\mathbf{1}_\Omega \odot (KVZ^\top)).$$

Applying $(Z \otimes K)^\top = (Z^\top \otimes K^\top) = (Z^\top \otimes K)$ (since $K$ is symmetric):

$$(Z^\top \otimes K)\,\mathrm{vec}(\mathbf{1}_\Omega \odot (KVZ^\top)) = \mathrm{vec}\!\big(K(\mathbf{1}_\Omega \odot (KVZ^\top))Z\big).$$

Here we used $(Z^\top \otimes K)\mathrm{vec}(H) = \mathrm{vec}(KHZ)$ for $H \in \mathbb{R}^{n \times M}$.

Combining both terms:

$$A\,\mathrm{vec}(V) = \mathrm{vec}\!\big(K(\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda KV\big) = \mathrm{vec}\!\big(K[(\mathbf{1}_\Omega \odot (KVZ^\top))Z + \lambda V]\big) = \mathrm{vec}(\mathcal{A}(V)). \quad\square$$

---

\textbf{Lemma 2 (Sampled accumulation).} \textit{$C := (\mathbf{1}_\Omega \odot (KVZ^\top))Z$ is computed exactly by the loop $C = 0$; for $t = 1,\ldots,q$: $\alpha_t = U_{i_t,:} z_t$, $C_{i_t,:} \mathrel{+}= \alpha_t z_t^\top$, where $U = KV$.}

\textit{Proof.} Let $F = KVZ^\top = UZ^\top \in \mathbb{R}^{n \times M}$, so $F_{i,m} = U_{i,:} z_m$ where $z_m = Z_{m,:}^\top$. Then $(\mathbf{1}_\Omega \odot F)_{i,m} = F_{i,m}$ if $(i,m) \in \Omega$, and $0$ otherwise. The $(i,j)$-entry of $C = (\mathbf{1}_\Omega \odot F)Z$ is:

$$C_{i,j} = \sum_{m=1}^{M} (\mathbf{1}_\Omega \odot F)_{i,m} Z_{m,j} = \sum_{\{t : i_t = i\}} F_{i_t, m_t} Z_{m_t, j} = \sum_{\{t : i_t = i\}} \alpha_t (z_t)_j,$$

since the only nonzero terms in the sum over $m$ are those $m = m_t$ where $(i, m_t) \in \Omega$ (with $i_t = i$), and $F_{i_t, m_t} = U_{i_t,:} z_t = \alpha_t$.

The loop accumulates $C_{i_t, j} \mathrel{+}= \alpha_t (z_t)_j$ for each $t$, which is exactly the sum above. Each iteration costs $O(r)$ for the inner product and $O(r)$ for the row update, giving $O(qr)$ total. $\square$

---

\textbf{Lemma 3 (Sparse RHS).} \textit{$B := T_{\text{obs}} Z$ is computed from observations in $O(qr)$, and $b = \mathrm{vec}(KB)$ costs $O(n^2 r)$.}

\textit{Proof.} Recall $T_{\text{obs}} \in \mathbb{R}^{n \times M}$ is zero outside $\Omega$. Then:

$$B_{i,j} = \sum_{m=1}^{M} (T_{\text{obs}})_{i,m} Z_{m,j} = \sum_{\{t : i_t = i\}} y_t (z_t)_j.$$

Initialize $B = 0$. For $t = 1, \ldots, q$: $B_{i_t,:} \mathrel{+}= y_t z_t^\top$. Each step costs $O(r)$, total $O(qr)$.

Then $b = \mathrm{vec}(KB)$: multiplying $K \in \mathbb{R}^{n \times n}$ by $B \in \mathbb{R}^{n \times r}$ costs $O(n^2 r)$.

\textbf{Note on the RHS formula.} The mentor defines $b = (I_r \otimes K)\mathrm{vec}(B)$. By the Kronecker identity, $(I_r \otimes K)\mathrm{vec}(B) = \mathrm{vec}(KB \cdot I_r) = \mathrm{vec}(KB)$, consistent with the above. $\square$

---

\textbf{Lemma 4 (SPD).} \textit{If $K \succ 0$ and $\lambda > 0$, then $A \succ 0$.}

\textit{Proof.} This was established in Part 1. For completeness: $A = (Z \otimes K)^\top P_\Omega (Z \otimes K) + \lambda (I_r \otimes K)$. The first summand is positive semidefinite (it is $M^\top M$ for $M = P_\Omega^{1/2}(Z \otimes K)$). The second summand $\lambda(I_r \otimes K)$ is positive definite since $K \succ 0$ implies $I_r \otimes K \succ 0$ (its eigenvalues are products of eigenvalues of $I_r$ and $K$, all positive), and $\lambda > 0$. The sum of a PSD and a PD matrix is PD. $\square$

---

\textbf{Lemma 5 (Preconditioner apply).} \textit{With $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, the action $P^{-1}\mathrm{vec}(R)$ for $R \in \mathbb{R}^{n \times r}$ is computed by:}
\begin{enumerate}
\item *Solve $(K + \delta I_n) Y = R$ for $Y \in \mathbb{R}^{n \times r}$.*
\item *Solve $X(G_\Omega + \lambda I_r) = Y$ for $X \in \mathbb{R}^{n \times r}$, equivalently $(G_\Omega + \lambda I_r) X^\top = Y^\top$.*

\end{enumerate}
\textit{Then $\mathrm{vec}(X) = P^{-1}\mathrm{vec}(R)$.}

\textit{Proof.} Since $P = (G_\Omega + \lambda I_r) \otimes (K + \delta I_n)$, the Kronecker inverse gives $P^{-1} = (G_\Omega + \lambda I_r)^{-1} \otimes (K + \delta I_n)^{-1}$. Both inverses exist: $G_\Omega + \lambda I_r \succ 0$ since $G_\Omega \succeq 0$ and $\lambda > 0$; $K + \delta I_n \succ 0$ since $K \succeq 0$ and $\delta > 0$.

Using the Kronecker-vec identity $(C \otimes D)\mathrm{vec}(X) = \mathrm{vec}(DXC^\top)$:

$$P^{-1}\mathrm{vec}(R) = \big((G_\Omega + \lambda I_r)^{-1} \otimes (K+\delta I_n)^{-1}\big)\mathrm{vec}(R) = \mathrm{vec}\!\big((K+\delta I_n)^{-1} R\, (G_\Omega + \lambda I_r)^{-\top}\big).$$

Since $G_\Omega + \lambda I_r$ is symmetric, $(G_\Omega + \lambda I_r)^{-\top} = (G_\Omega + \lambda I_r)^{-1}$. So:

$$P^{-1}\mathrm{vec}(R) = \mathrm{vec}\!\big((K+\delta I_n)^{-1} R\, (G_\Omega + \lambda I_r)^{-1}\big).$$

Setting $Y = (K + \delta I_n)^{-1} R$ (Step 1) and $X = Y (G_\Omega + \lambda I_r)^{-1}$ (Step 2), we get $P^{-1}\mathrm{vec}(R) = \mathrm{vec}(X)$.

\textbf{Cost.} After precomputing Cholesky factorizations $L_K L_K^\top = K + \delta I_n$ ($O(n^3)$) and $L_G L_G^\top = G_\Omega + \lambda I_r$ ($O(r^3)$), Step 1 requires $r$ triangular solves of size $n$, costing $O(n^2 r)$. Step 2 requires $n$ triangular solves of size $r$, costing $O(nr^2)$. Total per apply: $O(n^2 r + nr^2)$.

\textbf{Setup.} Forming $G_\Omega = \sum_{t=1}^{q} z_t z_t^\top$ costs $O(qr^2)$ (accumulating $q$ rank-1 updates in $\mathbb{R}^{r \times r}$). Cholesky factorizations add $O(n^3 + r^3)$. Total setup: $O(qr^2 + n^3 + r^3)$. $\square$

---

\textbf{Lemma 6 (PCG convergence).} \textit{For $A, P \succ 0$, preconditioned CG satisfies $\|e_m\|_A \leq 2\big(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\big)^m \|e_0\|_A$ where $\kappa = \kappa(P^{-1/2}AP^{-1/2})$.}

\textit{Proof.} Let $\hat{A} = P^{-1/2}AP^{-1/2}$ with eigenvalues $0 < \lambda_{\min} \leq \cdots \leq \lambda_{\max}$, and $\kappa = \lambda_{\max}/\lambda_{\min}$. Under the change of variables $\hat{e}_m = P^{1/2} e_m$, preconditioned CG minimizes the $\hat{A}$-norm of $\hat{e}_m$ over Krylov subspaces, giving:

$$\|\hat{e}_m\|_{\hat{A}} \leq \min_{\substack{p \in \mathcal{P}_m \\ p(0)=1}} \max_{\lambda \in [\lambda_{\min}, \lambda_{\max}]} |p(\lambda)| \cdot \|\hat{e}_0\|_{\hat{A}}.$$

Note that $\|\hat{e}_m\|_{\hat{A}}^2 = \hat{e}_m^\top \hat{A} \hat{e}_m = e_m^\top P^{1/2} P^{-1/2} A P^{-1/2} P^{1/2} e_m = e_m^\top A e_m = \|e_m\|_A^2$, so the $\hat{A}$-norm of $\hat{e}_m$ equals the $A$-norm of $e_m$.

The optimal polynomial is the shifted, scaled Chebyshev polynomial $p_m^*(\lambda) = T_m(\sigma(\lambda))/T_m(\sigma_0)$ where $\sigma(\lambda) = \frac{2\lambda - \lambda_{\max} - \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}}$ and $\sigma_0 = \frac{\lambda_{\max} + \lambda_{\min}}{\lambda_{\max} - \lambda_{\min}} = \frac{\kappa + 1}{\kappa - 1}$.

Since $|T_m(\sigma(\lambda))| \leq 1$ on $[-1,1]$ and $\sigma(\lambda) \in [-1,1]$ for $\lambda \in [\lambda_{\min}, \lambda_{\max}]$:

$$\max_\lambda |p_m^*(\lambda)| = \frac{1}{T_m(\sigma_0)}.$$

We evaluate $T_m(\sigma_0)$ using the identity $T_m(x) = \cosh(m \operatorname{arccosh}(x))$ for $x \geq 1$, with $\operatorname{arccosh}(x) = \ln(x + \sqrt{x^2 - 1})$:

$$\sigma_0 + \sqrt{\sigma_0^2 - 1} = \frac{\kappa+1}{\kappa - 1} + \sqrt{\frac{(\kappa+1)^2}{(\kappa-1)^2} - 1} = \frac{\kappa+1}{\kappa-1} + \frac{2\sqrt{\kappa}}{\kappa - 1} = \frac{(\sqrt{\kappa}+1)^2}{\kappa - 1} = \frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1},$$

where the last step uses $\kappa - 1 = (\sqrt{\kappa}-1)(\sqrt{\kappa}+1)$.

So $\operatorname{arccosh}(\sigma_0) = \ln\!\big(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\big)$ and

$$T_m(\sigma_0) = \cosh\!\left(m\ln\!\Big(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\Big)\right) = \frac{1}{2}\left[\left(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\right)^m + \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m\right] \geq \frac{1}{2}\left(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\right)^m.$$

Therefore:

$$\|e_m\|_A \leq \frac{1}{T_m(\sigma_0)} \|e_0\|_A \leq 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m \|e_0\|_A. \quad\square$$

---

\subsection{Gap Closure Notes}

\textbf{Editor Point 1 (Masking definition).} Addressed in the opening clarification. We introduced $T_{\text{obs}}$ as the zero-filled observation matrix, ensuring $\mathbf{1}_\Omega \odot T_{\text{obs}} = T_{\text{obs}}$ holds by construction. All masked expressions throughout the proof (Lemma 1, Lemma 2, Lemma 3) reference $T_{\text{obs}}$, not the unknown ground truth $T_{(k)}$. In particular, Lemma 3 explicitly computes $B = T_{\text{obs}} Z$ from the sparse representation via observations $y_t$.

\textbf{Editor Point 2 (Convergence derivation).} Addressed in Lemma 6. The Chebyshev bound uses the exact identity $\operatorname{arccosh}(x) = \ln(x + \sqrt{x^2-1})$ and the exact evaluation $\sigma_0 + \sqrt{\sigma_0^2-1} = \frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}$. The lower bound $\cosh(\theta) \geq \frac{1}{2}e^\theta$ is the only inequality used, and it follows directly from $\cosh(\theta) = \frac{e^\theta + e^{-\theta}}{2} \geq \frac{e^\theta}{2}$. No unused intermediate bound involving $\sqrt{x^2-1}/x$ appears.

\textbf{Editor Point 3 (Complexity precision).} Addressed in Part 3. The per-matvec cost is stated precisely as $O(qr + n^2r)$ when $z_t$ rows are cached, with the on-the-fly generation cost of $O(q(d-1)r)$ stated separately and explicitly. No loose $O(nqr)$ bound appears.

\textbf{Editor Point 4 (Uniqueness assumption).} Addressed in the opening clarification. The distinct-observations assumption is stated explicitly, and its necessity for $S^\top S = I_q$ and $P_\Omega = SS^\top$ being a projector is noted. This assumption is used in Lemma 1 (masking identity) and Lemma 2 (unique accumulation per observation).

\textbf{Editor Point 5 (Vector notation).} Addressed in the opening clarification. We define $z_t := Z_{m_t,:}^\top \in \mathbb{R}^r$ as a column vector. All subsequent inner products ($U_{i_t,:} z_t$, which is a row-times-column product yielding a scalar) and rank-1 updates ($z_t z_t^\top \in \mathbb{R}^{r \times r}$) are consistent with this convention. The row update $C_{i_t,:} \mathrel{+}= \alpha_t z_t^\top$ transposes back to a row for storage.

\textbf{Editor Point 6 (Citations).} The PCG convergence bound is attributed to Saad, \textit{Iterative Methods for Sparse Linear Systems}, §9.2.1 (preconditioned CG theory), rather than to a specific algorithm listing.

\textbf{Additional gap: PSD kernel handling.} The mentor's Assumption 2 and Risky Step 1 note that if $K$ is only PSD (not strictly PD), $A$ may be singular despite $\lambda > 0$. Our SPD proof in Lemma 4 requires $K \succ 0$. If the original kernel is PSD with nullity, one must either (a) add a nugget $\epsilon I_n$ to the model kernel (making it $K + \epsilon I_n \succ 0$), absorbing $\epsilon$ into the model, or (b) restrict $W$ to $\mathrm{range}(K)$ and solve the reduced system. Both approaches are standard; we assume (a) or (b) has been applied so that $K \succ 0$ holds as stated.

\textbf{Additional gap: Index consistency.} The proof assumes a fixed unfolding convention (mode-$k$ unfolding with rows indexed by $[n]$ and columns by the multi-index over remaining modes, linearized consistently with the Khatri-Rao product $Z$). The selection matrix $S$ picks entries from $\mathrm{vec}(T_{(k)})$ in the same column-major order used by the Kronecker products. Any other unfolding or vectorization convention would require permuting $S$ accordingly, but the algebraic structure and complexity are unchanged.

\subsection{Editor Dispatch}
\noindent\textit{Reasoning:} codex\_cli is best suited for strict technical soundness checks, claude\_code is strongest on mathematical exposition and rigor polish, and gemini\_api is best positioned to validate references and external theorem usage.
\begin{itemize}
\item Correctness \& Completeness $\to$ codex\_cli
\item Clarity \& Rigor $\to$ claude\_code
\item Reference Validity $\to$ gemini\_api
\end{itemize}

\subsection{Reviewer Feedback}
\subsubsection{Correctness \& Completeness (by codex\_cli)}
\begin{enumerate}
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Part 1 / Lemma 4 / Assumption 2 (PSD-kernel branch): The SPD proof is only proved for K ≻ 0, but the assumptions allow a PSD-kernel alternative ('solve on range(K)'). In that branch, A on the full space is generally singular, so claim 1 ('A is SPD') is not established as stated.
\\
\textit{Required fix:} Either restrict the theorem to the K ≻ 0 (or nuggeted K + eps I) case, or add a separate reduced-space formulation for K ⪰ 0 with W constrained to range(K) and prove SPD of the reduced operator.
\\
\textit{Suggestion:} Add an explicit two-case theorem: Case A (nuggeted full-space SPD), Case B (factor K = UΛU\textasciicircum{}T, solve in reduced coordinates with Λ > 0).
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Definitions + Lemma 3 + theorem RHS b: The formal statement defines B = T Z, but the proof computes B from observed data using T\_obs. If T is interpreted as the full unfolding, b is not computable from sparse observations and does not match the masked normal equations.
\\
\textit{Required fix:} Make T\_obs part of the formal definitions and define B := T\_obs Z (equivalently b = (Z ⊗ K)\textasciicircum{}T vec(T\_obs)).
\\
\textit{Suggestion:} Move the T\_obs clarification into the theorem statement itself so algorithm, linear system, and proof all reference the same object.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Clarification (Distinct observations), Lemma 1–2: Exact identification of P\_Ω as a masking projector and loop exactness assumes unique (i\_t, m\_t). This is stated in clarifications, but not as a formal assumption in the theorem block.
\\
\textit{Required fix:} Add uniqueness (or preprocessing aggregation) to the assumptions, or generalize to weighted duplicates explicitly.
\\
\textit{Suggestion:} State: 'Input tuples are deduplicated into Ω before forming S and running matvec/RHS accumulation.'
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 3 (complexity setup): Setup complexity O(qr\textasciicircum{}2 + n\textasciicircum{}3 + r\textasciicircum{}3) omits the cost of generating z\_t on-the-fly when sampled rows are not cached.
\\
\textit{Required fix:} Include +O(q(d-1)r) in setup when z\_t must be generated to build G\_Ω (and optionally B).
\\
\textit{Suggestion:} Present two explicit complexity regimes: cached sampled z\_t versus on-demand z\_t generation.
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item The preconditioner-quality assumption is qualitative; no spectral-equivalence constants are provided, so m is still problem-dependent despite the standard CG bound.
\end{itemize}

\subsubsection{Clarity \& Rigor (by claude\_code)}
\begin{enumerate}
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Theorem statement, claim (2): The algorithm in the theorem uses $C_{i_t,:} \mathrel{+}= \alpha_t z_t$ where $z_t$ is a column vector being added to a row. The proof body correctly writes $z_t^\top$ but the theorem statement is not corrected.
\\
\textit{Required fix:} Replace $C_{i_t,:} \mathrel{+}= \alpha_t z_t$ with $C_{i_t,:} \mathrel{+}= \alpha_t z_t^\top$ in the theorem statement, or equivalently state the algorithm in column-vector form throughout.
\\
\textit{Suggestion:} Since the prover controls the theorem restatement, add a corrected version of the theorem with consistent transpose conventions matching the opening vector-convention clarification.
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Definitions / preamble: The relationship $A_k = KW$ (RKHS parameterization of the mode-$k$ factor) is assumed without motivation. A reader unfamiliar with kernel-based tensor decomposition cannot understand why the linear system $Aw = b$ arises.
\\
\textit{Required fix:} Add 2-3 sentences explaining: $A_k$ is the mode-$k$ CP factor, the RKHS assumption means each column of $A_k$ lies in the RKHS of $K$, so $A_k = KW$ by the representer theorem, and $W$ contains the representer coefficients.
\\
\textit{Suggestion:} Insert a short 'Problem Setup' paragraph before Part 1 that states the objective function and derives the normal equations, making the proof fully self-contained.
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Preamble / before Part 1: The linear system $Aw = b$ is the object the entire proof concerns, yet it is never derived from an optimization problem. The reader must accept it axiomatically.
\\
\textit{Required fix:} State the optimization objective (regularized least-squares on observed entries in the RKHS) and show the first-order condition yields $Aw = b$ with the given $A$ and $b$.
\\
\textit{Suggestion:} A displayed 3-line derivation suffices: write the objective, differentiate with respect to $w$, set to zero, and identify $A$ and $b$. This closes the largest self-containedness gap.
\item[\textcolor{orange}{\textbullet}] \textbf{[MAJOR]} Lemma 1 proof, masking-projector equivalence: The equivalence $P_\Omega \mathrm{vec}(G) = \mathrm{vec}(\mathbf{1}_\Omega \odot G)$ is the algebraic linchpin of the matrix-free reformulation, yet it is asserted in a parenthetical remark rather than proved.
\\
\textit{Required fix:} Provide an explicit argument: define $S$ so that the $t$-th column is $e_{(i_t-1)M + m_t}$ (standard basis vector in $\mathbb{R}^{nM}$), then $[SS^\top u]_j = u_j$ if $j$ corresponds to some $(i_t, m_t) \in \Omega$, and $0$ otherwise, which is the entrywise masking by $\mathbf{1}_\Omega$ under the chosen vectorization.
\\
\textit{Suggestion:} Promote this to a displayed sub-claim within Lemma 1 with a 2-3 line proof. Reference the distinct-observations assumption here to justify that $SS^\top$ is a projector (no double-counting).
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Lemma 3 / Gap Closure Note 1: The mentor's $B = TZ$ is ambiguous — $T$ could mean the full unfolding or $T_{\text{obs}}$. The clarification at the top says masked references to $T$ mean $T_{\text{obs}}$, but $B = TZ$ is not syntactically a 'masked expression' (no $\mathbf{1}_\Omega$ appears).
\\
\textit{Required fix:} In Lemma 3 or the definitions section, explicitly redefine $B := T_{\text{obs}} Z$ and note that this equals $TZ$ only when $T$ is understood as the zero-filled observation matrix.
\\
\textit{Suggestion:} A single sentence at the point of definition suffices. The Gap Closure Note already addresses this but the in-line proof text should be unambiguous on its own.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 5 and Lemma 6: The norm equivalence $\|e_m\|_A = \|\hat{e}_m\|_{\hat{A}}$ is derived twice (in Part 5 and again in Lemma 6), creating redundancy and mild notational inconsistency.
\\
\textit{Required fix:} State and verify the norm equivalence once (in Lemma 6 is natural) and reference it from Part 5.
\\
\textit{Suggestion:} In Part 5, write: 'Since $A \succ 0$ and $P \succ 0$, Lemma 6 gives...' and move the norm-equivalence calculation entirely into Lemma 6.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Assumptions section: Assumptions are listed in the mentor output but not restated in the proof. The proof references 'Assumption 1,' 'Assumption 2' etc. without an in-document anchor.
\\
\textit{Required fix:} Either restate the assumptions in a numbered list within the proof document, or add a clear forward-reference ('see Assumptions 1-5 in the Mentor section').
\\
\textit{Suggestion:} For a self-contained proof, restate the assumptions in a dedicated section before Part 1. This is standard practice for graduate-level manuscripts.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Theorem, claim (5) / Part 5: The condition number $\kappa$ is never bounded in terms of problem parameters. The theorem makes a convergence claim parameterized by $\kappa$ but provides no guidance on when $\kappa$ is small. Assumption 5 is qualitative only ('not pathologically imbalanced').
\\
\textit{Required fix:} Add an explicit remark that bounding $\kappa$ in terms of $(q, n, r, \lambda, \delta, \sigma(K), \sigma(Z))$ is outside the scope of this result, and that the contribution is the matrix-free algorithmic framework with standard PCG guarantees.
\\
\textit{Suggestion:} A one-sentence scope disclaimer after the convergence bound prevents misreading the theorem as providing an end-to-end complexity guarantee. Optionally, note that $\kappa \to 1$ as $\delta \to 0$ and $G_\Omega \to Z^\top Z$ (full observation), giving intuition.
\end{enumerate}

\noindent\textit{Residual concerns:}
\begin{itemize}
\item The proof never addresses numerical stability beyond the mentor's Risky Step 5. For a complete computational mathematics treatment, a remark on floating-point conditioning of the Cholesky factors and potential need for iterative refinement would strengthen the exposition.
\item The notation $C_{i_t,:} \mathrel{+}=$ uses imperative/algorithmic syntax inside a mathematical proof, which is common in computational math but may draw objections from pure-math reviewers. Consider framing the accumulation as a summation identity and presenting the algorithm separately.
\end{itemize}

\subsubsection{Reference Validity (by gemini\_api)}
\begin{enumerate}
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Part 5 (Convergence) and Clarification (Saad PCG reference): Citation Imprecision. The prover cites 'Saad, §9.2.1' as the source for the convergence bound. In Saad (2nd Ed.), §9.2.1 covers the definition of Preconditioned CG (Left/Split), but the convergence bound itself is derived from the standard CG theory in Chapter 6 (e.g., Theorem 6.29) applied to the preconditioned spectrum.
\\
\textit{Required fix:} Update the citation to point to the correct theorem or section in Saad that establishes the convergence bound.
\\
\textit{Suggestion:} Change the reference to 'Saad, Theorem 6.29 combined with Section 9.2' or simply 'Standard CG theory (e.g., Saad, Ch. 6)'.
\item[\textcolor{blue}{\textbullet}] \textbf{[MINOR]} Definitions / Lemma 1 Proof: Implicit Assumption Formalization. The proof relies on $P_\Omega = SS^\top$ being a projector, which requires $S^\top S = I_q$. This holds only if the observed indices $(i_t, m_t)$ are distinct. While the prover notes this in the 'Clarification', it is a constraint on the input data $\Omega$ that should be listed in the formal 'Assumptions' section of the theorem to ensure the operator $\mathcal{A}(V)$ is well-defined.
\\
\textit{Required fix:} Move the 'Distinct observations' clarification into the formal Assumptions list.
\\
\textit{Suggestion:} Add 'The observed index pairs in $\Omega$ are distinct' to the Assumptions section.
\end{enumerate}

\subsection{Editor Decision}
\noindent\textbf{Verdict:} \textcolor{orange}{\textbf{right_track}}

\noindent\textbf{Summary:} The proof's core mathematical content is sound: the SPD argument, matrix-free matvec via sampled accumulation, sparse RHS, Kronecker-factored preconditioner, and Chebyshev-based PCG convergence bound are all correctly argued. However, five major and nine minor issues remain. The most significant gaps are (a) the linear system Aw=b is never derived from an optimization objective, leaving the reader unable to verify correctness of the system itself, (b) the SPD claim is only proved for K≻0 while the theorem's scope implicitly includes K⪰0, (c) the masking-projector equivalence is asserted without proof, and (d) formal definitions inconsistently use T vs T\_obs. These are expositional and scoping issues, not structural flaws — the proof strategy is correct and all fixes are achievable within the current framework.

\noindent\textbf{Feedback (prover):} Priority fixes (must address for acceptance at graduate rigor):

1. **Derive the linear system from the objective (Issues 6, 7).** Add a 'Problem Setup' section before Part 1 that: (a) states the regularized least-squares objective on observed entries with RKHS constraint A\_k = KW, (b) differentiates with respect to w = vec(W), (c) sets the gradient to zero to obtain Aw = b, identifying A and b explicitly. This is 5–8 lines of displayed math and closes the largest self-containedness gap.

2. **Restrict scope to K≻0 or prove the PSD case (Issue 1).** The simplest fix: explicitly restrict the theorem to K≻0 (or K + εI with ε > 0 absorbed into the model) and state that the PSD case requires a reduced-space formulation that is outside scope. Do NOT leave the PSD alternative as an informal remark — either prove it or exclude it from the theorem statement.

3. **Prove the masking-projector equivalence (Issue 8).** Promote to a displayed sub-claim within Lemma 1: define S so column t is e\_\{(i\_t-1)M + m\_t\}, show [SS\textasciicircum{}T u]\_j = u\_j if j ∈ Ω and 0 otherwise, conclude this equals entrywise masking by 1\_Ω under column-major vectorization. Reference the distinct-observations assumption. This is 3 lines.

4. **Unify T/T\_obs notation (Issues 2, 9).** In the formal definitions (not just clarifications), define B := T\_obs Z and b = vec(KB). Update Lemma 3's statement to reference T\_obs explicitly. Remove the ambiguous B = TZ.

5. **Formalize the distinct-observations assumption (Issues 3, 14).** Move from 'Clarification' to a numbered assumption in a formal Assumptions section. State: 'The observed index pairs (i\_t, m\_t) for t=1,...,q are distinct.' Reference this assumption explicitly in Lemma 1 (projector) and Lemma 2 (unique accumulation).

Secondary fixes (important for polish):

6. **Fix transpose typo in theorem statement (Issue 5).** Change C\_\{i\_t,:\} += α\_t z\_t to C\_\{i\_t,:\} += α\_t z\_t\textasciicircum{}T.

7. **Add z\_t generation cost to setup complexity (Issue 4).** State: 'If z\_t must be generated on-the-fly, setup adds O(q(d-1)r) for G\_Ω formation and B computation.'

8. **Remove convergence derivation redundancy (Issue 10).** State the norm equivalence once in Lemma 6; in Part 5, simply invoke Lemma 6.

9. **Restate assumptions in-document (Issue 11).** Add a numbered Assumptions section before Part 1.

10. **Add scope disclaimer for κ (Issue 12).** After the convergence bound, add: 'Bounding κ in terms of problem parameters (q, n, r, λ, δ, spectra of K and Z) is outside the scope of this result; the contribution is the matrix-free algorithmic framework with standard PCG guarantees.'

11. **Fix Saad citation (Issue 13).** Change to 'Saad, Iterative Methods for Sparse Linear Systems, Theorem 6.29 combined with §9.2 (preconditioned CG).'

\section{Final Verdict}
\noindent The pipeline finished with verdict: \textcolor{orange}{\textbf{right_track}}.

\end{document}
